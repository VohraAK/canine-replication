{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0","cell_type":"code","source":"# %pip install peft evaluate transformers Levenshtein ipywidgets\n# %pip install protobuf==3.20.3\n# !rm -rf /kaggle/working/cache","metadata":{"id":"c186240c","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:17:08.793089Z","iopub.execute_input":"2025-12-01T10:17:08.793689Z","iopub.status.idle":"2025-12-01T10:17:08.797177Z","shell.execute_reply.started":"2025-12-01T10:17:08.793665Z","shell.execute_reply":"2025-12-01T10:17:08.796293Z"}},"outputs":[],"execution_count":80},{"id":"1","cell_type":"code","source":"# X\n\nimport os\nos.environ[\"TRANSFORMERS_DISABLE_CHAT_TEMPLATES\"] = \"1\"\nos.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\nos.environ[\"TRANSFORMERS_NO_ADDITIONAL_CHAT_TEMPLATES\"] = \"1\"","metadata":{"id":"cd8da8ab","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:17:08.798544Z","iopub.execute_input":"2025-12-01T10:17:08.798797Z","iopub.status.idle":"2025-12-01T10:17:08.814087Z","shell.execute_reply.started":"2025-12-01T10:17:08.798780Z","shell.execute_reply":"2025-12-01T10:17:08.813417Z"}},"outputs":[],"execution_count":81},{"id":"2","cell_type":"code","source":"from datasets import load_dataset, load_from_disk\n# from UQA.canine_utils import preprocess_uqa, lora_config, print_trainable_parameters, normalize_answer, exact_match_score, f1_score, edit_distance_score, gold_answer, decode_prediction\nfrom transformers import CanineTokenizer\nfrom peft import LoraConfig, TaskType, get_peft_model\nimport re\nimport string\nfrom collections import Counter\nimport numpy as np\nimport Levenshtein\n\nfrom transformers import TrainingArguments, Trainer, TrainerCallback\nimport json\nfrom huggingface_hub import HfApi, notebook_login, whoami","metadata":{"id":"d87eba82","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:17:08.814764Z","iopub.execute_input":"2025-12-01T10:17:08.814978Z","iopub.status.idle":"2025-12-01T10:17:08.829944Z","shell.execute_reply.started":"2025-12-01T10:17:08.814953Z","shell.execute_reply":"2025-12-01T10:17:08.829276Z"}},"outputs":[],"execution_count":82},{"id":"3","cell_type":"code","source":"# notebook_login()\n# whoami()","metadata":{"id":"0e98cebe-4c08-4850-b3c1-1529564fdb1b","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:17:08.830669Z","iopub.execute_input":"2025-12-01T10:17:08.830897Z","iopub.status.idle":"2025-12-01T10:17:08.849726Z","shell.execute_reply.started":"2025-12-01T10:17:08.830873Z","shell.execute_reply":"2025-12-01T10:17:08.849020Z"}},"outputs":[],"execution_count":83},{"id":"4","cell_type":"code","source":"from transformers import CanineTokenizer, CanineForQuestionAnswering\nimport torch\nmodel_name = 'google/canine-s'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n\ntokenizer = CanineTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=False)\nmodel = CanineForQuestionAnswering.from_pretrained(model_name, trust_remote_code=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f2dd5a40","outputId":"140c30ea-575d-45cd-ea54-7818cdfe6bf5","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:17:08.851290Z","iopub.execute_input":"2025-12-01T10:17:08.851753Z","iopub.status.idle":"2025-12-01T10:17:10.383197Z","shell.execute_reply.started":"2025-12-01T10:17:08.851735Z","shell.execute_reply":"2025-12-01T10:17:10.382466Z"}},"outputs":[{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":84},{"id":"5","cell_type":"code","source":"uqa_dataset = load_dataset(\"uqa/UQA\")\nuqa_train = uqa_dataset[\"train\"].shuffle(seed=42).select(range(80000))\nuqa_val = uqa_dataset[\"validation\"].shuffle(seed=42).select(range(2000))","metadata":{"id":"d474e2e8","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:17:10.384066Z","iopub.execute_input":"2025-12-01T10:17:10.384323Z","iopub.status.idle":"2025-12-01T10:17:11.530181Z","shell.execute_reply.started":"2025-12-01T10:17:10.384297Z","shell.execute_reply":"2025-12-01T10:17:11.529423Z"}},"outputs":[],"execution_count":85},{"id":"07d32821-6814-435e-82f8-3ab14f14554b","cell_type":"code","source":"len(uqa_dataset['train'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:17:11.530961Z","iopub.execute_input":"2025-12-01T10:17:11.531162Z","iopub.status.idle":"2025-12-01T10:17:11.535720Z","shell.execute_reply.started":"2025-12-01T10:17:11.531146Z","shell.execute_reply":"2025-12-01T10:17:11.535023Z"}},"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"124745"},"metadata":{}}],"execution_count":86},{"id":"6","cell_type":"markdown","source":"---","metadata":{"id":"89c472d5"}},{"id":"7","cell_type":"markdown","source":"## Updated preprocessors!\n\nPreviously, we tried to apply the same approach we used in TYDIQA on UQA, the problem was the preprocessors were aligning the answer spans in units of **byte-level spans** instead of **character-level spans**. The calculations were adding byte-level offsets to the answer lengths, and since Urdu characters may be quantified in multiple bytes, the model was being fed the wrong spans -> GIGO!","metadata":{"id":"6e80a8d3"}},{"id":"8","cell_type":"code","source":"MAX_SEQ_LENGTH = 384\nDOC_STRIDE = 64\n\ndef preprocess_uqa(examples, tokenizer, max_length=MAX_SEQ_LENGTH, doc_stride=DOC_STRIDE, model_obj=None, indices=None):\n    # Handle tokenizer/model limits safely\n    tokenizer_max = getattr(tokenizer, \"model_max_length\", max_length)\n    model_max = getattr(model_obj.config, \"max_position_embeddings\", None) if model_obj is not None else None\n    max_allowed = max_length\n    if tokenizer_max is not None and tokenizer_max > 0:\n        max_allowed = min(max_allowed, tokenizer_max)\n    if model_max is not None and model_max > 0:\n        max_allowed = min(max_allowed, model_max)\n\n    questions = [q.strip() for q in examples[\"question\"]]\n    contexts = examples[\"context\"]\n    answers = examples[\"answer\"]\n    answer_starts = examples[\"answer_start\"]\n\n    encoded = {\n        \"input_ids\": [],\n        \"attention_mask\": [],\n        \"token_type_ids\": [],\n        \"start_positions\": [],\n        \"end_positions\": [],\n        \"overflow_to_sample_mapping\": []\n    }\n\n    for i, (question, context, answer, answer_start) in enumerate(zip(questions, contexts, answers, answer_starts)):\n        example_idx = indices[i] if indices is not None else i\n\n        # CANINE encodes to characters directly.\n        # add_special_tokens=False gives us raw character IDs.\n        question_ids = tokenizer.encode(question, add_special_tokens=False)\n        context_ids = tokenizer.encode(context, add_special_tokens=False)\n\n        # 1. Setup Targets (DIRECT MAPPING)\n        # UQA answer_start is a CHARACTER index. CANINE tokens are CHARACTERS.\n        # Therefore, answer_start maps 1:1 to the context_ids index.\n        if answer and answer_start != -1:\n            gold_char_start = answer_start\n            gold_char_end = answer_start + len(answer) # Points to char AFTER the answer\n        else:\n            gold_char_start = -1\n            gold_char_end = -1\n\n        # 2. Calculate Window Size\n        # [CLS] + Question + [SEP] + Context + [SEP]\n        special_tokens_count = tokenizer.num_special_tokens_to_add(pair=True)\n        max_context_length = max_allowed - len(question_ids) - special_tokens_count\n\n        if max_context_length <= 0:\n            # Edge case: Question is too long, skip or truncate question (skipping here for safety)\n            continue\n\n        # 3. Sliding Window Loop\n        stride_step = max_context_length - doc_stride\n        if stride_step <= 0: stride_step = max_context_length # Fallback if doc_stride is too big\n\n        for chunk_start_idx in range(0, len(context_ids), stride_step):\n            chunk_end_idx = min(chunk_start_idx + max_context_length, len(context_ids))\n            context_chunk = context_ids[chunk_start_idx:chunk_end_idx]\n\n            # Build inputs using tokenizer utility to handle special tokens correctly\n            input_ids = tokenizer.build_inputs_with_special_tokens(question_ids, context_chunk)\n            token_type_ids = tokenizer.create_token_type_ids_from_sequences(question_ids, context_chunk)\n            attention_mask = [1] * len(input_ids)\n\n            # Calculate Offset: Where does the context actually start in input_ids?\n            # Typically: [CLS] (1) + Q_Len + [SEP] (1) = Start of Context\n            # We calculate this dynamically to be safe:\n            sep_indices = [k for k, x in enumerate(input_ids) if x == tokenizer.sep_token_id]\n            if not sep_indices:\n                continue # Should not happen\n            context_offset_in_input = sep_indices[0] + 1\n\n            # 4. Label Assignment\n            # Check if the answer lies ENTIRELY within this specific chunk\n            is_answer_in_chunk = (\n                gold_char_start >= chunk_start_idx and\n                gold_char_end <= chunk_end_idx and\n                gold_char_start != -1\n            )\n\n            if is_answer_in_chunk:\n                # Map global context index to local window index\n                start_pos = context_offset_in_input + (gold_char_start - chunk_start_idx)\n                # -1 because end_positions is usually inclusive in HF Trainers\n                end_pos = context_offset_in_input + (gold_char_end - chunk_start_idx) - 1\n            else:\n                # Label as [CLS] (index 0) if answer is not here\n                start_pos = 0\n                end_pos = 0\n\n            # 5. Padding\n            pad_len = max_allowed - len(input_ids)\n            if pad_len > 0:\n                input_ids += [tokenizer.pad_token_id] * pad_len\n                attention_mask += [0] * pad_len\n                token_type_ids += [0] * pad_len\n\n            # 6. Final Safety Truncation (just in case)\n            if len(input_ids) > max_allowed:\n                input_ids = input_ids[:max_allowed]\n                attention_mask = attention_mask[:max_allowed]\n                token_type_ids = token_type_ids[:max_allowed]\n                # If labels were pushed out by truncation, reset to 0\n                if start_pos >= max_allowed or end_pos >= max_allowed:\n                    start_pos = 0\n                    end_pos = 0\n\n            encoded[\"input_ids\"].append(input_ids)\n            encoded[\"attention_mask\"].append(attention_mask)\n            encoded[\"token_type_ids\"].append(token_type_ids)\n            encoded[\"start_positions\"].append(start_pos)\n            encoded[\"end_positions\"].append(end_pos)\n            encoded[\"overflow_to_sample_mapping\"].append(example_idx)\n\n            # Break loop if this chunk reached the end of the context\n            if chunk_end_idx >= len(context_ids):\n                break\n\n    return encoded\n","metadata":{"id":"438d8765","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:17:11.537674Z","iopub.execute_input":"2025-12-01T10:17:11.537871Z","iopub.status.idle":"2025-12-01T10:17:11.553083Z","shell.execute_reply.started":"2025-12-01T10:17:11.537855Z","shell.execute_reply":"2025-12-01T10:17:11.552248Z"}},"outputs":[],"execution_count":87},{"id":"9","cell_type":"code","source":"# LoRA config\nlora_config = LoraConfig(\n    task_type=TaskType.QUESTION_ANS,\n    r=16,   # changed from 8\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"query\", \"value\", \"key\"],\n    bias=\"none\",\n    modules_to_save=[\"qa_outputs\"],\n)\n\ndef print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n","metadata":{"id":"a3e95eec","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:17:11.553932Z","iopub.execute_input":"2025-12-01T10:17:11.554148Z","iopub.status.idle":"2025-12-01T10:17:11.572210Z","shell.execute_reply.started":"2025-12-01T10:17:11.554129Z","shell.execute_reply":"2025-12-01T10:17:11.571429Z"}},"outputs":[],"execution_count":88},{"id":"10","cell_type":"code","source":"# preprocess the train and val splits\nprocessed_train = uqa_train.map(lambda examples, indices: preprocess_uqa(examples, tokenizer, indices=indices), batched=True, remove_columns=uqa_train.column_names, with_indices=True)\nprocessed_val = uqa_val.map(lambda examples, indices: preprocess_uqa(examples, tokenizer, indices=indices), batched=True, remove_columns=uqa_val.column_names, with_indices=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["b1984a6d29864e2d940119370816a37e","a307de6c263a4c20a6418344cbd98c0c","1db208af0dbc4f2facee78148266a207","d44d38a959ec44aa90e91c15a83abbd6","527baa5fc421480da4d2dc7041e19b1f","d398c81b546d4527a41dd97bd87ad7d8","ab4047c7f0144667857fe835d452f6c7","0120d513dd4d4fccac2d528eb7ff4696","c3e0981c2924416fbdf9ceab3e6b04ab","bc970c64373f4f69b6c6936087ed978a","4a74d22a2c334fbda54a95c5e29e712a"]},"id":"d11807b9","outputId":"64fc2534-2871-4bd2-b3fa-4b37973486e2","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:17:11.572855Z","iopub.execute_input":"2025-12-01T10:17:11.573090Z","iopub.status.idle":"2025-12-01T10:17:11.636424Z","shell.execute_reply.started":"2025-12-01T10:17:11.573072Z","shell.execute_reply":"2025-12-01T10:17:11.635731Z"}},"outputs":[],"execution_count":89},{"id":"11","cell_type":"code","source":"# processed_train","metadata":{"id":"D-emFQTIaZRL","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:17:11.637200Z","iopub.execute_input":"2025-12-01T10:17:11.637512Z","iopub.status.idle":"2025-12-01T10:17:11.640675Z","shell.execute_reply.started":"2025-12-01T10:17:11.637491Z","shell.execute_reply":"2025-12-01T10:17:11.639856Z"}},"outputs":[],"execution_count":90},{"id":"12","cell_type":"code","source":"# processed_val","metadata":{"id":"Yy3SiWwCabEi","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:17:11.641394Z","iopub.execute_input":"2025-12-01T10:17:11.641656Z","iopub.status.idle":"2025-12-01T10:17:11.654229Z","shell.execute_reply.started":"2025-12-01T10:17:11.641635Z","shell.execute_reply":"2025-12-01T10:17:11.653471Z"}},"outputs":[],"execution_count":91},{"id":"13","cell_type":"code","source":"processed_train.save_to_disk(\"/kaggle/working/cache/processed_train_uqa\")\nprocessed_val.save_to_disk(\"/kaggle/working/cache/processed_val_uqa\")   # cached it\n\n\nprocessed_train = load_from_disk(\"/kaggle/working/cache/processed_train_uqa\")\nprocessed_val = load_from_disk(\"/kaggle/working/cache/processed_val_uqa\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["05d936e2dc9d412b8637c174a3c0be64","7e0b41aa16f241a4ba3bb8a2f3525984","2bebe7e1f3f341dfaabf29963d2c5995","41b300b02ed2413ba80865aaa99ece2a","b4740a7137e742d687e2075b60d2be8a","80132c8e4fa743fca850936ecfebc7f7","303bb3d75f7d4e94aeb60c5491ea6e61","d7463faafecf4e46a87dc6863a646cea","bc199cddba714aeda650d97fef015a14","1a508a6457bb460ba17d5adb0a9e9f85","3aac2656907a416291a622717ccaf929","fa1af70d9c95443c9f09666359ba3769","ddb717fd4dbc40c6bd8422a02f925060","6d1342eeaf4f4f0489fe0746ceaaeb09","a10683e5c1164f349cbdc75b1567994c","71cfe2c8df474badb255d7d28da04348","077fbb403e5f4069841e558a3cc0c065","b068b9fac9f24eca9bd430bab30ea70c","8cbfc6f4ec434674ac59d3fbdbddcd3b","4d675a6788b641c6a09604ef17514dec","bd9b9f21be9744c49d99ac4bc76f11e1","89469ab4bd6f48d8b9aa369473c7230f"]},"id":"77ecdd17","outputId":"602e648b-4a75-424b-da09-d58f3295a65e","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:17:11.654941Z","iopub.execute_input":"2025-12-01T10:17:11.655133Z","iopub.status.idle":"2025-12-01T10:17:12.776158Z","shell.execute_reply.started":"2025-12-01T10:17:11.655119Z","shell.execute_reply":"2025-12-01T10:17:12.775472Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/2 shards):   0%|          | 0/233583 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d32dbff58bfa4f21a5c081d9f3c1713a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/6317 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd1868cb580149c98e977927e161e980"}},"metadata":{}}],"execution_count":92},{"id":"cf7177d8-2213-48b8-bb88-9b0093c952bc","cell_type":"code","source":"import random\nimport pandas as pd\n\n# Set display options to ensure Urdu text is readable\npd.set_option('display.max_colwidth', None)\n\ndef verify_training_data(dataset, original_dataset, tokenizer, num_samples=1000):\n    print(f\"ğŸ” Auditing {num_samples} random training samples...\")\n    \n    # Filter for samples that actually HAVE an answer marked (start_positions > 0)\n    # We want to check if the non-zero labels are correct.\n    indices_with_answers = [i for i, x in enumerate(dataset[\"start_positions\"]) if x > 0]\n    \n    if not indices_with_answers:\n        print(\"âš ï¸ No samples with answers found! (Is the dataset all [CLS] labels?)\")\n        return\n\n    sample_indices = random.sample(indices_with_answers, min(num_samples, len(indices_with_answers)))\n    \n    results = []\n    \n    for idx in sample_indices:\n        # 1. Get the Processed Data (What the model sees)\n        proc = dataset[idx]\n        input_ids = proc[\"input_ids\"]\n        start = proc[\"start_positions\"]\n        end = proc[\"end_positions\"]\n        \n        # 2. Get the Original Data (The Truth)\n        orig_idx = proc[\"overflow_to_sample_mapping\"]\n        orig = original_dataset[orig_idx]\n        gold_answer = orig[\"answer\"]\n        question = orig[\"question\"]\n        \n        # 3. Decode the Label Span\n        # We perform the exact slice the model is being taught to predict\n        label_ids = input_ids[start : end + 1]\n        decoded_label = tokenizer.decode(label_ids, skip_special_tokens=True)\n        \n        # 4. Compare\n        # Strip whitespace for fair comparison\n        is_match = (decoded_label.strip() == gold_answer.strip())\n        \n        results.append({\n            \"Original Index\": orig_idx,\n            \"Question\": question,\n            \"Gold Answer\": gold_answer,\n            \"Label Decoded (Model Target)\": decoded_label,\n            \"Status\": \"Match\" if is_match else \"âŒ FAIL\"\n        })\n\n    # Display Table\n    df = pd.DataFrame(results)\n    display(df)\n    \n    # Summary\n    pass_count = len(df[df[\"Status\"] == \"Match\"])\n    print(f\"Pass: {pass_count} / {num_samples}\")\n    \n\n# Run the verification\nverify_training_data(processed_train, uqa_train, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:17:12.777017Z","iopub.execute_input":"2025-12-01T10:17:12.777248Z","iopub.status.idle":"2025-12-01T10:17:16.400011Z","shell.execute_reply.started":"2025-12-01T10:17:12.777228Z","shell.execute_reply":"2025-12-01T10:17:16.399177Z"}},"outputs":[{"name":"stdout","text":"ğŸ” Auditing 1000 random training samples...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"     Original Index  \\\n0             74484   \n1             33643   \n2              2320   \n3             39786   \n4             77396   \n..              ...   \n995           16843   \n996           56850   \n997           55479   \n998           44542   \n999           16232   \n\n                                                               Question  \\\n0                                           Ù…Ø¯Ø±Ø³Û Ù„ÙØ¸ Ú©ÛŒ Ø§Ø¨ØªØ¯Ø§Ø¡ Ú©ÛŒØ§ ÛÛ’ØŸ   \n1                         \" Ø³ÛŒØ§Û ÙØ§Ù…\" Ù„ÙˆÚ¯ÙˆÚº Ú©ÛŒ Ø§ØµØ·Ù„Ø§Ø­ Ù…ÛŒÚº Ú©ÙˆÙ† Ø´Ø§Ù…Ù„ ÛÛŒÚºØŸ   \n2                  Ø§Ù¹Ù„Ø§Ù†Ù¹Ú© Ø³Ù¹ÛŒ Ú©Û’ Ø³Ø§Ø­Ù„ Ù¾Ø± ÙˆØ§Ù‚Ø¹ Ù„Ø§Ø¦Ù¹ ÛØ§Ø¤Ø³ Ú©Ø§ Ù†Ø§Ù… Ú©ÛŒØ§ ÛÛ’ØŸ   \n3                                           Ø´ÙˆÙ¾Ù† ÙˆØ§Ø±Ø³Ø§ Ù…ÛŒÚº Ú©Ø¨ ÙˆØ§Ù¾Ø³ Ø¢ÛŒØ§ØŸ   \n4                                             Ø§ÙˆÙ„Ù…Ù¾Ú© Ú©Ú¾ÛŒÙ„ Ú©Ø¨ Ø´Ø±ÙˆØ¹ ÛÙˆØ¦Û’ØŸ   \n..                                                                  ...   \n995  Ú©ÙˆÙ† Ø³ÛŒ Ù‚Ø³Ù… Ú©ÛŒ ÙˆØ±Ø²Ø´ Ø¯Ù…Ø§Øº Ú©Û’ Ù„ÛŒÛ’ Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û ÙØ§Ø¦Ø¯Û Ù…Ù†Ø¯ Ø«Ø§Ø¨Øª ÛÙˆØ¦ÛŒ ÛÛ’ØŸ   \n996                               Ù…Ø§Ø¦Ú©Ø±ÙˆØ³Ú©ÙˆÙ¾ÛŒ Ú©Ø¨ Ø§Ù†ØªÛØ§Ø¦ÛŒ Ù…Ø®ØµÙˆØµ ÛÙˆØªÛŒ ÛÛ’ØŸ   \n997                                               ÚˆÛŒ Ø³ÛŒ Ú©Ø§ Ù…Ø®ÙÙ Ú©ÛŒØ§ ÛÛ’ØŸ   \n998                            Ù„ÛŒÙ…Ø¨ Ú©Ø§ Ø¢Ø¦ÛŒÙ†Û Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ú©ÛŒØ§ Ù†Ø¸Ø±ÛŒÛ ØªÚ¾Ø§ØŸ   \n999   Ù…ØµØ± Ú©Û’ ØµØ¯Ø§Ø±ØªÛŒ Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª Ù…ÛŒÚº Ø³ÛŒØ³ÛŒ Ú©Û’ Ø¨Ø¹Ø¯ Ø¯ÙˆØ³Ø±Ø§ Ù†Ù…Ø¨Ø± Ú©Ø³ Ù†Û’ Ø­Ø§ØµÙ„ Ú©ÛŒØ§ØŸ   \n\n                                                         Gold Answer  \\\n0                                                               Ø¹Ø±Ø¨ÛŒ   \n1                                         Ø§ÙØ±ÛŒÙ‚ÛŒ ØŒ Ø±Ù†Ú¯ÛŒÙ† Ø§ÙˆØ± Ø§ÛŒØ´ÛŒØ§Ø¦ÛŒ   \n2                                                  Ø§ÛŒØ¨Ø³ÛŒÚ©Ù† Ù„Ø§Ø¦Ù¹ ÛØ§Ø¤Ø³   \n3                                                         Ø³ØªÙ…Ø¨Ø± 1829   \n4                                                       776 Ù‚Ø¨Ù„ Ù…Ø³ÛŒØ­   \n..                                                               ...   \n995                                                     Ø§ÛŒØ±ÙˆØ¨Ú© Ù…Ø´Ù‚ÛŒÚº   \n996  Ø¬Ø¨ Ø§ÛŒÙ†Ù¹ÛŒ Ø¨Ø§ÚˆÛŒ Ù¾Ø± Ù…Ø¨Ù†ÛŒ ØªÚ©Ù†ÛŒÚ© Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ù„ Ú©Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’Û”    \n997                                                     ÚˆØ§Ø¦Ø±ÛŒÚ©Ù¹ Ú©Ø±Ù†Ù¹   \n998                                                           Ø®Ø·Ø±Ù†Ø§Ú©   \n999                                                       Ø­Ù…Ø¯ÛŒÙ† Ø³Ø¨Ø­ÛŒ   \n\n                                        Label Decoded (Model Target) Status  \n0                                                               Ø¹Ø±Ø¨ÛŒ  Match  \n1                                         Ø§ÙØ±ÛŒÙ‚ÛŒ ØŒ Ø±Ù†Ú¯ÛŒÙ† Ø§ÙˆØ± Ø§ÛŒØ´ÛŒØ§Ø¦ÛŒ  Match  \n2                                                  Ø§ÛŒØ¨Ø³ÛŒÚ©Ù† Ù„Ø§Ø¦Ù¹ ÛØ§Ø¤Ø³  Match  \n3                                                         Ø³ØªÙ…Ø¨Ø± 1829  Match  \n4                                                       776 Ù‚Ø¨Ù„ Ù…Ø³ÛŒØ­  Match  \n..                                                               ...    ...  \n995                                                     Ø§ÛŒØ±ÙˆØ¨Ú© Ù…Ø´Ù‚ÛŒÚº  Match  \n996  Ø¬Ø¨ Ø§ÛŒÙ†Ù¹ÛŒ Ø¨Ø§ÚˆÛŒ Ù¾Ø± Ù…Ø¨Ù†ÛŒ ØªÚ©Ù†ÛŒÚ© Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ù„ Ú©Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’Û”   Match  \n997                                                     ÚˆØ§Ø¦Ø±ÛŒÚ©Ù¹ Ú©Ø±Ù†Ù¹  Match  \n998                                                           Ø®Ø·Ø±Ù†Ø§Ú©  Match  \n999                                                       Ø­Ù…Ø¯ÛŒÙ† Ø³Ø¨Ø­ÛŒ  Match  \n\n[1000 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Original Index</th>\n      <th>Question</th>\n      <th>Gold Answer</th>\n      <th>Label Decoded (Model Target)</th>\n      <th>Status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>74484</td>\n      <td>Ù…Ø¯Ø±Ø³Û Ù„ÙØ¸ Ú©ÛŒ Ø§Ø¨ØªØ¯Ø§Ø¡ Ú©ÛŒØ§ ÛÛ’ØŸ</td>\n      <td>Ø¹Ø±Ø¨ÛŒ</td>\n      <td>Ø¹Ø±Ø¨ÛŒ</td>\n      <td>Match</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>33643</td>\n      <td>\" Ø³ÛŒØ§Û ÙØ§Ù…\" Ù„ÙˆÚ¯ÙˆÚº Ú©ÛŒ Ø§ØµØ·Ù„Ø§Ø­ Ù…ÛŒÚº Ú©ÙˆÙ† Ø´Ø§Ù…Ù„ ÛÛŒÚºØŸ</td>\n      <td>Ø§ÙØ±ÛŒÙ‚ÛŒ ØŒ Ø±Ù†Ú¯ÛŒÙ† Ø§ÙˆØ± Ø§ÛŒØ´ÛŒØ§Ø¦ÛŒ</td>\n      <td>Ø§ÙØ±ÛŒÙ‚ÛŒ ØŒ Ø±Ù†Ú¯ÛŒÙ† Ø§ÙˆØ± Ø§ÛŒØ´ÛŒØ§Ø¦ÛŒ</td>\n      <td>Match</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2320</td>\n      <td>Ø§Ù¹Ù„Ø§Ù†Ù¹Ú© Ø³Ù¹ÛŒ Ú©Û’ Ø³Ø§Ø­Ù„ Ù¾Ø± ÙˆØ§Ù‚Ø¹ Ù„Ø§Ø¦Ù¹ ÛØ§Ø¤Ø³ Ú©Ø§ Ù†Ø§Ù… Ú©ÛŒØ§ ÛÛ’ØŸ</td>\n      <td>Ø§ÛŒØ¨Ø³ÛŒÚ©Ù† Ù„Ø§Ø¦Ù¹ ÛØ§Ø¤Ø³</td>\n      <td>Ø§ÛŒØ¨Ø³ÛŒÚ©Ù† Ù„Ø§Ø¦Ù¹ ÛØ§Ø¤Ø³</td>\n      <td>Match</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>39786</td>\n      <td>Ø´ÙˆÙ¾Ù† ÙˆØ§Ø±Ø³Ø§ Ù…ÛŒÚº Ú©Ø¨ ÙˆØ§Ù¾Ø³ Ø¢ÛŒØ§ØŸ</td>\n      <td>Ø³ØªÙ…Ø¨Ø± 1829</td>\n      <td>Ø³ØªÙ…Ø¨Ø± 1829</td>\n      <td>Match</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>77396</td>\n      <td>Ø§ÙˆÙ„Ù…Ù¾Ú© Ú©Ú¾ÛŒÙ„ Ú©Ø¨ Ø´Ø±ÙˆØ¹ ÛÙˆØ¦Û’ØŸ</td>\n      <td>776 Ù‚Ø¨Ù„ Ù…Ø³ÛŒØ­</td>\n      <td>776 Ù‚Ø¨Ù„ Ù…Ø³ÛŒØ­</td>\n      <td>Match</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>16843</td>\n      <td>Ú©ÙˆÙ† Ø³ÛŒ Ù‚Ø³Ù… Ú©ÛŒ ÙˆØ±Ø²Ø´ Ø¯Ù…Ø§Øº Ú©Û’ Ù„ÛŒÛ’ Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û ÙØ§Ø¦Ø¯Û Ù…Ù†Ø¯ Ø«Ø§Ø¨Øª ÛÙˆØ¦ÛŒ ÛÛ’ØŸ</td>\n      <td>Ø§ÛŒØ±ÙˆØ¨Ú© Ù…Ø´Ù‚ÛŒÚº</td>\n      <td>Ø§ÛŒØ±ÙˆØ¨Ú© Ù…Ø´Ù‚ÛŒÚº</td>\n      <td>Match</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>56850</td>\n      <td>Ù…Ø§Ø¦Ú©Ø±ÙˆØ³Ú©ÙˆÙ¾ÛŒ Ú©Ø¨ Ø§Ù†ØªÛØ§Ø¦ÛŒ Ù…Ø®ØµÙˆØµ ÛÙˆØªÛŒ ÛÛ’ØŸ</td>\n      <td>Ø¬Ø¨ Ø§ÛŒÙ†Ù¹ÛŒ Ø¨Ø§ÚˆÛŒ Ù¾Ø± Ù…Ø¨Ù†ÛŒ ØªÚ©Ù†ÛŒÚ© Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ù„ Ú©Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’Û”</td>\n      <td>Ø¬Ø¨ Ø§ÛŒÙ†Ù¹ÛŒ Ø¨Ø§ÚˆÛŒ Ù¾Ø± Ù…Ø¨Ù†ÛŒ ØªÚ©Ù†ÛŒÚ© Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ù„ Ú©Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’Û”</td>\n      <td>Match</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>55479</td>\n      <td>ÚˆÛŒ Ø³ÛŒ Ú©Ø§ Ù…Ø®ÙÙ Ú©ÛŒØ§ ÛÛ’ØŸ</td>\n      <td>ÚˆØ§Ø¦Ø±ÛŒÚ©Ù¹ Ú©Ø±Ù†Ù¹</td>\n      <td>ÚˆØ§Ø¦Ø±ÛŒÚ©Ù¹ Ú©Ø±Ù†Ù¹</td>\n      <td>Match</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>44542</td>\n      <td>Ù„ÛŒÙ…Ø¨ Ú©Ø§ Ø¢Ø¦ÛŒÙ†Û Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ú©ÛŒØ§ Ù†Ø¸Ø±ÛŒÛ ØªÚ¾Ø§ØŸ</td>\n      <td>Ø®Ø·Ø±Ù†Ø§Ú©</td>\n      <td>Ø®Ø·Ø±Ù†Ø§Ú©</td>\n      <td>Match</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>16232</td>\n      <td>Ù…ØµØ± Ú©Û’ ØµØ¯Ø§Ø±ØªÛŒ Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª Ù…ÛŒÚº Ø³ÛŒØ³ÛŒ Ú©Û’ Ø¨Ø¹Ø¯ Ø¯ÙˆØ³Ø±Ø§ Ù†Ù…Ø¨Ø± Ú©Ø³ Ù†Û’ Ø­Ø§ØµÙ„ Ú©ÛŒØ§ØŸ</td>\n      <td>Ø­Ù…Ø¯ÛŒÙ† Ø³Ø¨Ø­ÛŒ</td>\n      <td>Ø­Ù…Ø¯ÛŒÙ† Ø³Ø¨Ø­ÛŒ</td>\n      <td>Match</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows Ã— 5 columns</p>\n</div>"},"metadata":{}},{"name":"stdout","text":"Pass: 1000 / 1000\n","output_type":"stream"}],"execution_count":93},{"id":"96e0bb83-3f8d-497a-8d1c-76b139a21016","cell_type":"markdown","source":"### Checking how many rows have unanswerable questions...","metadata":{}},{"id":"84843ca5-fa97-4acc-b9bc-0e76054f5fe2","cell_type":"code","source":"if 'processed_train' not in globals():\n    try:\n        print(\"Loading processed training set from disk...\")\n\n        processed_train = load_from_disk(\"/kaggle/working/cache/processed_train_uqa\")\n    except Exception as e:\n        print(f\"Error loading data: {e}\")\n        print(\"Please ensure 'processed_val' is defined or the path is correct.\")\n\n\nprint(f\"Analyzing {len(processed_train)} training samples...\")\n\nunanswerable_count = 0\ntotal_count = len(processed_train)\n\nfor i in range(total_count):\n\n    if processed_train[i][\"start_positions\"] == 0 and processed_train[i][\"end_positions\"] == 0:\n        unanswerable_count += 1\n\npercentage = (unanswerable_count / total_count) * 100\n\n\nprint(\"-\" * 40)\nprint(f\"Total Samples:       {total_count}\")\nprint(f\"Unanswerable ([CLS]): {unanswerable_count}\")\nprint(f\"Baseline Percentage:  {percentage:.2f}%\")\nprint(\"-\" * 40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:17:16.400897Z","iopub.execute_input":"2025-12-01T10:17:16.401098Z","iopub.status.idle":"2025-12-01T10:20:35.423000Z","shell.execute_reply.started":"2025-12-01T10:17:16.401083Z","shell.execute_reply":"2025-12-01T10:20:35.422147Z"}},"outputs":[{"name":"stdout","text":"Analyzing 233583 training samples...\n----------------------------------------\nTotal Samples:       233583\nUnanswerable ([CLS]): 174798\nBaseline Percentage:  74.83%\n----------------------------------------\n","output_type":"stream"}],"execution_count":94},{"id":"874eb5bc-a94e-42b4-bfd2-8cb396c0a16c","cell_type":"code","source":"if 'processed_val' not in globals():\n    try:\n        print(\"Loading processed validation set from disk...\")\n\n        processed_val = load_from_disk(\"/kaggle/working/cache/processed_val_uqa\")\n    except Exception as e:\n        print(f\"Error loading data: {e}\")\n        print(\"Please ensure 'processed_val' is defined or the path is correct.\")\n\n\nprint(f\"Analyzing {len(processed_val)} validation samples...\")\n\nunanswerable_count = 0\ntotal_count = len(processed_val)\n\nfor i in range(total_count):\n\n    if processed_val[i][\"start_positions\"] == 0 and processed_val[i][\"end_positions\"] == 0:\n        unanswerable_count += 1\n\npercentage = (unanswerable_count / total_count) * 100\n\nprint(\"-\" * 40)\nprint(f\"Total Samples:       {total_count}\")\nprint(f\"Unanswerable ([CLS]): {unanswerable_count}\")\nprint(f\"Baseline Percentage:  {percentage:.2f}%\")\nprint(\"-\" * 40)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:20:35.425713Z","iopub.execute_input":"2025-12-01T10:20:35.426158Z","iopub.status.idle":"2025-12-01T10:20:40.673703Z","shell.execute_reply.started":"2025-12-01T10:20:35.426139Z","shell.execute_reply":"2025-12-01T10:20:40.672997Z"}},"outputs":[{"name":"stdout","text":"Analyzing 6317 validation samples...\n----------------------------------------\nTotal Samples:       6317\nUnanswerable ([CLS]): 4834\nBaseline Percentage:  76.52%\n----------------------------------------\n","output_type":"stream"}],"execution_count":95},{"id":"68378d49-1b73-4070-8eab-b90e9ea4fe75","cell_type":"markdown","source":"There is a **severe** class imbalance issue here. Aorund 75% of the data is unanswerable!\n\nNeed to downsample the training dataset...","metadata":{}},{"id":"aab916f9-9772-4109-b976-621168958cf6","cell_type":"code","source":"from datasets import concatenate_datasets\n\ndef create_balanced_dataset(dataset):\n    print(f\"Original Training Size: {len(dataset)}\")\n    \n    # 1. Split into Positive (Has Answer) and Negative (No Answer)\n    # In SQuAD preprocessing, start_positions=0 indicates [CLS] / No Answer\n    positives = dataset.filter(lambda x: x[\"start_positions\"] > 0)\n    negatives = dataset.filter(lambda x: x[\"start_positions\"] == 0)\n    \n    n_pos = len(positives)\n    n_neg = len(negatives)\n    \n    print(f\"  - Answerable Samples:   {n_pos}\")\n    print(f\"  - Unanswerable Samples: {n_neg}\")\n    \n    # 2. Downsample Negatives to match Positives (1:1 Ratio)\n    # We shuffle first to get a random distribution\n    negatives = negatives.shuffle(seed=42)\n    \n    # Select only as many negatives as we have positives\n    negatives_balanced = negatives.select(range(n_pos))\n    \n    print(f\"  - Negatives Kept:       {len(negatives_balanced)}\")\n    \n    # 3. Combine and Shuffle\n    balanced_dataset = concatenate_datasets([positives, negatives_balanced])\n    balanced_dataset = balanced_dataset.shuffle(seed=42)\n    \n    print(f\"Balanced Training Size: {len(balanced_dataset)}\")\n    \n    return balanced_dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:20:40.674763Z","iopub.execute_input":"2025-12-01T10:20:40.674990Z","iopub.status.idle":"2025-12-01T10:20:40.680618Z","shell.execute_reply.started":"2025-12-01T10:20:40.674972Z","shell.execute_reply":"2025-12-01T10:20:40.680012Z"}},"outputs":[],"execution_count":96},{"id":"395bf81d-682f-4b64-bd59-94035fcf4f94","cell_type":"code","source":"balanced_train = create_balanced_dataset(processed_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:20:40.681248Z","iopub.execute_input":"2025-12-01T10:20:40.682289Z","iopub.status.idle":"2025-12-01T10:20:40.727450Z","shell.execute_reply.started":"2025-12-01T10:20:40.682265Z","shell.execute_reply":"2025-12-01T10:20:40.726912Z"}},"outputs":[{"name":"stdout","text":"Original Training Size: 233583\n  - Answerable Samples:   58785\n  - Unanswerable Samples: 174798\n  - Negatives Kept:       58785\nBalanced Training Size: 117570\n","output_type":"stream"}],"execution_count":97},{"id":"090f6b3d-88e3-4b42-b02b-607abde1c0b2","cell_type":"markdown","source":"---","metadata":{}},{"id":"81bdd5c7-3158-4436-909e-cdee5e89bc95","cell_type":"code","source":"balanced_train.save_to_disk(\"/kaggle/working/cache/balanced_train_uqa\")   # cached it\n\n\nbalanced_train = load_from_disk(\"/kaggle/working/cache/balanced_train_uqa\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:20:40.728274Z","iopub.execute_input":"2025-12-01T10:20:40.728466Z","iopub.status.idle":"2025-12-01T10:20:42.346776Z","shell.execute_reply.started":"2025-12-01T10:20:40.728451Z","shell.execute_reply":"2025-12-01T10:20:42.345501Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/117570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5623400a121472ea2fb3ea06ef3a623"}},"metadata":{}}],"execution_count":98},{"id":"14","cell_type":"code","source":"def print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n\n","metadata":{"id":"c0e06e6b","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:20:42.349110Z","iopub.execute_input":"2025-12-01T10:20:42.349463Z","iopub.status.idle":"2025-12-01T10:20:42.354419Z","shell.execute_reply.started":"2025-12-01T10:20:42.349431Z","shell.execute_reply":"2025-12-01T10:20:42.353843Z"}},"outputs":[],"execution_count":99},{"id":"15","cell_type":"code","source":"# build LoRA model\n\npeft_model = get_peft_model(model, lora_config)\npeft_model.gradient_checkpointing_enable()\nprint_trainable_parameters(peft_model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ba9eeeed","outputId":"27071b6e-b703-4b47-9288-9a1c6f3eba55","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:20:42.355179Z","iopub.execute_input":"2025-12-01T10:20:42.355376Z","iopub.status.idle":"2025-12-01T10:20:42.423084Z","shell.execute_reply.started":"2025-12-01T10:20:42.355361Z","shell.execute_reply":"2025-12-01T10:20:42.422385Z"}},"outputs":[{"name":"stdout","text":"trainable params: 1033730 || all params: 133118212 || trainable%: 0.7765503941714602\n","output_type":"stream"}],"execution_count":100},{"id":"16","cell_type":"code","source":"def normalize_answer(text):\n    text = (text or \"\").lower()\n    def remove_articles(s):\n        return re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n    def remove_punctuation(s):\n        return \"\".join(ch for ch in s if ch not in string.punctuation)\n    def white_space_fix(s):\n        return \" \".join(s.split())\n    return white_space_fix(remove_articles(remove_punctuation(text)))\n\ndef exact_match_score(prediction, ground_truth):\n    return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n\ndef f1_score(prediction, ground_truth):\n    pred_tokens = normalize_answer(prediction).split()\n    gold_tokens = normalize_answer(ground_truth).split()\n    if not gold_tokens:\n        return 1.0 if not pred_tokens else 0.0\n    if not pred_tokens:\n        return 0.0\n    common = Counter(pred_tokens) & Counter(gold_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0.0\n    precision = num_same / len(pred_tokens)\n    recall = num_same / len(gold_tokens)\n    return 2 * precision * recall / (precision + recall)\n\ndef decode_prediction(input_ids, start_idx, end_idx, tokenizer):\n    cls_index = input_ids.index(tokenizer.cls_token_id)\n    if start_idx == cls_index and end_idx == cls_index:\n        return \"\" # Empty answer\n    if start_idx > end_idx:\n        return \"\" # Invalid range -> Empty answer\n    \n    # Ensure indices are within bounds\n    start_idx = max(start_idx, 0)\n    end_idx = min(end_idx, len(input_ids) - 1)\n    \n    text = tokenizer.decode(input_ids[start_idx:end_idx + 1], skip_special_tokens=True)\n    return text.strip()\n\ndef gold_answer(example):\n    if example[\"answer_start\"] == -1:\n        return \"\"\n    return example[\"answer\"]\n\ndef edit_distance_score(prediction, ground_truth):\n    return Levenshtein.ratio(normalize_answer(prediction), normalize_answer(ground_truth))\n\n\ndef evaluate_checkpoint(checkpoint_path=None, model_instance=None, eval_dataset=None):\n    \"\"\"Evaluate either a checkpoint path (loads model) or a provided model instance.\n\n    - checkpoint_path: path to checkpoint folder\n    - model_instance: an in-memory model (preferably a PeftModel or CanineForQuestionAnswering)\n    - eval_dataset: optional dataset to evaluate; if None the default processed_val will be used\n    \"\"\"\n    if eval_dataset is None:\n        eval_dataset = processed_val\n\n    # If a model_instance is given, use it directly (avoid re-loading a fresh base model)\n    if model_instance is not None:\n        eval_model = model_instance\n    else:\n        base_model = CanineForQuestionAnswering.from_pretrained(model_name, trust_remote_code=False)\n        eval_model = get_peft_model(base_model, lora_config)\n        # Try loading adapter weights; fall back to PeftModel.from_pretrained if needed\n        try:\n            eval_model.load_adapter(checkpoint_path)\n        except Exception:\n            from peft import PeftModel\n            eval_model = PeftModel.from_pretrained(base_model, checkpoint_path)\n\n    eval_model.to(device)\n\n    eval_args = TrainingArguments(\n        # Small evaluation config; uses cpu/mps if no gpu during eval\n        output_dir=\"outputs/canine-s-uqa\",\n        per_device_eval_batch_size=16,\n        dataloader_drop_last=False,\n        fp16=True,\n        bf16=False,\n        report_to=\"none\",\n    )\n\n    # Run evaluation via a lightweight Trainer so prediction loop is standard\n    eval_trainer = Trainer(\n        model=eval_model,\n        args=eval_args,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n    )\n\n    predictions = eval_trainer.predict(eval_dataset)\n    start_logits, end_logits = predictions.predictions\n    best_predictions = {}\n    for feature_index, feature in enumerate(eval_dataset):\n        sample_idx = int(feature[\"overflow_to_sample_mapping\"])\n        input_ids = feature[\"input_ids\"]\n        start_idx = int(np.argmax(start_logits[feature_index]))\n        end_idx = int(np.argmax(end_logits[feature_index]))\n        score = float(start_logits[feature_index][start_idx] + end_logits[feature_index][end_idx])\n        prediction_text = decode_prediction(input_ids, start_idx, end_idx, tokenizer=tokenizer)\n        stored = best_predictions.get(sample_idx)\n        if stored is None or score > stored[0]:\n            best_predictions[sample_idx] = (score, prediction_text)\n\n    em_scores = []\n    f1_scores = []\n    edit_dist_scores = []\n    for sample_idx, (_, prediction_text) in best_predictions.items():\n        reference = gold_answer(uqa_val[int(sample_idx)])\n        em_scores.append(exact_match_score(prediction_text, reference))\n        f1_scores.append(f1_score(prediction_text, reference))\n        edit_dist_scores.append(edit_distance_score(prediction_text, reference))\n\n    em = float(np.mean(em_scores)) if em_scores else 0.0\n    f1 = float(np.mean(f1_scores)) if f1_scores else 0.0\n    edit_dist = float(np.mean(edit_dist_scores)) if edit_dist_scores else 0.0\n\n    # ---\n    print(\"\\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\")\n    print(f\"{'Question':<50} | {'Gold':<20} | {'Prediction':<20}\")\n    print(\"-\" * 100)\n    \n    # Print first 5 items from best_predictions to the console log\n    count = 0\n    for sample_idx, (score, pred_text) in best_predictions.items():\n        if count >= 5: break\n        # Get question text (you might need to store it or fetch from dataset)\n        raw_gold = gold_answer(uqa_val[int(sample_idx)])\n        print(f\"Sample #{sample_idx:<8} | {raw_gold:<20} | {pred_text:<20}\")\n        count += 1\n    print(\"-\" * 100 + \"\\n\")\n    # ---\n    \n    print(f\"Examples evaluated: {len(em_scores)}\")\n    print(f\"Exact Match: {em * 100:.2f}\")\n    print(f\"F1: {f1 * 100:.2f}\")\n    print(f\"Edit Distance (normalized): {edit_dist * 100:.2f}\")\n    return {\"exact_match\": em, \"f1\": f1, \"edit_distance\": edit_dist}\n","metadata":{"id":"61c5c7a7","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:20:42.423916Z","iopub.execute_input":"2025-12-01T10:20:42.424142Z","iopub.status.idle":"2025-12-01T10:20:42.439173Z","shell.execute_reply.started":"2025-12-01T10:20:42.424117Z","shell.execute_reply":"2025-12-01T10:20:42.438630Z"}},"outputs":[],"execution_count":101},{"id":"17","cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"outputs/canine-s-uqa\",\n\n    \n\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=16,\n\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n\n    num_train_epochs=2,\n    learning_rate=3e-4,     # trying a higher lr\n    weight_decay=0.00,      # trying 0 weight decay\n    lr_scheduler_type=\"constant\",   # CHANGED (Don't let LR decay to 0)\n    warmup_ratio=0.05,              # Small warmup for stability\n    \n    eval_strategy=\"no\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=500,\n    logging_steps=50,\n    fp16=True,\n    bf16=False,\n    report_to=\"none\",\n    push_to_hub=True,\n    hub_model_id=\"VohraAK/canine-s-uqa\",\n    hub_strategy=\"checkpoint\",\n    )\n\nclass CustomEvalCallback(TrainerCallback):\n    def __init__(self, eval_func, eval_dataset, use_in_memory_model=True, verbose=True):\n        self.eval_func = eval_func\n        self.eval_dataset = eval_dataset\n        self.use_in_memory_model = use_in_memory_model\n        self.verbose = verbose\n        # trainer reference (set after trainer exists)\n        self.trainer = None\n\n    def on_save(self, args, state, control, model=None, **kwargs):\n        checkpoint_path = f\"{args.output_dir}/checkpoint-{state.global_step}\"\n        if self.verbose:\n            print(f\"\\nğŸ” Running custom evaluation at step {state.global_step}...\")\n\n        # Prefer evaluating the in-memory trainer model (fast + avoids re-loading)\n        if self.use_in_memory_model and self.trainer is not None:\n            if self.verbose:\n                print(\"Using in-memory model for evaluation (no reloading).\")\n            try:\n                metrics = self.eval_func(checkpoint_path=None, model_instance=self.trainer.model, eval_dataset=self.eval_dataset)\n            except Exception as e:\n                print(\"âš ï¸ in-memory evaluation failed, falling back to checkpoint load:\", e)\n                metrics = self.eval_func(checkpoint_path)\n        else:\n            metrics = self.eval_func(checkpoint_path)\n\n        # record metrics in state.log_history\n        state.log_history.append({\n            \"step\": state.global_step,\n            \"eval_exact_match\": metrics.get(\"exact_match\"),\n            \"eval_f1\": metrics.get(\"f1\"),\n            \"eval_edit_distance\": metrics.get(\"edit_distance\"),\n        })\n\n        if self.verbose:\n            print(f\"âœ… Step {state.global_step}: EM={metrics.get('exact_match',0)*100:.2f}, F1={metrics.get('f1',0)*100:.2f}, EditDist={metrics.get('edit_distance',0)*100:.2f}\")\n\n        # Update trainer_state.json to include custom metrics\n        state_path = f\"{checkpoint_path}/trainer_state.json\"\n        try:\n            with open(state_path, 'r') as f:\n                state_dict = json.load(f)\n            state_dict['log_history'] = state.log_history\n            with open(state_path, 'w') as f:\n                json.dump(state_dict, f, indent=2)\n            if self.verbose:\n                print(f\"ğŸ’¾ Updated trainer_state.json with custom metrics\")\n        except Exception as e:\n            if self.verbose:\n                print(f\"âš ï¸  Warning: Could not update trainer_state.json: {e}\")\n\n        try:\n            if self.verbose:\n                print(f\"â˜ï¸  Pushing checkpoint-{state.global_step} to Hub...\")\n            api = HfApi()\n            api.upload_folder(\n                folder_path=checkpoint_path,\n                repo_id=args.hub_model_id,\n                path_in_repo=f\"checkpoint-{state.global_step}\",\n                commit_message=f\"Add checkpoint {state.global_step} (EM={metrics.get('exact_match',0)*100:.1f}%, F1={metrics.get('f1',0)*100:.1f}%)\",\n                repo_type=\"model\"\n            )\n            if self.verbose:\n                print(f\"âœ… Pushed checkpoint-{state.global_step} to Hub\")\n        except Exception as e:\n            if self.verbose:\n                print(f\"âš ï¸  Warning: Could not push to Hub: {e}\")\n\n        return control","metadata":{"id":"c4abaaab","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:20:42.439929Z","iopub.execute_input":"2025-12-01T10:20:42.440172Z","iopub.status.idle":"2025-12-01T10:20:42.485136Z","shell.execute_reply.started":"2025-12-01T10:20:42.440150Z","shell.execute_reply":"2025-12-01T10:20:42.484572Z"}},"outputs":[],"execution_count":102},{"id":"18","cell_type":"code","source":"trainer_cb = CustomEvalCallback(evaluate_checkpoint, processed_val, use_in_memory_model=True)\n\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=balanced_train,\n    eval_dataset=processed_val,\n    callbacks=[trainer_cb],\n)\n","metadata":{"id":"055f5dda","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:20:42.485854Z","iopub.execute_input":"2025-12-01T10:20:42.486086Z","iopub.status.idle":"2025-12-01T10:20:42.799003Z","shell.execute_reply.started":"2025-12-01T10:20:42.486066Z","shell.execute_reply":"2025-12-01T10:20:42.798420Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":103},{"id":"19","cell_type":"code","source":"trainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"TOUimesUX5Re","outputId":"cfa62dcd-8eb4-475a-910b-1c38a3894cc2","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T10:20:42.799793Z","iopub.execute_input":"2025-12-01T10:20:42.800000Z","iopub.status.idle":"2025-12-01T11:12:56.324011Z","shell.execute_reply.started":"2025-12-01T10:20:42.799984Z","shell.execute_reply":"2025-12-01T11:12:56.322777Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10678' max='14698' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10678/14698 52:12 < 19:39, 3.41 it/s, Epoch 1.45/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>5.808600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>5.547500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>5.377200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>5.228000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>5.089500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>4.927800</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>4.830500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>4.735100</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>4.766400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>4.620400</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>4.526100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>4.329500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>4.384000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>4.417400</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>4.378100</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>4.215300</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>4.281600</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>4.162900</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>4.107000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>4.116800</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>4.033000</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>4.010900</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>4.116100</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>3.943400</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>4.119400</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>4.040000</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>3.843600</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>3.980500</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>3.990000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>3.912800</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>3.900600</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>3.761700</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>3.798500</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>3.854100</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>3.817400</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>3.709500</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>3.797100</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>3.795900</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>3.701500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>3.736900</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>3.723800</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>3.865000</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>3.629700</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>3.763600</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>3.768100</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>3.683100</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>3.595800</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>3.711600</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>3.643600</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>3.634800</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>3.741200</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>3.631600</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>3.574100</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>3.603100</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>3.688800</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>3.606200</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>3.638700</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>3.647400</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>3.566700</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>3.617800</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>3.514600</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>3.488000</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>3.487200</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>3.617400</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>3.660300</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>3.527300</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>3.578000</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>3.603900</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>3.629400</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>3.617100</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>3.621700</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>3.462900</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>3.553200</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>3.444200</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>3.499100</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>3.527500</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>3.597000</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>3.470400</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>3.558500</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>3.528200</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>3.594300</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>3.477600</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>3.408000</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>3.465600</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>3.368800</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>3.595600</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>3.543300</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>3.527200</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>3.532700</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>3.425500</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>3.516300</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>3.519700</td>\n    </tr>\n    <tr>\n      <td>4650</td>\n      <td>3.652600</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>3.391400</td>\n    </tr>\n    <tr>\n      <td>4750</td>\n      <td>3.417600</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>3.611800</td>\n    </tr>\n    <tr>\n      <td>4850</td>\n      <td>3.411900</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>3.409600</td>\n    </tr>\n    <tr>\n      <td>4950</td>\n      <td>3.397600</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>3.427100</td>\n    </tr>\n    <tr>\n      <td>5050</td>\n      <td>3.377300</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>3.466100</td>\n    </tr>\n    <tr>\n      <td>5150</td>\n      <td>3.465300</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>3.379500</td>\n    </tr>\n    <tr>\n      <td>5250</td>\n      <td>3.456600</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>3.431100</td>\n    </tr>\n    <tr>\n      <td>5350</td>\n      <td>3.624600</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>3.462200</td>\n    </tr>\n    <tr>\n      <td>5450</td>\n      <td>3.314800</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>3.472000</td>\n    </tr>\n    <tr>\n      <td>5550</td>\n      <td>3.456700</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>3.334900</td>\n    </tr>\n    <tr>\n      <td>5650</td>\n      <td>3.430500</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>3.448500</td>\n    </tr>\n    <tr>\n      <td>5750</td>\n      <td>3.356000</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>3.518600</td>\n    </tr>\n    <tr>\n      <td>5850</td>\n      <td>3.444000</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>3.360400</td>\n    </tr>\n    <tr>\n      <td>5950</td>\n      <td>3.469800</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>3.338600</td>\n    </tr>\n    <tr>\n      <td>6050</td>\n      <td>3.252400</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>3.581400</td>\n    </tr>\n    <tr>\n      <td>6150</td>\n      <td>3.428600</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>3.436300</td>\n    </tr>\n    <tr>\n      <td>6250</td>\n      <td>3.371800</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>3.375900</td>\n    </tr>\n    <tr>\n      <td>6350</td>\n      <td>3.485300</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>3.403600</td>\n    </tr>\n    <tr>\n      <td>6450</td>\n      <td>3.193200</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>3.433400</td>\n    </tr>\n    <tr>\n      <td>6550</td>\n      <td>3.398700</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>3.363700</td>\n    </tr>\n    <tr>\n      <td>6650</td>\n      <td>3.520000</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>3.501000</td>\n    </tr>\n    <tr>\n      <td>6750</td>\n      <td>3.423400</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>3.406600</td>\n    </tr>\n    <tr>\n      <td>6850</td>\n      <td>3.471700</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>3.357300</td>\n    </tr>\n    <tr>\n      <td>6950</td>\n      <td>3.418200</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>3.390200</td>\n    </tr>\n    <tr>\n      <td>7050</td>\n      <td>3.513900</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>3.248800</td>\n    </tr>\n    <tr>\n      <td>7150</td>\n      <td>3.312300</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>3.434400</td>\n    </tr>\n    <tr>\n      <td>7250</td>\n      <td>3.250800</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>3.370300</td>\n    </tr>\n    <tr>\n      <td>7350</td>\n      <td>3.441300</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>3.328300</td>\n    </tr>\n    <tr>\n      <td>7450</td>\n      <td>3.329900</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>3.415700</td>\n    </tr>\n    <tr>\n      <td>7550</td>\n      <td>3.376800</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>3.475700</td>\n    </tr>\n    <tr>\n      <td>7650</td>\n      <td>3.361400</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>3.384400</td>\n    </tr>\n    <tr>\n      <td>7750</td>\n      <td>3.458000</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>3.498000</td>\n    </tr>\n    <tr>\n      <td>7850</td>\n      <td>3.391500</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>3.348800</td>\n    </tr>\n    <tr>\n      <td>7950</td>\n      <td>3.428800</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>3.403300</td>\n    </tr>\n    <tr>\n      <td>8050</td>\n      <td>3.403900</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>3.329400</td>\n    </tr>\n    <tr>\n      <td>8150</td>\n      <td>3.415500</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>3.361000</td>\n    </tr>\n    <tr>\n      <td>8250</td>\n      <td>3.282100</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>3.361900</td>\n    </tr>\n    <tr>\n      <td>8350</td>\n      <td>3.241600</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>3.463200</td>\n    </tr>\n    <tr>\n      <td>8450</td>\n      <td>3.330200</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>3.343500</td>\n    </tr>\n    <tr>\n      <td>8550</td>\n      <td>3.418900</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>3.469600</td>\n    </tr>\n    <tr>\n      <td>8650</td>\n      <td>3.428200</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>3.416700</td>\n    </tr>\n    <tr>\n      <td>8750</td>\n      <td>3.393600</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>3.260500</td>\n    </tr>\n    <tr>\n      <td>8850</td>\n      <td>3.316100</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>3.372000</td>\n    </tr>\n    <tr>\n      <td>8950</td>\n      <td>3.440300</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>3.315500</td>\n    </tr>\n    <tr>\n      <td>9050</td>\n      <td>3.370300</td>\n    </tr>\n    <tr>\n      <td>9100</td>\n      <td>3.418400</td>\n    </tr>\n    <tr>\n      <td>9150</td>\n      <td>3.478200</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>3.362600</td>\n    </tr>\n    <tr>\n      <td>9250</td>\n      <td>3.316400</td>\n    </tr>\n    <tr>\n      <td>9300</td>\n      <td>3.264600</td>\n    </tr>\n    <tr>\n      <td>9350</td>\n      <td>3.435200</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>3.287100</td>\n    </tr>\n    <tr>\n      <td>9450</td>\n      <td>3.265700</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>3.391100</td>\n    </tr>\n    <tr>\n      <td>9550</td>\n      <td>3.399500</td>\n    </tr>\n    <tr>\n      <td>9600</td>\n      <td>3.340500</td>\n    </tr>\n    <tr>\n      <td>9650</td>\n      <td>3.389000</td>\n    </tr>\n    <tr>\n      <td>9700</td>\n      <td>3.343000</td>\n    </tr>\n    <tr>\n      <td>9750</td>\n      <td>3.412900</td>\n    </tr>\n    <tr>\n      <td>9800</td>\n      <td>3.303400</td>\n    </tr>\n    <tr>\n      <td>9850</td>\n      <td>3.405600</td>\n    </tr>\n    <tr>\n      <td>9900</td>\n      <td>3.345600</td>\n    </tr>\n    <tr>\n      <td>9950</td>\n      <td>3.336700</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>3.316600</td>\n    </tr>\n    <tr>\n      <td>10050</td>\n      <td>3.364600</td>\n    </tr>\n    <tr>\n      <td>10100</td>\n      <td>3.374500</td>\n    </tr>\n    <tr>\n      <td>10150</td>\n      <td>3.273700</td>\n    </tr>\n    <tr>\n      <td>10200</td>\n      <td>3.307700</td>\n    </tr>\n    <tr>\n      <td>10250</td>\n      <td>3.486500</td>\n    </tr>\n    <tr>\n      <td>10300</td>\n      <td>3.449400</td>\n    </tr>\n    <tr>\n      <td>10350</td>\n      <td>3.423100</td>\n    </tr>\n    <tr>\n      <td>10400</td>\n      <td>3.377700</td>\n    </tr>\n    <tr>\n      <td>10450</td>\n      <td>3.224800</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>3.249800</td>\n    </tr>\n    <tr>\n      <td>10550</td>\n      <td>3.326900</td>\n    </tr>\n    <tr>\n      <td>10600</td>\n      <td>3.335000</td>\n    </tr>\n    <tr>\n      <td>10650</td>\n      <td>3.430500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 500...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       | Ù¾Ù„ÛŒØ³Ø³               \nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         | Ú¯Ø§Ù„ÛŒÚ©Ù†Ø² Ø§ÙˆØ±         \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 28.65\nF1: 29.14\nEdit Distance (normalized): 30.37\nâœ… Step 500: EM=28.65, F1=29.14, EditDist=30.37\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-500 to Hub...\nâœ… Pushed checkpoint-500 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 1000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       |                     \nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 30.50\nF1: 30.86\nEdit Distance (normalized): 31.58\nâœ… Step 1000: EM=30.50, F1=30.86, EditDist=31.58\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-1000 to Hub...\nâœ… Pushed checkpoint-1000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 1500...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       |                     \nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 31.00\nF1: 31.20\nEdit Distance (normalized): 31.64\nâœ… Step 1500: EM=31.00, F1=31.20, EditDist=31.64\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-1500 to Hub...\nâœ… Pushed checkpoint-1500 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 2000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       |                     \nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 31.50\nF1: 31.73\nEdit Distance (normalized): 32.12\nâœ… Step 2000: EM=31.50, F1=31.73, EditDist=32.12\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-2000 to Hub...\nâœ… Pushed checkpoint-2000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 2500...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       |                     \nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 31.80\nF1: 31.98\nEdit Distance (normalized): 32.34\nâœ… Step 2500: EM=31.80, F1=31.98, EditDist=32.34\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-2500 to Hub...\nâœ… Pushed checkpoint-2500 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 3000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       |                     \nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 31.75\nF1: 31.96\nEdit Distance (normalized): 32.34\nâœ… Step 3000: EM=31.75, F1=31.96, EditDist=32.34\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-3000 to Hub...\nâœ… Pushed checkpoint-3000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 3500...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       |                     \nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 31.90\nF1: 32.11\nEdit Distance (normalized): 32.40\nâœ… Step 3500: EM=31.90, F1=32.11, EditDist=32.40\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-3500 to Hub...\nâœ… Pushed checkpoint-3500 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 4000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       |                     \nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 31.95\nF1: 32.09\nEdit Distance (normalized): 32.40\nâœ… Step 4000: EM=31.95, F1=32.09, EditDist=32.40\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-4000 to Hub...\nâœ… Pushed checkpoint-4000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 4500...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       |                     \nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 31.95\nF1: 32.15\nEdit Distance (normalized): 32.44\nâœ… Step 4500: EM=31.95, F1=32.15, EditDist=32.44\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-4500 to Hub...\nâœ… Pushed checkpoint-4500 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 5000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       |                     \nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 31.80\nF1: 31.98\nEdit Distance (normalized): 32.25\nâœ… Step 5000: EM=31.80, F1=31.98, EditDist=32.25\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-5000 to Hub...\nâœ… Pushed checkpoint-5000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 5500...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       |                     \nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 32.55\nF1: 32.67\nEdit Distance (normalized): 32.85\nâœ… Step 5500: EM=32.55, F1=32.67, EditDist=32.85\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-5500 to Hub...\nâœ… Pushed checkpoint-5500 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 6000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       |                     \nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 32.60\nF1: 32.71\nEdit Distance (normalized): 32.88\nâœ… Step 6000: EM=32.60, F1=32.71, EditDist=32.88\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-6000 to Hub...\nâœ… Pushed checkpoint-6000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 6500...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       |                     \nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 32.75\nF1: 32.86\nEdit Distance (normalized): 33.05\nâœ… Step 6500: EM=32.75, F1=32.86, EditDist=33.05\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-6500 to Hub...\nâœ… Pushed checkpoint-6500 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 7000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       |                     \nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 32.45\nF1: 32.56\nEdit Distance (normalized): 32.74\nâœ… Step 7000: EM=32.45, F1=32.56, EditDist=32.74\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-7000 to Hub...\nâœ… Pushed checkpoint-7000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 7500...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       |                     \nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 32.90\nF1: 33.04\nEdit Distance (normalized): 33.20\nâœ… Step 7500: EM=32.90, F1=33.04, EditDist=33.20\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-7500 to Hub...\nâœ… Pushed checkpoint-7500 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 8000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       | Ø¬Ù†ÙˆØ¨ÛŒ Ø§ÙØ±ÛŒÙ‚Û Ù…ÛŒÚº Ø³Ø§Ø¨Ù‚Û ÛÙˆÚ¯Ù†ÙˆÙ¹ ÙØ§Ø±Ù… Ú©ÛØ§Úº Ù¾Ø§Ø¦Û’ Ø¬Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚºØŸØŒ ÚˆÙˆ Ù¾Ù„ÛŒØ³Ø³\nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 32.35\nF1: 32.51\nEdit Distance (normalized): 32.73\nâœ… Step 8000: EM=32.35, F1=32.51, EditDist=32.73\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-8000 to Hub...\nâœ… Pushed checkpoint-8000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 8500...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       |                     \nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 32.75\nF1: 32.88\nEdit Distance (normalized): 33.04\nâœ… Step 8500: EM=32.75, F1=32.88, EditDist=33.04\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-8500 to Hub...\nâœ… Pushed checkpoint-8500 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 9000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       |                     \nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 32.50\nF1: 32.61\nEdit Distance (normalized): 32.79\nâœ… Step 9000: EM=32.50, F1=32.61, EditDist=32.79\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-9000 to Hub...\nâœ… Pushed checkpoint-9000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 9500...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       |                     \nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 32.70\nF1: 32.80\nEdit Distance (normalized): 32.94\nâœ… Step 9500: EM=32.70, F1=32.80, EditDist=32.94\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-9500 to Hub...\nâœ… Pushed checkpoint-9500 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 10000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       | Ø¬Ù†ÙˆØ¨ÛŒ Ø§ÙØ±ÛŒÙ‚Û Ù…ÛŒÚº Ø³Ø§Ø¨Ù‚Û ÛÙˆÚ¯Ù†ÙˆÙ¹ ÙØ§Ø±Ù… Ú©ÛØ§Úº Ù¾Ø§Ø¦Û’ Ø¬Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚºØŸØŒ ÚˆÙˆ Ù¾Ù„ÛŒØ³Ø³\nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 32.65\nF1: 32.75\nEdit Distance (normalized): 32.95\nâœ… Step 10000: EM=32.65, F1=32.75, EditDist=32.95\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-10000 to Hub...\nâœ… Pushed checkpoint-10000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 10500...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_111/2115667162.py:88: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ‘€ VISUAL AUDIT (Top 5 Predictions):\nQuestion                                           | Gold                 | Prediction          \n----------------------------------------------------------------------------------------------------\nSample #0        | Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û       | Ø¬Ù†ÙˆØ¨ÛŒ Ø§ÙØ±ÛŒÙ‚Û Ù…ÛŒÚº Ø³Ø§Ø¨Ù‚Û ÛÙˆÚ¯Ù†ÙˆÙ¹ ÙØ§Ø±Ù… Ú©ÛØ§Úº Ù¾Ø§Ø¦Û’ Ø¬Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚºØŸØŒ ÚˆÙˆ Ù¾Ù„ÛŒØ³Ø³\nSample #1        |                      |                     \nSample #2        |                      |                     \nSample #3        | ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û         |                     \nSample #4        |                      |                     \n----------------------------------------------------------------------------------------------------\n\nExamples evaluated: 2000\nExact Match: 32.50\nF1: 32.64\nEdit Distance (normalized): 32.80\nâœ… Step 10500: EM=32.50, F1=32.64, EditDist=32.80\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-10500 to Hub...\nâœ… Pushed checkpoint-10500 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_111/4032920361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2195\u001b[0m                 \u001b[0;31m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2196\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2197\u001b[0;31m                 return inner_training_loop(\n\u001b[0m\u001b[1;32m   2198\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m                     \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2546\u001b[0m                     )\n\u001b[1;32m   2547\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2548\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3748\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3749\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3751\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3834\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3835\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3836\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3837\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3838\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   2708\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2709\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2710\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   2711\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2712\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/canine/modeling_canine.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m         outputs = self.canine(\n\u001b[0m\u001b[1;32m   1487\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/canine/modeling_canine.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;31m# Deep BERT encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0;31m# `molecule_sequence_output`: shape (batch_size, mol_seq_len, mol_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1102\u001b[0m             \u001b[0minit_molecule_encoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_molecule_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/canine/modeling_canine.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mlayer_head_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhead_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m             \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m             )\n\u001b[1;32m    744\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;34m\"use_reentrant=False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             )\n\u001b[0;32m--> 489\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         gen = _checkpoint_without_reentrant_generator(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/canine/modeling_canine.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n\u001b[0;32m--> 713\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    714\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/canine/modeling_canine.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    580\u001b[0m     ) -> tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0mself_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m             \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/canine/modeling_canine.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, from_tensor, to_tensor, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1733\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":104},{"id":"28b41337-dc8a-4b6f-a66a-e409e2dc19ad","cell_type":"markdown","source":"---","metadata":{}},{"id":"20","cell_type":"markdown","source":"### Diagnosing Preprocessing Functions!!!\n\nThese functions are just analysing the preprocessing logic above, they're just using the base model, NOT our trained model...","metadata":{"id":"cc44692c-6652-4cda-9ba4-8a03acdab88d"}},{"id":"21","cell_type":"code","source":"# Diagnostic cell (fixed): Investigate preprocessing and truncation for many samples\nimport random\nimport pandas as pd\nfrom transformers import AutoTokenizer\n\n# Set display options to see full Urdu text\npd.set_option('display.max_colwidth', None)\n\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(\"google/canine-s\")\nexcept Exception:\n    tokenizer = None\n\nnum_samples = 20000  # Number of samples to check\nresults = []\n\nfor split_name, orig_data, proc_data in [\n    (\"train\", uqa_train, processed_train),\n    (\"val\", uqa_val, processed_val)\n]:\n    # Sample random indices\n    if len(proc_data) < num_samples:\n        current_indices = range(len(proc_data))\n    else:\n        current_indices = random.sample(range(len(proc_data)), num_samples)\n\n    for idx in current_indices:\n        proc = proc_data[idx]\n        # Use overflow_to_sample_mapping to get the correct original index\n        orig_idx = proc[\"overflow_to_sample_mapping\"]\n        orig = orig_data[orig_idx]\n\n        input_ids = proc[\"input_ids\"]\n        start_pos = proc[\"start_positions\"]\n        end_pos = proc[\"end_positions\"]\n\n        gold_answer = orig.get(\"gold_answer\", orig.get(\"answer\", \"\"))\n        question = orig.get(\"question\", \"\")\n\n        # Decode input_ids to text (for debugging context)\n        if tokenizer:\n            decoded_text = tokenizer.decode(input_ids, skip_special_tokens=False)\n        else:\n            decoded_text = str(input_ids)\n\n        # Extract predicted answer span\n        if 0 <= start_pos < len(input_ids) and 0 <= end_pos < len(input_ids):\n            if tokenizer:\n                pred_span = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True)\n            else:\n                pred_span = str(input_ids[start_pos:end_pos+1])\n        else:\n            pred_span = \"[CLS]\" # Represents no answer found in this chunk or invalid\n\n        # Check if pred_span matches gold answer\n        # We strip() to ignore minor whitespace differences\n        pred_matches_gold = pred_span.strip() == gold_answer.strip()\n\n        # Check if gold is even reachable in this chunk\n        gold_in_decoded = gold_answer in decoded_text\n\n        results.append({\n            \"Split\": split_name,\n            \"Question\": question,\n            \"Gold Answer\": gold_answer,\n            \"Extracted Answer\": pred_span,\n            \"Match\": pred_matches_gold,\n            \"Gold Reachable\": gold_in_decoded,\n            \"orig_idx\": orig_idx\n        })\n\n# Create DataFrame\nresults_df = pd.DataFrame(results)\n\n# --- SIDE BY SIDE COMPARISON ---\n\n# 1. Filter for Solvable Mismatches (Gold was there, but we predicted wrong)\nproblem_cases = results_df[\n    (results_df[\"Gold Reachable\"] == True) &\n    (results_df[\"Match\"] == False)\n][[\"Question\", \"Gold Answer\", \"Extracted Answer\", \"Split\"]]\n\nprint(f\"ğŸ” Checked {len(results_df)} samples.\")\nprint(f\"âŒ Found {len(problem_cases)} cases where Gold was present but Extraction failed.\")\n\nprint(\"\\nğŸ“Š Side-by-Side Comparison (Top 20 Failures):\")\ndisplay(problem_cases.head(50))\n\nprint(\"\\nâœ… Side-by-Side Comparison (First 10 Rows - Mixed):\")\ndisplay(results_df[[\"Question\", \"Gold Answer\", \"Extracted Answer\", \"Match\"]].head(50))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"49f3717d","outputId":"38f435a4-1b55-4c2b-b6a5-86540fc23755","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:12:56.324561Z","iopub.status.idle":"2025-12-01T11:12:56.324788Z","shell.execute_reply.started":"2025-12-01T11:12:56.324680Z","shell.execute_reply":"2025-12-01T11:12:56.324689Z"}},"outputs":[],"execution_count":null},{"id":"cf6b2e55-60c0-44f2-80bc-2fa32a3b074e","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}