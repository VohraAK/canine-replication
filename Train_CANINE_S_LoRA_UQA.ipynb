{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:12.770949Z",
     "iopub.status.busy": "2025-12-03T19:08:12.770574Z",
     "iopub.status.idle": "2025-12-03T19:08:12.774490Z",
     "shell.execute_reply": "2025-12-03T19:08:12.773824Z",
     "shell.execute_reply.started": "2025-12-03T19:08:12.770906Z"
    },
    "id": "c186240c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %pip install peft evaluate transformers Levenshtein ipywidgets\n",
    "# %pip install protobuf==3.20.3\n",
    "# !rm -rf /kaggle/working/cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:12.775882Z",
     "iopub.status.busy": "2025-12-03T19:08:12.775641Z",
     "iopub.status.idle": "2025-12-03T19:08:12.791146Z",
     "shell.execute_reply": "2025-12-03T19:08:12.790491Z",
     "shell.execute_reply.started": "2025-12-03T19:08:12.775861Z"
    },
    "id": "cd8da8ab",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# X\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_DISABLE_CHAT_TEMPLATES\"] = \"1\"\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADDITIONAL_CHAT_TEMPLATES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:12.931653Z",
     "iopub.status.busy": "2025-12-03T19:08:12.931149Z",
     "iopub.status.idle": "2025-12-03T19:08:12.935762Z",
     "shell.execute_reply": "2025-12-03T19:08:12.935057Z",
     "shell.execute_reply.started": "2025-12-03T19:08:12.931634Z"
    },
    "id": "d87eba82",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "# from UQA.canine_utils import preprocess_uqa, lora_config, print_trainable_parameters, normalize_answer, exact_match_score, f1_score, edit_distance_score, gold_answer, decode_prediction\n",
    "from transformers import CanineTokenizer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
    "import json\n",
    "from huggingface_hub import HfApi, notebook_login, whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:12.937202Z",
     "iopub.status.busy": "2025-12-03T19:08:12.937021Z",
     "iopub.status.idle": "2025-12-03T19:08:12.948697Z",
     "shell.execute_reply": "2025-12-03T19:08:12.948080Z",
     "shell.execute_reply.started": "2025-12-03T19:08:12.937189Z"
    },
    "id": "0e98cebe-4c08-4850-b3c1-1529564fdb1b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# notebook_login()\n",
    "# whoami()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:12.970313Z",
     "iopub.status.busy": "2025-12-03T19:08:12.969771Z",
     "iopub.status.idle": "2025-12-03T19:08:14.802314Z",
     "shell.execute_reply": "2025-12-03T19:08:14.801744Z",
     "shell.execute_reply.started": "2025-12-03T19:08:12.970295Z"
    },
    "id": "f2dd5a40",
    "outputId": "140c30ea-575d-45cd-ea54-7818cdfe6bf5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CanineTokenizer, CanineForQuestionAnswering\n",
    "import torch\n",
    "model_name = 'google/canine-s'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "tokenizer = CanineTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=False)\n",
    "model = CanineForQuestionAnswering.from_pretrained(model_name, trust_remote_code=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:14.803506Z",
     "iopub.status.busy": "2025-12-03T19:08:14.803294Z",
     "iopub.status.idle": "2025-12-03T19:08:16.628098Z",
     "shell.execute_reply": "2025-12-03T19:08:16.627518Z",
     "shell.execute_reply.started": "2025-12-03T19:08:14.803489Z"
    },
    "id": "d474e2e8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load dataset and filter out impossible/unanswerable questions (like XLM-RoBERTa baseline)\n",
    "def filter_answerable(example):\n",
    "    \"\"\"Filter out questions without valid answers (answer_start == -1)\"\"\"\n",
    "    return example['answer_start'] != -1\n",
    "\n",
    "uqa_dataset = load_dataset(\"uqa/UQA\")\n",
    "\n",
    "# Apply filtering to remove impossible questions\n",
    "uqa_dataset_filtered = uqa_dataset.filter(filter_answerable)\n",
    "\n",
    "# Use 60k examples (50% more than before, still manageable for hardware constraints)\n",
    "uqa_train = uqa_dataset_filtered[\"train\"].shuffle(seed=42).select(range(60000))\n",
    "uqa_val = uqa_dataset_filtered[\"validation\"].shuffle(seed=42).select(range(2000))\n",
    "\n",
    "print(f\"ðŸ“Š Dataset after filtering:\")\n",
    "print(f\"   Original train size: {len(uqa_dataset['train']):,}\")\n",
    "print(f\"   Filtered train size: {len(uqa_dataset_filtered['train']):,}\")\n",
    "print(f\"   Using for training: {len(uqa_train):,}\")\n",
    "print(f\"   Validation size: {len(uqa_val):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145a43ff",
   "metadata": {},
   "source": [
    "## ðŸ”§ Hardware-Optimized Training Configuration\n",
    "\n",
    "Based on comparison with XLM-RoBERTa baseline, the following optimizations have been applied:\n",
    "\n",
    "### Critical Fixes:\n",
    "1. **âœ… Filter impossible questions** - Remove `answer_start == -1` examples (like XLM-RoBERTa does)\n",
    "2. **âœ… Increase dataset size** - 60k examples (up from 40k, +50% more training data)\n",
    "3. **âœ… Lower learning rate** - 5e-5 (down from 3e-4, prevents overshooting)\n",
    "4. **âœ… More training epochs** - 2 epochs (up from 1, allows convergence)\n",
    "5. **âœ… Better overlap** - DOC_STRIDE=96 (up from 64, more training signals)\n",
    "6. **âœ… Reduce checkpoint overhead** - save_steps=1000 (down from 500)\n",
    "\n",
    "### Expected Improvements:\n",
    "- **Filtering impossible questions**: +15-20% performance (removes label noise)\n",
    "- **Lower learning rate**: +10-15% performance (stable training)\n",
    "- **2 epochs**: +20-25% performance (sufficient learning time)\n",
    "- **Combined effect**: Should see **50-70% EM/F1** (vs current 33%)\n",
    "\n",
    "### Hardware Considerations:\n",
    "- Kept batch size at 4Ã—4=16 (memory-friendly)\n",
    "- 60k examples instead of full dataset (manageable)\n",
    "- 2 epochs instead of 6 (time-efficient)\n",
    "- Learning rate 5e-5 instead of 2e-5 (faster convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:16.629029Z",
     "iopub.status.busy": "2025-12-03T19:08:16.628787Z",
     "iopub.status.idle": "2025-12-03T19:08:16.987248Z",
     "shell.execute_reply": "2025-12-03T19:08:16.986433Z",
     "shell.execute_reply.started": "2025-12-03T19:08:16.629009Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "UQA DATASET STRUCTURE\n",
      "================================================================================\n",
      "Training set size: 40,000 examples\n",
      "Validation set size: 2,000 examples\n",
      "\n",
      "Dataset columns: ['id', 'title', 'context', 'question', 'is_impossible', 'answer', 'answer_start']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ EXAMPLE 1 - Question with Answer\n",
      "================================================================================\n",
      "Question: Ø¬Ø¯ÛŒØ¯ ÛÙˆØ§Ø¦ÛŒ Ø¬ÛØ§Ø²ÙˆÚº Ú©ÛŒ ØªØ¹Ù…ÛŒØ± Ù…ÛŒÚº Ú©ÛŒØ§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ØªÚ¾Ø§ØŸ\n",
      "\n",
      "Context (first 300 chars): 1906 Ù…ÛŒÚº ØŒ Ø§Ù„ÙØ±ÛŒÚˆ ÙˆÙÙ„Ù… Ù†Û’ Ø¨Ø§Ø±Ø´ Ø³Û’ Ø³Ø®Øª ÛÙˆÙ†Û’ ÙˆØ§Ù„Û’ Ù…Ø±Ú©Ø¨ Ø¯Ø±ÛŒØ§ÙØª Ú©ÛŒÛ’Û” Ø¨Ø§Ø±Ø´ Ø³Û’ Ø³Ø®Øª ÛÙˆÙ†Û’ ÙˆØ§Ù„Û’ Ù…Ø±Ú©Ø¨ ØŒ Ø¬ÛŒØ³Û’ Ø§ÛŒÙ„ÙˆÙ…ÛŒÙ†ÛŒÙ… ØŒ Ù¹Ø§Ø¦Ù¹ÛŒÙ†ÛŒÙ… Ø§ÙˆØ± ØªØ§Ù†Ø¨Û’ Ú©Û’ Ú©Ú†Ú¾ Ù…Ø±Ú©Ø¨ ØŒ Ú¯Ø±Ù…ÛŒ Ø³Û’ Ø¹Ù„Ø§Ø¬ Ú©Ø±Ù†Û’ ÙˆØ§Ù„Û’ Ù…Ø±Ú©Ø¨ ÛÛŒÚº Ø¬Ùˆ Ù¹Ú¾Ù†ÚˆØ§ ÛÙˆÙ†Û’ Ù¾Ø± Ù†Ø±Ù… ÛÙˆØ¬Ø§ØªÛ’ ÛÛŒÚº (Ø¬Ù„Ø¯ÛŒ Ø³Û’ Ù¹Ú¾Ù†ÚˆØ§ ÛÙˆØ¬Ø§ØªÛ’ ÛÛŒÚº) ØŒ Ø§ÙˆØ± Ù¾Ú¾Ø± ÙˆÙ‚Øª Ú©Û’ Ø³Ø§ØªÚ¾ Ø³Ø®Øª ÛÙˆØ¬Ø§ØªÛ’ ÛÛŒÚºÛ” Ø§ÛŒÙ„ÙˆÙ…ÛŒÙ†ÛŒÙ… ØŒ ØªØ§Ù†Ø¨Û’ Ø§ÙˆØ± Ù…ÛŒÚ¯Ù†ÛŒ...\n",
      "\n",
      "Answer: 'ÚˆÙˆØ±Ø§Ù„ÙˆÙ…ÛŒÙ†'\n",
      "Answer starts at character position: 503\n",
      "âœ“ Extracted from context: 'ÚˆÙˆØ±Ø§Ù„ÙˆÙ…ÛŒÙ†'\n",
      "âœ“ Match: True\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ EXAMPLE 2 - Another Question\n",
      "================================================================================\n",
      "Question: Ú©ÙˆÙ† Ø³Ø§ Ù…Ù†Ú¯ Ø´ÛØ²Ø§Ø¯Û Ù†Ø§Ù†Ø¬Ù†Ú¯ Ù…ÛŒÚº ØªØ®Øª Ù†Ø´ÛŒÙ† ÛÙˆØ§ ØªÚ¾Ø§ØŸ\n",
      "\n",
      "Context length: 447 characters\n",
      "Answer: 'Ø¬Ùˆ ÛŒÙˆ Ø³ÙˆÙ†Ú¯'\n",
      "Answer starts at position: 262\n",
      "\n",
      "Context around answer:\n",
      "...Ø§Ù„ÛŒ Ú†Ù†Ú¯ Ø®Ø§Ù†Ø¯Ø§Ù† Ú©Û’ ÛØ§ØªÚ¾ÙˆÚº Ú¯Ø±Ù†Û’ Ú©Û’ Ø¨Ø¹Ø¯ ØŒ Ù…Ù†Ú¯ Ø´ÛØ²Ø§Ø¯Û Ø¬Ùˆ ÛŒÙˆ Ø³ÙˆÙ†Ú¯ Ú©Ùˆ Ø¬ÙˆÙ† 1644 Ù…ÛŒÚº ÛØ§Ù†Ú¯ Ú¯ÙˆØ§Ù†Ú¯ Ø´ÛÙ†Ø´Ø§Û Ú©Û’ Ø·ÙˆØ± Ù¾Ø± Ù†Ø§Ù†Ø¬Ù†...\n",
      "                                                      ~~~~~~~~~~ (answer here)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š DATASET STATISTICS\n",
      "================================================================================\n",
      "Question length (chars): mean=54.1, max=155\n",
      "Context length (chars): mean=685.5, max=3179\n",
      "Answer length (chars): mean=11.5, max=142\n",
      "Questions with answers: 64.9%\n",
      "Questions without answers: 35.1%\n"
     ]
    }
   ],
   "source": [
    "# Explore raw UQA dataset structure\n",
    "print(\"=\"*80)\n",
    "print(\"UQA DATASET STRUCTURE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training set size: {len(uqa_train):,} examples\")\n",
    "print(f\"Validation set size: {len(uqa_val):,} examples\")\n",
    "print(f\"\\nDataset columns: {uqa_train.column_names}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Show a few examples\n",
    "print(\"\\nðŸ“ EXAMPLE 1 - Question with Answer\")\n",
    "print(\"=\"*80)\n",
    "ex1 = uqa_train[0]\n",
    "print(f\"Question: {ex1['question']}\")\n",
    "print(f\"\\nContext (first 300 chars): {ex1['context'][:300]}...\")\n",
    "print(f\"\\nAnswer: '{ex1['answer']}'\")\n",
    "print(f\"Answer starts at character position: {ex1['answer_start']}\")\n",
    "\n",
    "# Verify the answer extraction\n",
    "if ex1['answer_start'] != -1:\n",
    "    extracted = ex1['context'][ex1['answer_start']:ex1['answer_start']+len(ex1['answer'])]\n",
    "    print(f\"âœ“ Extracted from context: '{extracted}'\")\n",
    "    print(f\"âœ“ Match: {extracted == ex1['answer']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nðŸ“ EXAMPLE 2 - Another Question\")\n",
    "print(\"=\"*80)\n",
    "ex2 = uqa_train[100]\n",
    "print(f\"Question: {ex2['question']}\")\n",
    "print(f\"\\nContext length: {len(ex2['context'])} characters\")\n",
    "print(f\"Answer: '{ex2['answer']}'\")\n",
    "print(f\"Answer starts at position: {ex2['answer_start']}\")\n",
    "\n",
    "# Show answer in context\n",
    "if ex2['answer_start'] != -1:\n",
    "    start = max(0, ex2['answer_start'] - 50)\n",
    "    end = min(len(ex2['context']), ex2['answer_start'] + len(ex2['answer']) + 50)\n",
    "    context_snippet = ex2['context'][start:end]\n",
    "    answer_pos = ex2['answer_start'] - start\n",
    "    print(f\"\\nContext around answer:\")\n",
    "    print(f\"...{context_snippet}...\")\n",
    "    print(f\"    {' '*answer_pos}{'~'*len(ex2['answer'])} (answer here)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nðŸ“Š DATASET STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compute some basic statistics\n",
    "import numpy as np\n",
    "question_lengths = [len(ex['question']) for ex in uqa_train.select(range(1000))]\n",
    "context_lengths = [len(ex['context']) for ex in uqa_train.select(range(1000))]\n",
    "answer_lengths = [len(ex['answer']) if ex['answer'] else 0 for ex in uqa_train.select(range(1000))]\n",
    "has_answer = [ex['answer_start'] != -1 for ex in uqa_train.select(range(1000))]\n",
    "\n",
    "print(f\"Question length (chars): mean={np.mean(question_lengths):.1f}, max={np.max(question_lengths)}\")\n",
    "print(f\"Context length (chars): mean={np.mean(context_lengths):.1f}, max={np.max(context_lengths)}\")\n",
    "print(f\"Answer length (chars): mean={np.mean(answer_lengths):.1f}, max={np.max(answer_lengths)}\")\n",
    "print(f\"Questions with answers: {sum(has_answer)/len(has_answer)*100:.1f}%\")\n",
    "print(f\"Questions without answers: {(1-sum(has_answer)/len(has_answer))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## ðŸ” Data Exploration: Understanding the UQA Dataset\n",
    "\n",
    "Let's explore what the raw dataset looks like before preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "89c472d5"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "6e80a8d3"
   },
   "source": [
    "## Updated preprocessors!\n",
    "\n",
    "Previously, we tried to apply the same approach we used in TYDIQA on UQA, the problem was the preprocessors were aligning the answer spans in units of **byte-level spans** instead of **character-level spans**. The calculations were adding byte-level offsets to the answer lengths, and since Urdu characters may be quantified in multiple bytes, the model was being fed the wrong spans -> GIGO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:16.989384Z",
     "iopub.status.busy": "2025-12-03T19:08:16.989135Z",
     "iopub.status.idle": "2025-12-03T19:08:17.004649Z",
     "shell.execute_reply": "2025-12-03T19:08:17.003927Z",
     "shell.execute_reply.started": "2025-12-03T19:08:16.989368Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FIXED preprocessing function for UQA with CANINE-S.\n",
    "Copy this into Train_CANINE_S_LoRA_UQA.ipynb cell 8 to replace the existing preprocess_uqa function.\n",
    "\n",
    "Key fixes:\n",
    "1. Added byte-to-char conversion helpers (from TyDiQA)\n",
    "2. Support both byte-based and character-based offsets via use_byte_offsets parameter\n",
    "3. Changed gold_char_end calculation to be inclusive (removed +1, added -1 after len(answer))\n",
    "4. Use dynamic cls_index for no-answer cases instead of hardcoded 0\n",
    "5. Fixed answer chunk boundary check (< instead of <=)\n",
    "6. Removed incorrect -1 subtraction from end_pos calculation\n",
    "\"\"\"\n",
    "\n",
    "from bisect import bisect_right\n",
    "\n",
    "MAX_SEQ_LENGTH = 384\n",
    "DOC_STRIDE = 96  # Increased from 64 to 96 for better overlap (compromise between 64 and 128)\n",
    "\n",
    "def _build_byte_to_char_index(text: str) -> list:\n",
    "    \"\"\"Build cumulative UTF-8 byte offsets for each character boundary.\"\"\"\n",
    "    cumulative = [0]\n",
    "    for char in text:\n",
    "        cumulative.append(cumulative[-1] + len(char.encode(\"utf-8\")))\n",
    "    return cumulative\n",
    "\n",
    "def _byte_to_char(cumulative_bytes: list, byte_index: int) -> int:\n",
    "    \"\"\"Map a byte offset to the nearest character index (floor).\"\"\"\n",
    "    position = bisect_right(cumulative_bytes, byte_index) - 1\n",
    "    return max(position, 0)\n",
    "\n",
    "def preprocess_uqa(examples, tokenizer, max_length=MAX_SEQ_LENGTH, doc_stride=DOC_STRIDE, model_obj=None, indices=None, use_byte_offsets=False):\n",
    "    \"\"\"\n",
    "    Robust preprocessing for UQA (Urdu Question Answering) with CANINE-S.\n",
    "    \n",
    "    Args:\n",
    "        examples: Batch of examples with question, context, answer, answer_start fields\n",
    "        tokenizer: CanineTokenizer instance\n",
    "        max_length: Maximum sequence length\n",
    "        doc_stride: Sliding window stride\n",
    "        model_obj: Optional model object (unused, for compatibility)\n",
    "        indices: Optional example indices for overflow mapping\n",
    "        use_byte_offsets: If True, treats answer_start as byte offset (like TyDiQA)\n",
    "                         If False, treats as character offset (default UQA behavior)\n",
    "    \n",
    "    Returns:\n",
    "        Dict with input_ids, attention_mask, token_type_ids, start_positions, \n",
    "        end_positions, overflow_to_sample_mapping\n",
    "    \"\"\"\n",
    "    # Handle tokenizer/model limits safely\n",
    "    tokenizer_max = getattr(tokenizer, \"model_max_length\", max_length)\n",
    "    model_max = getattr(model_obj.config, \"max_position_embeddings\", None) if model_obj is not None else None\n",
    "    max_allowed = max_length\n",
    "    if tokenizer_max is not None and tokenizer_max > 0:\n",
    "        max_allowed = min(max_allowed, tokenizer_max)\n",
    "    if model_max is not None and model_max > 0:\n",
    "        max_allowed = min(max_allowed, model_max)\n",
    "\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    contexts = examples[\"context\"]\n",
    "    answers = examples[\"answer\"]\n",
    "    answer_starts = examples[\"answer_start\"]\n",
    "\n",
    "    encoded = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"start_positions\": [],\n",
    "        \"end_positions\": [],\n",
    "        \"overflow_to_sample_mapping\": []\n",
    "    }\n",
    "\n",
    "    for i, (question, context, answer, answer_start) in enumerate(zip(questions, contexts, answers, answer_starts)):\n",
    "        example_idx = indices[i] if indices is not None else i\n",
    "\n",
    "        # CANINE encodes to characters directly (1 char = 1 token)\n",
    "        question_ids = tokenizer.encode(question, add_special_tokens=False)\n",
    "        context_ids = tokenizer.encode(context, add_special_tokens=False)\n",
    "\n",
    "        # 1. Setup Targets - Convert offsets to character indices\n",
    "        if answer and answer_start != -1:\n",
    "            if use_byte_offsets:\n",
    "                # UQA might use byte offsets for multi-byte Urdu characters\n",
    "                byte_map = _build_byte_to_char_index(context)\n",
    "                gold_char_start = _byte_to_char(byte_map, answer_start)\n",
    "                answer_end_byte = answer_start + len(answer.encode('utf-8'))\n",
    "                gold_char_end = _byte_to_char(byte_map, answer_end_byte - 1)\n",
    "            else:\n",
    "                # Standard character-based offsets\n",
    "                gold_char_start = answer_start\n",
    "                # CRITICAL FIX: gold_char_end is INCLUSIVE (points to last char, not past it)\n",
    "                gold_char_end = answer_start + len(answer) - 1\n",
    "        else:\n",
    "            gold_char_start = -1\n",
    "            gold_char_end = -1\n",
    "\n",
    "        # 2. Calculate Window Size\n",
    "        special_tokens_count = tokenizer.num_special_tokens_to_add(pair=True)\n",
    "        max_context_length = max_allowed - len(question_ids) - special_tokens_count\n",
    "\n",
    "        if max_context_length <= 0:\n",
    "            continue\n",
    "\n",
    "        # 3. Sliding Window Loop\n",
    "        stride_step = max_context_length - doc_stride\n",
    "        if stride_step <= 0:\n",
    "            stride_step = max_context_length\n",
    "\n",
    "        for chunk_start_idx in range(0, len(context_ids), stride_step):\n",
    "            chunk_end_idx = min(chunk_start_idx + max_context_length, len(context_ids))\n",
    "            context_chunk = context_ids[chunk_start_idx:chunk_end_idx]\n",
    "\n",
    "            # Build inputs with special tokens: [CLS] question [SEP] context [SEP]\n",
    "            input_ids = tokenizer.build_inputs_with_special_tokens(question_ids, context_chunk)\n",
    "            token_type_ids = tokenizer.create_token_type_ids_from_sequences(question_ids, context_chunk)\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "\n",
    "            # Find where context starts in input_ids\n",
    "            sep_indices = [k for k, x in enumerate(input_ids) if x == tokenizer.sep_token_id]\n",
    "            if not sep_indices:\n",
    "                continue\n",
    "            context_offset_in_input = sep_indices[0] + 1\n",
    "            \n",
    "            # Find CLS position dynamically (should be 0 for CANINE, but be safe)\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n",
    "\n",
    "            # 4. Label Assignment\n",
    "            # Check if answer is ENTIRELY within this chunk (both start and end)\n",
    "            is_answer_in_chunk = (\n",
    "                gold_char_start >= chunk_start_idx and\n",
    "                gold_char_end <= chunk_end_idx and  # Inclusive: answer must fit within chunk\n",
    "                gold_char_start != -1\n",
    "            )\n",
    "\n",
    "            if is_answer_in_chunk:\n",
    "                # Map global context indices to local input_ids indices\n",
    "                start_pos = context_offset_in_input + (gold_char_start - chunk_start_idx)\n",
    "                end_pos = context_offset_in_input + (gold_char_end - chunk_start_idx)\n",
    "                # NO -1 here because gold_char_end is already INCLUSIVE\n",
    "            else:\n",
    "                # No answer in this chunk - point to [CLS] token\n",
    "                start_pos = cls_index\n",
    "                end_pos = cls_index\n",
    "\n",
    "            # 5. Padding\n",
    "            pad_len = max_allowed - len(input_ids)\n",
    "            if pad_len > 0:\n",
    "                input_ids += [tokenizer.pad_token_id] * pad_len\n",
    "                attention_mask += [0] * pad_len\n",
    "                token_type_ids += [0] * pad_len\n",
    "\n",
    "            # 6. Final Safety Truncation\n",
    "            if len(input_ids) > max_allowed:\n",
    "                input_ids = input_ids[:max_allowed]\n",
    "                attention_mask = attention_mask[:max_allowed]\n",
    "                token_type_ids = token_type_ids[:max_allowed]\n",
    "                if start_pos >= max_allowed or end_pos >= max_allowed:\n",
    "                    start_pos = cls_index\n",
    "                    end_pos = cls_index\n",
    "\n",
    "            encoded[\"input_ids\"].append(input_ids)\n",
    "            encoded[\"attention_mask\"].append(attention_mask)\n",
    "            encoded[\"token_type_ids\"].append(token_type_ids)\n",
    "            encoded[\"start_positions\"].append(start_pos)\n",
    "            encoded[\"end_positions\"].append(end_pos)\n",
    "            encoded[\"overflow_to_sample_mapping\"].append(example_idx)\n",
    "\n",
    "            # Break if we've covered the entire context\n",
    "            if chunk_end_idx >= len(context_ids):\n",
    "                break\n",
    "\n",
    "    return encoded\n",
    "\n",
    "\n",
    "# USAGE EXAMPLE:\n",
    "# First, test which offset type UQA uses:\n",
    "# Run: python diagnose_uqa_offsets.py\n",
    "#\n",
    "# If character-based (expected):\n",
    "# processed_train = uqa_train.map(\n",
    "#     lambda examples, indices: preprocess_uqa(examples, tokenizer, indices=indices, use_byte_offsets=False),\n",
    "#     batched=True, remove_columns=uqa_train.column_names, with_indices=True\n",
    "# )\n",
    "#\n",
    "# If byte-based (like TyDiQA):\n",
    "# processed_train = uqa_train.map(\n",
    "#     lambda examples, indices: preprocess_uqa(examples, tokenizer, indices=indices, use_byte_offsets=True),\n",
    "#     batched=True, remove_columns=uqa_train.column_names, with_indices=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:17.005852Z",
     "iopub.status.busy": "2025-12-03T19:08:17.005518Z",
     "iopub.status.idle": "2025-12-03T19:08:17.020995Z",
     "shell.execute_reply": "2025-12-03T19:08:17.020250Z",
     "shell.execute_reply.started": "2025-12-03T19:08:17.005829Z"
    },
    "id": "438d8765",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# âœ… FIXED preprocess_uqa function is defined in cell 16 below (with evaluation helpers)\n",
    "# The updated function includes:\n",
    "# - Character-level offset handling (UQA uses character offsets, not bytes)\n",
    "# - Dynamic cls_index for no-answer cases (not hardcoded 0)\n",
    "# - Inclusive end position calculation (gold_char_end points to last char)\n",
    "# - Correct chunk boundary check (<=)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:17.021830Z",
     "iopub.status.busy": "2025-12-03T19:08:17.021644Z",
     "iopub.status.idle": "2025-12-03T19:08:17.033564Z",
     "shell.execute_reply": "2025-12-03T19:08:17.032965Z",
     "shell.execute_reply.started": "2025-12-03T19:08:17.021816Z"
    },
    "id": "a3e95eec",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.QUESTION_ANS,\n",
    "    r=16,   # changed from 8\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\", \"key\"],\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"qa_outputs\"],\n",
    ")\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:17.035193Z",
     "iopub.status.busy": "2025-12-03T19:08:17.034954Z",
     "iopub.status.idle": "2025-12-03T19:08:17.056817Z",
     "shell.execute_reply": "2025-12-03T19:08:17.056185Z",
     "shell.execute_reply.started": "2025-12-03T19:08:17.035178Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ”¬ PREPROCESSING WALKTHROUGH - Single Example\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ ORIGINAL DATA\n",
      "--------------------------------------------------------------------------------\n",
      "Question: Ø¬Ø¯ÛŒØ¯ ÛÙˆØ§Ø¦ÛŒ Ø¬ÛØ§Ø²ÙˆÚº Ú©ÛŒ ØªØ¹Ù…ÛŒØ± Ù…ÛŒÚº Ú©ÛŒØ§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ØªÚ¾Ø§ØŸ\n",
      "Answer: 'ÚˆÙˆØ±Ø§Ù„ÙˆÙ…ÛŒÙ†'\n",
      "Answer position: 503\n",
      "Context length: 767 characters\n",
      "\n",
      "2ï¸âƒ£ AFTER PREPROCESSING\n",
      "--------------------------------------------------------------------------------\n",
      "Number of chunks created: 3\n",
      "(Sliding window creates multiple chunks per example)\n",
      "\n",
      "3ï¸âƒ£ CHUNK 0 DETAILS\n",
      "--------------------------------------------------------------------------------\n",
      "Input IDs length: 384 tokens\n",
      "Start position: 0\n",
      "End position: 0\n",
      "Maps to original example: 0\n",
      "\n",
      "4ï¸âƒ£ DECODED INPUT (first 400 chars, with special tokens)\n",
      "--------------------------------------------------------------------------------\n",
      "î€€Ø¬Ø¯ÛŒØ¯ ÛÙˆØ§Ø¦ÛŒ Ø¬ÛØ§Ø²ÙˆÚº Ú©ÛŒ ØªØ¹Ù…ÛŒØ± Ù…ÛŒÚº Ú©ÛŒØ§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ØªÚ¾Ø§ØŸî€1906 Ù…ÛŒÚº ØŒ Ø§Ù„ÙØ±ÛŒÚˆ ÙˆÙÙ„Ù… Ù†Û’ Ø¨Ø§Ø±Ø´ Ø³Û’ Ø³Ø®Øª ÛÙˆÙ†Û’ ÙˆØ§Ù„Û’ Ù…Ø±Ú©Ø¨ Ø¯Ø±ÛŒØ§ÙØª Ú©ÛŒÛ’Û” Ø¨Ø§Ø±Ø´ Ø³Û’ Ø³Ø®Øª ÛÙˆÙ†Û’ ÙˆØ§Ù„Û’ Ù…Ø±Ú©Ø¨ ØŒ Ø¬ÛŒØ³Û’ Ø§ÛŒÙ„ÙˆÙ…ÛŒÙ†ÛŒÙ… ØŒ Ù¹Ø§Ø¦Ù¹ÛŒÙ†ÛŒÙ… Ø§ÙˆØ± ØªØ§Ù†Ø¨Û’ Ú©Û’ Ú©Ú†Ú¾ Ù…Ø±Ú©Ø¨ ØŒ Ú¯Ø±Ù…ÛŒ Ø³Û’ Ø¹Ù„Ø§Ø¬ Ú©Ø±Ù†Û’ ÙˆØ§Ù„Û’ Ù…Ø±Ú©Ø¨ ÛÛŒÚº Ø¬Ùˆ Ù¹Ú¾Ù†ÚˆØ§ ÛÙˆÙ†Û’ Ù¾Ø± Ù†Ø±Ù… ÛÙˆØ¬Ø§ØªÛ’ ÛÛŒÚº (Ø¬Ù„Ø¯ÛŒ Ø³Û’ Ù¹Ú¾Ù†ÚˆØ§ ÛÙˆØ¬Ø§ØªÛ’ ÛÛŒÚº) ØŒ Ø§ÙˆØ± Ù¾Ú¾Ø± ÙˆÙ‚Øª Ú©Û’ Ø³Ø§ØªÚ¾ Ø³Ø®Øª ÛÙˆØ¬Ø§ØªÛ’ ÛÛŒÚºÛ” Ø§ÛŒÙ„ÙˆÙ…ÛŒÙ†ÛŒÙ… ØŒ ØªØ§Ù†Ø¨Û’ Ø§ÙˆØ± Ù…ÛŒÚ¯Ù†ÛŒØ´ÛŒÙ… Ú©Û’ Ù¹Ø±Ù†Ø±ÛŒ Ù…Ø±Ú©Ø¨ Ú©Ùˆ Ù¹Ú¾Ù†Úˆî€...\n",
      "\n",
      "5ï¸âƒ£ LABELED ANSWER SPAN IN THIS CHUNK\n",
      "--------------------------------------------------------------------------------\n",
      "Gold answer: 'ÚˆÙˆØ±Ø§Ù„ÙˆÙ…ÛŒÙ†'\n",
      "Labeled span: '[NO ANSWER IN THIS CHUNK]'\n",
      "Match: False\n",
      "\n",
      "6ï¸âƒ£ ALL CHUNKS FOR THIS EXAMPLE\n",
      "--------------------------------------------------------------------------------\n",
      "  Chunk 0: âŒ '[NO ANSWER]'\n",
      "  Chunk 1: âœ… 'ÚˆÙˆØ±Ø§Ù„ÙˆÙ…ÛŒÙ†'\n",
      "  Chunk 2: âŒ '[NO ANSWER]'\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Let's manually preprocess ONE example to see what happens step by step\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ”¬ PREPROCESSING WALKTHROUGH - Single Example\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Take one example\n",
    "example = uqa_train[0]\n",
    "print(f\"\\n1ï¸âƒ£ ORIGINAL DATA\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Question: {example['question']}\")\n",
    "print(f\"Answer: '{example['answer']}'\")\n",
    "print(f\"Answer position: {example['answer_start']}\")\n",
    "print(f\"Context length: {len(example['context'])} characters\")\n",
    "\n",
    "# Preprocess it\n",
    "batch = {\n",
    "    'question': [example['question']],\n",
    "    'context': [example['context']],\n",
    "    'answer': [example['answer']],\n",
    "    'answer_start': [example['answer_start']]\n",
    "}\n",
    "processed = preprocess_uqa(batch, tokenizer, indices=[0])\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£ AFTER PREPROCESSING\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Number of chunks created: {len(processed['input_ids'])}\")\n",
    "print(f\"(Sliding window creates multiple chunks per example)\")\n",
    "\n",
    "# Show first chunk in detail\n",
    "chunk_idx = 0\n",
    "print(f\"\\n3ï¸âƒ£ CHUNK {chunk_idx} DETAILS\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Input IDs length: {len(processed['input_ids'][chunk_idx])} tokens\")\n",
    "print(f\"Start position: {processed['start_positions'][chunk_idx]}\")\n",
    "print(f\"End position: {processed['end_positions'][chunk_idx]}\")\n",
    "print(f\"Maps to original example: {processed['overflow_to_sample_mapping'][chunk_idx]}\")\n",
    "\n",
    "# Decode the inputs to show what the model sees\n",
    "input_ids = processed['input_ids'][chunk_idx]\n",
    "decoded_input = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "print(f\"\\n4ï¸âƒ£ DECODED INPUT (first 400 chars, with special tokens)\")\n",
    "print(\"-\"*80)\n",
    "print(decoded_input[:400] + \"...\")\n",
    "\n",
    "# Decode the labeled answer span\n",
    "start_pos = processed['start_positions'][chunk_idx]\n",
    "end_pos = processed['end_positions'][chunk_idx]\n",
    "cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n",
    "\n",
    "if start_pos == cls_idx and end_pos == cls_idx:\n",
    "    labeled_answer = \"[NO ANSWER IN THIS CHUNK]\"\n",
    "else:\n",
    "    labeled_answer = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\n5ï¸âƒ£ LABELED ANSWER SPAN IN THIS CHUNK\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Gold answer: '{example['answer']}'\")\n",
    "print(f\"Labeled span: '{labeled_answer}'\")\n",
    "print(f\"Match: {labeled_answer.strip() == example['answer'].strip()}\")\n",
    "\n",
    "# Show all chunks for this example\n",
    "print(f\"\\n6ï¸âƒ£ ALL CHUNKS FOR THIS EXAMPLE\")\n",
    "print(\"-\"*80)\n",
    "for i in range(len(processed['input_ids'])):\n",
    "    start = processed['start_positions'][i]\n",
    "    end = processed['end_positions'][i]\n",
    "    if start == cls_idx and end == cls_idx:\n",
    "        chunk_answer = \"[NO ANSWER]\"\n",
    "    else:\n",
    "        chunk_answer = tokenizer.decode(processed['input_ids'][i][start:end+1], skip_special_tokens=True).strip()\n",
    "    has_answer = \"âœ…\" if chunk_answer == example['answer'].strip() else \"âŒ\"\n",
    "    print(f\"  Chunk {i}: {has_answer} '{chunk_answer[:50]}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## ðŸ”§ Preprocessing Exploration: Raw Data â†’ Model Input\n",
    "\n",
    "Now let's see what happens during preprocessing - how we convert text to token IDs and create training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "b1984a6d29864e2d940119370816a37e",
      "a307de6c263a4c20a6418344cbd98c0c",
      "1db208af0dbc4f2facee78148266a207",
      "d44d38a959ec44aa90e91c15a83abbd6",
      "527baa5fc421480da4d2dc7041e19b1f",
      "d398c81b546d4527a41dd97bd87ad7d8",
      "ab4047c7f0144667857fe835d452f6c7",
      "0120d513dd4d4fccac2d528eb7ff4696",
      "c3e0981c2924416fbdf9ceab3e6b04ab",
      "bc970c64373f4f69b6c6936087ed978a",
      "4a74d22a2c334fbda54a95c5e29e712a"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:17.057639Z",
     "iopub.status.busy": "2025-12-03T19:08:17.057418Z",
     "iopub.status.idle": "2025-12-03T19:08:17.072084Z",
     "shell.execute_reply": "2025-12-03T19:08:17.071411Z",
     "shell.execute_reply.started": "2025-12-03T19:08:17.057613Z"
    },
    "id": "d11807b9",
    "outputId": "64fc2534-2871-4bd2-b3fa-4b37973486e2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# preprocess the train and val splits\n",
    "# processed_train = uqa_train.map(lambda examples, indices: preprocess_uqa(examples, tokenizer, indices=indices), batched=True, remove_columns=uqa_train.column_names, with_indices=True)\n",
    "# processed_val = uqa_val.map(lambda examples, indices: preprocess_uqa(examples, tokenizer, indices=indices), batched=True, remove_columns=uqa_val.column_names, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:17.073227Z",
     "iopub.status.busy": "2025-12-03T19:08:17.072880Z",
     "iopub.status.idle": "2025-12-03T19:08:17.088083Z",
     "shell.execute_reply": "2025-12-03T19:08:17.087474Z",
     "shell.execute_reply.started": "2025-12-03T19:08:17.073206Z"
    },
    "id": "D-emFQTIaZRL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# processed_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:17.090508Z",
     "iopub.status.busy": "2025-12-03T19:08:17.090281Z",
     "iopub.status.idle": "2025-12-03T19:08:22.078218Z",
     "shell.execute_reply": "2025-12-03T19:08:22.077520Z",
     "shell.execute_reply.started": "2025-12-03T19:08:17.090494Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“¦ PREPROCESSED DATASET STRUCTURE\n",
      "================================================================================\n",
      "Original training examples: 40,000\n",
      "Preprocessed training chunks: 116,995\n",
      "Expansion ratio: 2.92x\n",
      "(Each example creates ~2.0 chunks due to sliding window)\n",
      "\n",
      "Features in preprocessed data: ['input_ids', 'attention_mask', 'token_type_ids', 'start_positions', 'end_positions', 'overflow_to_sample_mapping']\n",
      "\n",
      "Feature shapes (for one chunk):\n",
      "  - input_ids: list of 384 elements\n",
      "  - attention_mask: list of 384 elements\n",
      "  - token_type_ids: list of 384 elements\n",
      "  - start_positions: scalar value = 0\n",
      "  - end_positions: scalar value = 0\n",
      "  - overflow_to_sample_mapping: scalar value = 0\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š LABEL DISTRIBUTION IN PREPROCESSED DATA\n",
      "================================================================================\n",
      "Chunks with answer: 1,265 (25.3%)\n",
      "Chunks without answer: 3,735 (74.7%)\n",
      "\n",
      "ðŸ’¡ This is expected! Most chunks don't contain the answer due to sliding window.\n",
      "   Each question gets ~3-5 chunks, but only 1 contains the answer.\n",
      "\n",
      "================================================================================\n",
      "ðŸ” SAMPLE PREPROCESSED CHUNKS\n",
      "================================================================================\n",
      "\n",
      "Chunk 0 (from example 0):\n",
      "  Question: Ø¬Ø¯ÛŒØ¯ ÛÙˆØ§Ø¦ÛŒ Ø¬ÛØ§Ø²ÙˆÚº Ú©ÛŒ ØªØ¹Ù…ÛŒØ± Ù…ÛŒÚº Ú©ÛŒØ§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ØªÚ¾Ø§ØŸ...\n",
      "  Gold answer: 'ÚˆÙˆØ±Ø§Ù„ÙˆÙ…ÛŒÙ†'\n",
      "  This chunk's label: '[NO ANSWER]'\n",
      "  Positions: [0, 0]\n",
      "\n",
      "Chunk 10 (from example 3):\n",
      "  Question: Ù†ÛŒÙˆ Ù…ÛŒÚ©Ø³ÛŒÚ©Ùˆ Ù…ÛŒÚº Ø§ÛŒÚ© Ø§ÙØ±ÛŒÙ‚ÛŒ Ø´ÛØ± Ù¾Ø± Ø­Ù…Ù„Û Ú©Ø±Ù†Û’ ÙˆØ§Ù„Û’ Ù…ÛŒÚ©Ø³ÛŒÚ©Ù† Ø¬Ù†Ø±...\n",
      "  Gold answer: ''\n",
      "  This chunk's label: '[NO ANSWER]'\n",
      "  Positions: [0, 0]\n",
      "\n",
      "Chunk 20 (from example 6):\n",
      "  Question: Ù…Ø§Ø±ÙˆÙ„ Ú©ÛŒ Ø¬Ø§Ø³ÙˆØ³ÛŒ ÙÚ©Ø´Ù† Ù…Ø²Ø§Ø­ÛŒÛ Ú©Ø³ ÛÛŒÙ†ÚˆÙ„ Ú©Û’ ØªØ­Øª Ø´Ø§Ø¦Ø¹ Ú©ÛŒ Ú¯Ø¦ÛŒÚºØŸ...\n",
      "  Gold answer: 'Ù…Ø§Ø±ÙˆÙ„ Ø§Ø³Ø±Ø§Ø± Ú©Ø§Ù…Ú©Ø³'\n",
      "  This chunk's label: 'Ù…Ø§Ø±ÙˆÙ„ Ø§Ø³Ø±Ø§Ø± Ú©Ø§Ù…Ú©Ø³'\n",
      "  Positions: [64, 80]\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Explore preprocessed dataset structure\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ“¦ PREPROCESSED DATASET STRUCTURE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Original training examples: {len(uqa_train):,}\")\n",
    "print(f\"Preprocessed training chunks: {len(processed_train):,}\")\n",
    "print(f\"Expansion ratio: {len(processed_train)/len(uqa_train):.2f}x\")\n",
    "print(f\"(Each example creates ~{len(processed_train)//len(uqa_train):.1f} chunks due to sliding window)\")\n",
    "\n",
    "print(f\"\\nFeatures in preprocessed data: {processed_train.column_names}\")\n",
    "print(f\"\\nFeature shapes (for one chunk):\")\n",
    "for col in processed_train.column_names:\n",
    "    sample = processed_train[0][col]\n",
    "    if isinstance(sample, list):\n",
    "        print(f\"  - {col}: list of {len(sample)} elements\")\n",
    "    else:\n",
    "        print(f\"  - {col}: scalar value = {sample}\")\n",
    "\n",
    "# Analyze label distribution\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š LABEL DISTRIBUTION IN PREPROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample_size = min(5000, len(processed_train))\n",
    "cls_idx = 0  # CLS is at position 0 for CANINE\n",
    "\n",
    "no_answer_chunks = 0\n",
    "answer_chunks = 0\n",
    "\n",
    "for i in range(sample_size):\n",
    "    start = processed_train[i]['start_positions']\n",
    "    end = processed_train[i]['end_positions']\n",
    "    if start == cls_idx and end == cls_idx:\n",
    "        no_answer_chunks += 1\n",
    "    else:\n",
    "        answer_chunks += 1\n",
    "\n",
    "print(f\"Chunks with answer: {answer_chunks:,} ({answer_chunks/sample_size*100:.1f}%)\")\n",
    "print(f\"Chunks without answer: {no_answer_chunks:,} ({no_answer_chunks/sample_size*100:.1f}%)\")\n",
    "print(f\"\\nðŸ’¡ This is expected! Most chunks don't contain the answer due to sliding window.\")\n",
    "print(f\"   Each question gets ~3-5 chunks, but only 1 contains the answer.\")\n",
    "\n",
    "# Show a few preprocessed examples\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ” SAMPLE PREPROCESSED CHUNKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in [0, 10, 20]:\n",
    "    chunk = processed_train[i]\n",
    "    orig_idx = chunk['overflow_to_sample_mapping']\n",
    "    original = uqa_train[orig_idx]\n",
    "    \n",
    "    input_ids = chunk['input_ids']\n",
    "    start_pos = chunk['start_positions']\n",
    "    end_pos = chunk['end_positions']\n",
    "    \n",
    "    # Decode\n",
    "    if start_pos == 0 and end_pos == 0:\n",
    "        labeled = \"[NO ANSWER]\"\n",
    "    else:\n",
    "        labeled = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True).strip()\n",
    "    \n",
    "    print(f\"\\nChunk {i} (from example {orig_idx}):\")\n",
    "    print(f\"  Question: {original['question'][:60]}...\")\n",
    "    print(f\"  Gold answer: '{original['answer']}'\")\n",
    "    print(f\"  This chunk's label: '{labeled}'\")\n",
    "    print(f\"  Positions: [{start_pos}, {end_pos}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Understanding the Preprocessed Dataset\n",
    "\n",
    "Let's explore the full preprocessed dataset structure that gets fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:22.079176Z",
     "iopub.status.busy": "2025-12-03T19:08:22.078888Z",
     "iopub.status.idle": "2025-12-03T19:08:22.082508Z",
     "shell.execute_reply": "2025-12-03T19:08:22.081826Z",
     "shell.execute_reply.started": "2025-12-03T19:08:22.079152Z"
    },
    "id": "Yy3SiWwCabEi",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# processed_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "05d936e2dc9d412b8637c174a3c0be64",
      "7e0b41aa16f241a4ba3bb8a2f3525984",
      "2bebe7e1f3f341dfaabf29963d2c5995",
      "41b300b02ed2413ba80865aaa99ece2a",
      "b4740a7137e742d687e2075b60d2be8a",
      "80132c8e4fa743fca850936ecfebc7f7",
      "303bb3d75f7d4e94aeb60c5491ea6e61",
      "d7463faafecf4e46a87dc6863a646cea",
      "bc199cddba714aeda650d97fef015a14",
      "1a508a6457bb460ba17d5adb0a9e9f85",
      "3aac2656907a416291a622717ccaf929",
      "fa1af70d9c95443c9f09666359ba3769",
      "ddb717fd4dbc40c6bd8422a02f925060",
      "6d1342eeaf4f4f0489fe0746ceaaeb09",
      "a10683e5c1164f349cbdc75b1567994c",
      "71cfe2c8df474badb255d7d28da04348",
      "077fbb403e5f4069841e558a3cc0c065",
      "b068b9fac9f24eca9bd430bab30ea70c",
      "8cbfc6f4ec434674ac59d3fbdbddcd3b",
      "4d675a6788b641c6a09604ef17514dec",
      "bd9b9f21be9744c49d99ac4bc76f11e1",
      "89469ab4bd6f48d8b9aa369473c7230f"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:22.083472Z",
     "iopub.status.busy": "2025-12-03T19:08:22.083214Z",
     "iopub.status.idle": "2025-12-03T19:08:22.108730Z",
     "shell.execute_reply": "2025-12-03T19:08:22.107907Z",
     "shell.execute_reply.started": "2025-12-03T19:08:22.083450Z"
    },
    "id": "77ecdd17",
    "outputId": "602e648b-4a75-424b-da09-d58f3295a65e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# processed_train.save_to_disk(\"/kaggle/working/cache/processed_train_uqa\")\n",
    "# processed_val.save_to_disk(\"/kaggle/working/cache/processed_val_uqa\")   # cached it\n",
    "\n",
    "\n",
    "processed_train = load_from_disk(\"/kaggle/working/cache/processed_train_uqa\")\n",
    "processed_val = load_from_disk(\"/kaggle/working/cache/processed_val_uqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:22.109734Z",
     "iopub.status.busy": "2025-12-03T19:08:22.109444Z",
     "iopub.status.idle": "2025-12-03T19:08:22.113924Z",
     "shell.execute_reply": "2025-12-03T19:08:22.113309Z",
     "shell.execute_reply.started": "2025-12-03T19:08:22.109710Z"
    },
    "id": "c0e06e6b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:22.114797Z",
     "iopub.status.busy": "2025-12-03T19:08:22.114571Z",
     "iopub.status.idle": "2025-12-03T19:08:22.181587Z",
     "shell.execute_reply": "2025-12-03T19:08:22.181073Z",
     "shell.execute_reply.started": "2025-12-03T19:08:22.114781Z"
    },
    "id": "ba9eeeed",
    "outputId": "27071b6e-b703-4b47-9288-9a1c6f3eba55",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1033730 || all params: 133118212 || trainable%: 0.7765503941714602\n"
     ]
    }
   ],
   "source": [
    "# build LoRA model\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.gradient_checkpointing_enable()\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:22.182615Z",
     "iopub.status.busy": "2025-12-03T19:08:22.182262Z",
     "iopub.status.idle": "2025-12-03T19:08:22.201104Z",
     "shell.execute_reply": "2025-12-03T19:08:22.200411Z",
     "shell.execute_reply.started": "2025-12-03T19:08:22.182594Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸŽ“ MODEL TRAINING DATA FLOW\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ BATCH STRUCTURE\n",
      "--------------------------------------------------------------------------------\n",
      "Batch size: 4 chunks\n",
      "Each chunk in the batch contains:\n",
      "  - input_ids: shape (4, 384)\n",
      "  - attention_mask: shape (4, 384)\n",
      "  - token_type_ids: shape (4, 384)\n",
      "  - start_positions: shape (4,)\n",
      "  - end_positions: shape (4,)\n",
      "  - overflow_to_sample_mapping: shape (4,)\n",
      "\n",
      "2ï¸âƒ£ WHAT THE MODEL RECEIVES (for 1 chunk in batch)\n",
      "--------------------------------------------------------------------------------\n",
      "Input IDs: 384 tokens\n",
      "  First 10 token IDs: [57344, 1580, 1583, 1740, 1583, 32, 1729, 1608, 1575, 1574]\n",
      "\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]...\n",
      "  (1=attend to token, 0=ignore padding)\n",
      "\n",
      "Token type IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...\n",
      "  (0=question tokens, 1=context tokens)\n",
      "\n",
      "3ï¸âƒ£ TRAINING TARGETS (what model learns to predict)\n",
      "--------------------------------------------------------------------------------\n",
      "Target start position: 0\n",
      "Target end position: 0\n",
      "\n",
      "ðŸ’¡ The model learns to output these exact positions!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Show what the model sees during training\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸŽ“ MODEL TRAINING DATA FLOW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Take one batch from preprocessed data\n",
    "batch_size = 4\n",
    "sample_batch = processed_train.select(range(batch_size))\n",
    "\n",
    "print(f\"\\n1ï¸âƒ£ BATCH STRUCTURE\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Batch size: {batch_size} chunks\")\n",
    "print(f\"Each chunk in the batch contains:\")\n",
    "\n",
    "# Show batch structure\n",
    "for key in sample_batch.column_names:\n",
    "    sample_value = sample_batch[0][key]\n",
    "    if isinstance(sample_value, list):\n",
    "        print(f\"  - {key}: shape ({batch_size}, {len(sample_value)})\")\n",
    "    else:\n",
    "        print(f\"  - {key}: shape ({batch_size},)\")\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£ WHAT THE MODEL RECEIVES (for 1 chunk in batch)\")\n",
    "print(\"-\"*80)\n",
    "example_idx = 0\n",
    "print(f\"Input IDs: {len(sample_batch[example_idx]['input_ids'])} tokens\")\n",
    "print(f\"  First 10 token IDs: {sample_batch[example_idx]['input_ids'][:10]}\")\n",
    "print(f\"\\nAttention mask: {sample_batch[example_idx]['attention_mask'][:20]}...\")\n",
    "print(f\"  (1=attend to token, 0=ignore padding)\")\n",
    "print(f\"\\nToken type IDs: {sample_batch[example_idx]['token_type_ids'][:20]}...\")\n",
    "print(f\"  (0=question tokens, 1=context tokens)\")\n",
    "\n",
    "print(f\"\\n3ï¸âƒ£ TRAINING TARGETS (what model learns to predict)\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Target start position: {sample_batch[example_idx]['start_positions']}\")\n",
    "print(f\"Target end position: {sample_batch[example_idx]['end_positions']}\")\n",
    "print(f\"\\nðŸ’¡ The model learns to output these exact positions!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## ðŸ§  Model Training: How Data Flows Through CANINE\n",
    "\n",
    "Let's understand what happens during training - how the preprocessed chunks get fed into the model and what it learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:22.202205Z",
     "iopub.status.busy": "2025-12-03T19:08:22.201955Z",
     "iopub.status.idle": "2025-12-03T19:08:22.218619Z",
     "shell.execute_reply": "2025-12-03T19:08:22.218006Z",
     "shell.execute_reply.started": "2025-12-03T19:08:22.202183Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_answer(text):\n",
    "    text = (text or \"\").lower()\n",
    "    def remove_articles(s):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n",
    "    def remove_punctuation(s):\n",
    "        return \"\".join(ch for ch in s if ch not in string.punctuation)\n",
    "    def white_space_fix(s):\n",
    "        return \" \".join(s.split())\n",
    "    return white_space_fix(remove_articles(remove_punctuation(text)))\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    gold_tokens = normalize_answer(ground_truth).split()\n",
    "    if not gold_tokens:\n",
    "        return 1.0 if not pred_tokens else 0.0\n",
    "    if not pred_tokens:\n",
    "        return 0.0\n",
    "    common = Counter(pred_tokens) & Counter(gold_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gold_tokens)\n",
    "    # BUGFIX: Prevent division by zero if both precision and recall are 0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def decode_prediction(input_ids, start_idx, end_idx, tokenizer):\n",
    "    # Dynamic CLS handling\n",
    "    cls_index = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n",
    "    \n",
    "    # No answer case (both point to CLS)\n",
    "    if start_idx == cls_index and end_idx == cls_index:\n",
    "        return \"\"\n",
    "    \n",
    "    # Invalid range (start after end) - treat as no answer\n",
    "    if start_idx > end_idx:\n",
    "        return \"\"\n",
    "    \n",
    "    # Defensive bounds checking\n",
    "    if start_idx < 0 or end_idx < 0:\n",
    "        return \"\"\n",
    "    if start_idx >= len(input_ids) or end_idx >= len(input_ids):\n",
    "        return \"\"\n",
    "    \n",
    "    # Clamp to valid range (additional safety)\n",
    "    start_idx = max(start_idx, 0)\n",
    "    end_idx = min(end_idx, len(input_ids) - 1)\n",
    "    \n",
    "    # Decode with inclusive slicing [start:end+1]\n",
    "    text = tokenizer.decode(input_ids[start_idx:end_idx + 1], skip_special_tokens=True)\n",
    "    return text.strip()\n",
    "\n",
    "def gold_answer(example):\n",
    "    if example[\"answer_start\"] == -1:\n",
    "        return \"\"\n",
    "    return example[\"answer\"]\n",
    "\n",
    "def edit_distance_score(prediction, ground_truth):\n",
    "    return Levenshtein.ratio(normalize_answer(prediction), normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def evaluate_checkpoint(checkpoint_path=None, model_instance=None, eval_dataset=None):\n",
    "    \"\"\"Evaluate either a checkpoint path (loads model) or a provided model instance.\n",
    "\n",
    "    - checkpoint_path: path to checkpoint folder\n",
    "    - model_instance: an in-memory model (preferably a PeftModel or CanineForQuestionAnswering)\n",
    "    - eval_dataset: optional dataset to evaluate; if None the default processed_val will be used\n",
    "    \"\"\"\n",
    "    if eval_dataset is None:\n",
    "        eval_dataset = processed_val\n",
    "\n",
    "    # If a model_instance is given, use it directly (avoid re-loading a fresh base model)\n",
    "    if model_instance is not None:\n",
    "        eval_model = model_instance\n",
    "    else:\n",
    "        base_model = CanineForQuestionAnswering.from_pretrained(model_name, trust_remote_code=False)\n",
    "        eval_model = get_peft_model(base_model, lora_config)\n",
    "        # Try loading adapter weights; fall back to PeftModel.from_pretrained if needed\n",
    "        try:\n",
    "            eval_model.load_adapter(checkpoint_path)\n",
    "        except Exception:\n",
    "            from peft import PeftModel\n",
    "            eval_model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "\n",
    "    eval_model.to(device)\n",
    "\n",
    "    eval_args = TrainingArguments(\n",
    "        # Small evaluation config; uses cpu/mps if no gpu during eval\n",
    "        output_dir=\"outputs/canine-s-uqa\",\n",
    "        per_device_eval_batch_size=16,\n",
    "        dataloader_drop_last=False,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # Run evaluation via a lightweight Trainer so prediction loop is standard\n",
    "    eval_trainer = Trainer(\n",
    "        model=eval_model,\n",
    "        args=eval_args,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    predictions = eval_trainer.predict(eval_dataset)\n",
    "    start_logits, end_logits = predictions.predictions\n",
    "    \n",
    "    # BUGFIX: Validate logits shape before processing\n",
    "    if len(start_logits) == 0 or len(end_logits) == 0:\n",
    "        print(\"âš ï¸ Warning: Empty logits received from model!\")\n",
    "        return {\"exact_match\": 0.0, \"f1\": 0.0, \"edit_distance\": 0.0}\n",
    "    \n",
    "    if start_logits.shape[0] != end_logits.shape[0]:\n",
    "        print(f\"âš ï¸ Warning: Mismatched logits shapes: {start_logits.shape} vs {end_logits.shape}\")\n",
    "        return {\"exact_match\": 0.0, \"f1\": 0.0, \"edit_distance\": 0.0}\n",
    "    \n",
    "    best_predictions = {}\n",
    "    for feature_index, feature in enumerate(eval_dataset):\n",
    "        # Defensive check: ensure feature_index is within logits bounds\n",
    "        if feature_index >= len(start_logits) or feature_index >= len(end_logits):\n",
    "            print(f\"âš ï¸ Warning: Feature index {feature_index} out of bounds (logits length: {len(start_logits)})\")\n",
    "            continue\n",
    "            \n",
    "        sample_idx = int(feature[\"overflow_to_sample_mapping\"])\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        \n",
    "        # BUGFIX: Validate logits arrays are non-empty before argmax\n",
    "        if len(start_logits[feature_index]) == 0 or len(end_logits[feature_index]) == 0:\n",
    "            print(f\"âš ï¸ Warning: Empty logits at feature {feature_index}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        start_idx = int(np.argmax(start_logits[feature_index]))\n",
    "        end_idx = int(np.argmax(end_logits[feature_index]))\n",
    "        score = float(start_logits[feature_index][start_idx] + end_logits[feature_index][end_idx])\n",
    "        prediction_text = decode_prediction(input_ids, start_idx, end_idx, tokenizer=tokenizer)\n",
    "        stored = best_predictions.get(sample_idx)\n",
    "        if stored is None or score > stored[0]:\n",
    "            best_predictions[sample_idx] = (score, prediction_text)\n",
    "\n",
    "    em_scores = []\n",
    "    f1_scores = []\n",
    "    edit_dist_scores = []\n",
    "    for sample_idx, (_, prediction_text) in best_predictions.items():\n",
    "        # BUGFIX: Validate sample_idx is within dataset bounds\n",
    "        if sample_idx >= len(uqa_val):\n",
    "            print(f\"âš ï¸ Warning: sample_idx {sample_idx} out of bounds (dataset size: {len(uqa_val)})\")\n",
    "            continue\n",
    "            \n",
    "        reference = gold_answer(uqa_val[int(sample_idx)])\n",
    "        em_scores.append(exact_match_score(prediction_text, reference))\n",
    "        f1_scores.append(f1_score(prediction_text, reference))\n",
    "        edit_dist_scores.append(edit_distance_score(prediction_text, reference))\n",
    "\n",
    "    em = float(np.mean(em_scores)) if em_scores else 0.0\n",
    "    f1 = float(np.mean(f1_scores)) if f1_scores else 0.0\n",
    "    edit_dist = float(np.mean(edit_dist_scores)) if edit_dist_scores else 0.0\n",
    "    print(f\"Examples evaluated: {len(em_scores)}\")\n",
    "    print(f\"Exact Match: {em * 100:.2f}\")\n",
    "    print(f\"F1: {f1 * 100:.2f}\")\n",
    "    print(f\"Edit Distance (normalized): {edit_dist * 100:.2f}\")\n",
    "    return {\"exact_match\": em, \"f1\": f1, \"edit_distance\": edit_dist}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:22.219585Z",
     "iopub.status.busy": "2025-12-03T19:08:22.219367Z",
     "iopub.status.idle": "2025-12-03T19:08:22.261234Z",
     "shell.execute_reply": "2025-12-03T19:08:22.260704Z",
     "shell.execute_reply.started": "2025-12-03T19:08:22.219571Z"
    },
    "id": "c4abaaab",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/canine-s-uqa\",\n",
    "\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    # OPTIMIZED HYPERPARAMETERS (hardware-constrained)\n",
    "    num_train_epochs=2,  # Increased from 1 to 2 (critical for convergence)\n",
    "    learning_rate=5e-5,  # Reduced from 3e-4 to 5e-5 (prevents overshooting)\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    eval_strategy=\"no\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,  # Increased from 500 to 1000 (reduces checkpoint overhead)\n",
    "    logging_steps=50,\n",
    "    \n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"VohraAK/canine-s-uqa\",\n",
    "    hub_strategy=\"checkpoint\",\n",
    "    )\n",
    "\n",
    "class CustomEvalCallback(TrainerCallback):\n",
    "    def __init__(self, eval_func, eval_dataset, use_in_memory_model=True, verbose=True):\n",
    "        self.eval_func = eval_func\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.use_in_memory_model = use_in_memory_model\n",
    "        self.verbose = verbose\n",
    "        # trainer reference (set after trainer exists)\n",
    "        self.trainer = None\n",
    "\n",
    "    def on_save(self, args, state, control, model=None, **kwargs):\n",
    "        checkpoint_path = f\"{args.output_dir}/checkpoint-{state.global_step}\"\n",
    "        if self.verbose:\n",
    "            print(f\"\\nðŸ” Running custom evaluation at step {state.global_step}...\")\n",
    "\n",
    "        # Prefer evaluating the in-memory trainer model (fast + avoids re-loading)\n",
    "        if self.use_in_memory_model and self.trainer is not None:\n",
    "            if self.verbose:\n",
    "                print(\"Using in-memory model for evaluation (no reloading).\")\n",
    "            try:\n",
    "                metrics = self.eval_func(checkpoint_path=None, model_instance=self.trainer.model, eval_dataset=self.eval_dataset)\n",
    "            except Exception as e:\n",
    "                print(\"âš ï¸ in-memory evaluation failed, falling back to checkpoint load:\", e)\n",
    "                metrics = self.eval_func(checkpoint_path)\n",
    "        else:\n",
    "            metrics = self.eval_func(checkpoint_path)\n",
    "\n",
    "        # record metrics in state.log_history\n",
    "        state.log_history.append({\n",
    "            \"step\": state.global_step,\n",
    "            \"eval_exact_match\": metrics.get(\"exact_match\"),\n",
    "            \"eval_f1\": metrics.get(\"f1\"),\n",
    "            \"eval_edit_distance\": metrics.get(\"edit_distance\"),\n",
    "        })\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"âœ… Step {state.global_step}: EM={metrics.get('exact_match',0)*100:.2f}, F1={metrics.get('f1',0)*100:.2f}, EditDist={metrics.get('edit_distance',0)*100:.2f}\")\n",
    "\n",
    "        # Update trainer_state.json to include custom metrics\n",
    "        state_path = f\"{checkpoint_path}/trainer_state.json\"\n",
    "        try:\n",
    "            with open(state_path, 'r') as f:\n",
    "                state_dict = json.load(f)\n",
    "            state_dict['log_history'] = state.log_history\n",
    "            with open(state_path, 'w') as f:\n",
    "                json.dump(state_dict, f, indent=2)\n",
    "            if self.verbose:\n",
    "                print(f\"ðŸ’¾ Updated trainer_state.json with custom metrics\")\n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                print(f\"âš ï¸  Warning: Could not update trainer_state.json: {e}\")\n",
    "\n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(f\"â˜ï¸  Pushing checkpoint-{state.global_step} to Hub...\")\n",
    "            api = HfApi()\n",
    "            api.upload_folder(\n",
    "                folder_path=checkpoint_path,\n",
    "                repo_id=args.hub_model_id,\n",
    "                path_in_repo=f\"checkpoint-{state.global_step}\",\n",
    "                commit_message=f\"Add checkpoint {state.global_step} (EM={metrics.get('exact_match',0)*100:.1f}%, F1={metrics.get('f1',0)*100:.1f}%)\",\n",
    "                repo_type=\"model\"\n",
    "            )\n",
    "            if self.verbose:\n",
    "                print(f\"âœ… Pushed checkpoint-{state.global_step} to Hub\")\n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                print(f\"âš ï¸  Warning: Could not push to Hub: {e}\")\n",
    "\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:22.262143Z",
     "iopub.status.busy": "2025-12-03T19:08:22.261883Z",
     "iopub.status.idle": "2025-12-03T19:08:22.678077Z",
     "shell.execute_reply": "2025-12-03T19:08:22.677536Z",
     "shell.execute_reply.started": "2025-12-03T19:08:22.262121Z"
    },
    "id": "055f5dda",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer_cb = CustomEvalCallback(evaluate_checkpoint, processed_val, use_in_memory_model=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_train,\n",
    "    eval_dataset=processed_val,\n",
    "    callbacks=[trainer_cb],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-12-03T19:08:22.679285Z",
     "iopub.status.busy": "2025-12-03T19:08:22.679049Z",
     "iopub.status.idle": "2025-12-03T19:44:25.641357Z",
     "shell.execute_reply": "2025-12-03T19:44:25.640596Z",
     "shell.execute_reply.started": "2025-12-03T19:08:22.679261Z"
    },
    "id": "TOUimesUX5Re",
    "outputId": "cfa62dcd-8eb4-475a-910b-1c38a3894cc2",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7313' max='7313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7313/7313 36:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.293300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.658700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.462600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.354500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.146500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.937400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.814500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.802200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.592300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.574600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.622100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.489500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.398100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.194300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.230500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>3.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.017100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.990400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.906900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.971200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.937400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.804800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.915400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.905300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.922100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.960200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.715900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.848400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.849900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.793200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.619300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>2.776900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.712000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>2.597100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.443700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>2.563600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.486200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>2.498200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.518600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>2.444800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.678400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>2.640200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.330500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>2.556700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.434300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>2.482500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.618900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>2.556200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.681100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2.436500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.527800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>2.655300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.512400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>2.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.368300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>2.477200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.439000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>2.341400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.414400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>2.554700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.319900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>2.444300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.344100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>2.493900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.628300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>2.345100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>2.456200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.428700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>2.438300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.376500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>2.438400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>2.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.485700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>2.578900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>2.334400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>2.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.557400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>2.498000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>2.523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>2.407000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.285800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>2.489800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>2.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.441500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>2.447500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>2.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>2.483700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.473400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>2.319500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>2.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>2.434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.495600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>2.585600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>2.433400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>2.371100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>2.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>2.434300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>2.340400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.451900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>2.324500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.464500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>2.521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>2.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>2.485400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>2.352600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.351300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>2.464600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>2.371800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>2.447400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.418700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>2.537300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>2.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>2.437000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.392500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>2.290500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>2.292200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>2.269800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.287800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>2.395600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>2.160900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.462100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>2.319600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>2.536500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>2.366900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.386800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>2.339500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>2.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>2.365200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.395500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>2.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>2.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>2.389500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>2.517600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>2.305800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running custom evaluation at step 500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_111/4053719186.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  eval_trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples evaluated: 2000\n",
      "Exact Match: 32.35\n",
      "F1: 32.45\n",
      "Edit Distance (normalized): 32.76\n",
      "âœ… Step 500: EM=32.35, F1=32.45, EditDist=32.76\n",
      "ðŸ’¾ Updated trainer_state.json with custom metrics\n",
      "â˜ï¸  Pushing checkpoint-500 to Hub...\n",
      "âœ… Pushed checkpoint-500 to Hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running custom evaluation at step 1000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_111/4053719186.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  eval_trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples evaluated: 2000\n",
      "Exact Match: 32.75\n",
      "F1: 32.78\n",
      "Edit Distance (normalized): 32.88\n",
      "âœ… Step 1000: EM=32.75, F1=32.78, EditDist=32.88\n",
      "ðŸ’¾ Updated trainer_state.json with custom metrics\n",
      "â˜ï¸  Pushing checkpoint-1000 to Hub...\n",
      "âœ… Pushed checkpoint-1000 to Hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running custom evaluation at step 1500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_111/4053719186.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  eval_trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples evaluated: 2000\n",
      "Exact Match: 32.90\n",
      "F1: 32.97\n",
      "Edit Distance (normalized): 33.07\n",
      "âœ… Step 1500: EM=32.90, F1=32.97, EditDist=33.07\n",
      "ðŸ’¾ Updated trainer_state.json with custom metrics\n",
      "â˜ï¸  Pushing checkpoint-1500 to Hub...\n",
      "âœ… Pushed checkpoint-1500 to Hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running custom evaluation at step 2000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_111/4053719186.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  eval_trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples evaluated: 2000\n",
      "Exact Match: 32.90\n",
      "F1: 32.97\n",
      "Edit Distance (normalized): 33.06\n",
      "âœ… Step 2000: EM=32.90, F1=32.97, EditDist=33.06\n",
      "ðŸ’¾ Updated trainer_state.json with custom metrics\n",
      "â˜ï¸  Pushing checkpoint-2000 to Hub...\n",
      "âœ… Pushed checkpoint-2000 to Hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running custom evaluation at step 2500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_111/4053719186.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  eval_trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples evaluated: 2000\n",
      "Exact Match: 33.10\n",
      "F1: 33.12\n",
      "Edit Distance (normalized): 33.17\n",
      "âœ… Step 2500: EM=33.10, F1=33.12, EditDist=33.17\n",
      "ðŸ’¾ Updated trainer_state.json with custom metrics\n",
      "â˜ï¸  Pushing checkpoint-2500 to Hub...\n",
      "âœ… Pushed checkpoint-2500 to Hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running custom evaluation at step 3000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_111/4053719186.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  eval_trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples evaluated: 2000\n",
      "Exact Match: 33.10\n",
      "F1: 33.14\n",
      "Edit Distance (normalized): 33.21\n",
      "âœ… Step 3000: EM=33.10, F1=33.14, EditDist=33.21\n",
      "ðŸ’¾ Updated trainer_state.json with custom metrics\n",
      "â˜ï¸  Pushing checkpoint-3000 to Hub...\n",
      "âœ… Pushed checkpoint-3000 to Hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running custom evaluation at step 3500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_111/4053719186.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  eval_trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples evaluated: 2000\n",
      "Exact Match: 33.15\n",
      "F1: 33.19\n",
      "Edit Distance (normalized): 33.25\n",
      "âœ… Step 3500: EM=33.15, F1=33.19, EditDist=33.25\n",
      "ðŸ’¾ Updated trainer_state.json with custom metrics\n",
      "â˜ï¸  Pushing checkpoint-3500 to Hub...\n",
      "âœ… Pushed checkpoint-3500 to Hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running custom evaluation at step 4000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_111/4053719186.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  eval_trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples evaluated: 2000\n",
      "Exact Match: 33.20\n",
      "F1: 33.24\n",
      "Edit Distance (normalized): 33.30\n",
      "âœ… Step 4000: EM=33.20, F1=33.24, EditDist=33.30\n",
      "ðŸ’¾ Updated trainer_state.json with custom metrics\n",
      "â˜ï¸  Pushing checkpoint-4000 to Hub...\n",
      "âœ… Pushed checkpoint-4000 to Hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running custom evaluation at step 4500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_111/4053719186.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  eval_trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples evaluated: 2000\n",
      "Exact Match: 33.25\n",
      "F1: 33.29\n",
      "Edit Distance (normalized): 33.34\n",
      "âœ… Step 4500: EM=33.25, F1=33.29, EditDist=33.34\n",
      "ðŸ’¾ Updated trainer_state.json with custom metrics\n",
      "â˜ï¸  Pushing checkpoint-4500 to Hub...\n",
      "âœ… Pushed checkpoint-4500 to Hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running custom evaluation at step 5000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_111/4053719186.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  eval_trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples evaluated: 2000\n",
      "Exact Match: 33.30\n",
      "F1: 33.33\n",
      "Edit Distance (normalized): 33.37\n",
      "âœ… Step 5000: EM=33.30, F1=33.33, EditDist=33.37\n",
      "ðŸ’¾ Updated trainer_state.json with custom metrics\n",
      "â˜ï¸  Pushing checkpoint-5000 to Hub...\n",
      "âœ… Pushed checkpoint-5000 to Hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running custom evaluation at step 5500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_111/4053719186.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  eval_trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples evaluated: 2000\n",
      "Exact Match: 33.35\n",
      "F1: 33.36\n",
      "Edit Distance (normalized): 33.39\n",
      "âœ… Step 5500: EM=33.35, F1=33.36, EditDist=33.39\n",
      "ðŸ’¾ Updated trainer_state.json with custom metrics\n",
      "â˜ï¸  Pushing checkpoint-5500 to Hub...\n",
      "âœ… Pushed checkpoint-5500 to Hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running custom evaluation at step 6000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_111/4053719186.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  eval_trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples evaluated: 2000\n",
      "Exact Match: 33.30\n",
      "F1: 33.31\n",
      "Edit Distance (normalized): 33.34\n",
      "âœ… Step 6000: EM=33.30, F1=33.31, EditDist=33.34\n",
      "ðŸ’¾ Updated trainer_state.json with custom metrics\n",
      "â˜ï¸  Pushing checkpoint-6000 to Hub...\n",
      "âœ… Pushed checkpoint-6000 to Hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running custom evaluation at step 6500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_111/4053719186.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  eval_trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples evaluated: 2000\n",
      "Exact Match: 33.35\n",
      "F1: 33.36\n",
      "Edit Distance (normalized): 33.41\n",
      "âœ… Step 6500: EM=33.35, F1=33.36, EditDist=33.41\n",
      "ðŸ’¾ Updated trainer_state.json with custom metrics\n",
      "â˜ï¸  Pushing checkpoint-6500 to Hub...\n",
      "âœ… Pushed checkpoint-6500 to Hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running custom evaluation at step 7000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_111/4053719186.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  eval_trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples evaluated: 2000\n",
      "Exact Match: 33.35\n",
      "F1: 33.38\n",
      "Edit Distance (normalized): 33.43\n",
      "âœ… Step 7000: EM=33.35, F1=33.38, EditDist=33.43\n",
      "ðŸ’¾ Updated trainer_state.json with custom metrics\n",
      "â˜ï¸  Pushing checkpoint-7000 to Hub...\n",
      "âœ… Pushed checkpoint-7000 to Hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running custom evaluation at step 7313...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_111/4053719186.py:103: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  eval_trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples evaluated: 2000\n",
      "Exact Match: 33.35\n",
      "F1: 33.38\n",
      "Edit Distance (normalized): 33.43\n",
      "âœ… Step 7313: EM=33.35, F1=33.38, EditDist=33.43\n",
      "ðŸ’¾ Updated trainer_state.json with custom metrics\n",
      "â˜ï¸  Pushing checkpoint-7313 to Hub...\n",
      "âœ… Pushed checkpoint-7313 to Hub\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7313, training_loss=2.688961574940973, metrics={'train_runtime': 2162.4746, 'train_samples_per_second': 54.102, 'train_steps_per_second': 3.382, 'total_flos': 2.9095953406848e+16, 'train_loss': 2.688961574940973, 'epoch': 1.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "cc44692c-6652-4cda-9ba4-8a03acdab88d"
   },
   "source": [
    "### Diagnosing Preprocessing Functions!!!\n",
    "\n",
    "These functions are just analysing the preprocessing logic above, they're just using the base model, NOT our trained model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-12-03T19:44:25.643047Z",
     "iopub.status.busy": "2025-12-03T19:44:25.642303Z",
     "iopub.status.idle": "2025-12-03T19:44:25.647828Z",
     "shell.execute_reply": "2025-12-03T19:44:25.647249Z",
     "shell.execute_reply.started": "2025-12-03T19:44:25.643026Z"
    },
    "id": "49f3717d",
    "outputId": "38f435a4-1b55-4c2b-b6a5-86540fc23755",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Diagnostic cell (fixed): Investigate preprocessing and truncation for many samples\n",
    "# import random\n",
    "# import pandas as pd\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # Set display options to see full Urdu text\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# try:\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"google/canine-s\")\n",
    "# except Exception:\n",
    "#     tokenizer = None\n",
    "\n",
    "# num_samples = 20000  # Number of samples to check\n",
    "# results = []\n",
    "\n",
    "# for split_name, orig_data, proc_data in [\n",
    "#     (\"train\", uqa_train, processed_train),\n",
    "#     (\"val\", uqa_val, processed_val)\n",
    "# ]:\n",
    "#     # Sample random indices\n",
    "#     if len(proc_data) < num_samples:\n",
    "#         current_indices = range(len(proc_data))\n",
    "#     else:\n",
    "#         current_indices = random.sample(range(len(proc_data)), num_samples)\n",
    "\n",
    "#     for idx in current_indices:\n",
    "#         proc = proc_data[idx]\n",
    "#         # Use overflow_to_sample_mapping to get the correct original index\n",
    "#         orig_idx = proc[\"overflow_to_sample_mapping\"]\n",
    "#         orig = orig_data[orig_idx]\n",
    "\n",
    "#         input_ids = proc[\"input_ids\"]\n",
    "#         start_pos = proc[\"start_positions\"]\n",
    "#         end_pos = proc[\"end_positions\"]\n",
    "\n",
    "#         gold_answer = orig.get(\"gold_answer\", orig.get(\"answer\", \"\"))\n",
    "#         question = orig.get(\"question\", \"\")\n",
    "\n",
    "#         # Decode input_ids to text (for debugging context)\n",
    "#         if tokenizer:\n",
    "#             decoded_text = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "#         else:\n",
    "#             decoded_text = str(input_ids)\n",
    "\n",
    "#         # Extract predicted answer span\n",
    "#         if 0 <= start_pos < len(input_ids) and 0 <= end_pos < len(input_ids):\n",
    "#             if tokenizer:\n",
    "#                 pred_span = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True)\n",
    "#             else:\n",
    "#                 pred_span = str(input_ids[start_pos:end_pos+1])\n",
    "#         else:\n",
    "#             pred_span = \"[CLS]\" # Represents no answer found in this chunk or invalid\n",
    "\n",
    "#         # Check if pred_span matches gold answer\n",
    "#         # We strip() to ignore minor whitespace differences\n",
    "#         pred_matches_gold = pred_span.strip() == gold_answer.strip()\n",
    "\n",
    "#         # Check if gold is even reachable in this chunk\n",
    "#         gold_in_decoded = gold_answer in decoded_text\n",
    "\n",
    "#         results.append({\n",
    "#             \"Split\": split_name,\n",
    "#             \"Question\": question,\n",
    "#             \"Gold Answer\": gold_answer,\n",
    "#             \"Extracted Answer\": pred_span,\n",
    "#             \"Match\": pred_matches_gold,\n",
    "#             \"Gold Reachable\": gold_in_decoded,\n",
    "#             \"orig_idx\": orig_idx\n",
    "#         })\n",
    "\n",
    "# # Create DataFrame\n",
    "# results_df = pd.DataFrame(results)\n",
    "\n",
    "# # --- SIDE BY SIDE COMPARISON ---\n",
    "\n",
    "# # 1. Filter for Solvable Mismatches (Gold was there, but we predicted wrong)\n",
    "# problem_cases = results_df[\n",
    "#     (results_df[\"Gold Reachable\"] == True) &\n",
    "#     (results_df[\"Match\"] == False)\n",
    "# ][[\"Question\", \"Gold Answer\", \"Extracted Answer\", \"Split\"]]\n",
    "\n",
    "# print(f\"ðŸ” Checked {len(results_df)} samples.\")\n",
    "# print(f\"âŒ Found {len(problem_cases)} cases where Gold was present but Extraction failed.\")\n",
    "\n",
    "# print(\"\\nðŸ“Š Side-by-Side Comparison (Top 20 Failures):\")\n",
    "# display(problem_cases.head(50))\n",
    "\n",
    "# print(\"\\nâœ… Side-by-Side Comparison (First 10 Rows - Mixed):\")\n",
    "# display(results_df[[\"Question\", \"Gold Answer\", \"Extracted Answer\", \"Match\"]].head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-03T19:44:25.648751Z",
     "iopub.status.busy": "2025-12-03T19:44:25.648530Z",
     "iopub.status.idle": "2025-12-03T19:44:25.665350Z",
     "shell.execute_reply": "2025-12-03T19:44:25.664778Z",
     "shell.execute_reply.started": "2025-12-03T19:44:25.648735Z"
    },
    "id": "e67abc12",
    "outputId": "c597ec41-a56e-4e5d-9eb6-e71bd0eafd38",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Accuracy: fraction of rows where extracted answer matches gold answer\n",
    "# accuracy = (results_df[\"Match\"]).mean()\n",
    "\n",
    "# # Precision: among rows where extracted answer is non-empty, fraction that matches gold\n",
    "# # We filter out cases where the model predicted nothing (empty string) or just whitespace\n",
    "# non_empty_pred = results_df[\"Extracted Answer\"].str.strip() != \"\"\n",
    "\n",
    "# # Avoid division by zero if no predictions were made\n",
    "# if non_empty_pred.sum() > 0:\n",
    "#     precision = (results_df[\"Match\"] & non_empty_pred).sum() / non_empty_pred.sum()\n",
    "# else:\n",
    "#     precision = 0.0\n",
    "\n",
    "# print(f\"Accuracy: {accuracy:.3f}\")\n",
    "# print(f\"Precision: {precision:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
