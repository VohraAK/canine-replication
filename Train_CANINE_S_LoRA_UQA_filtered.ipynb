{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0","cell_type":"code","source":"# %pip install peft evaluate transformers Levenshtein ipywidgets\n# %pip install protobuf==3.20.3\n# !rm -rf /kaggle/working/cache\n# !rm -rf /kaggle/working/outputs","metadata":{"execution":{"iopub.status.busy":"2025-12-12T11:38:11.246738Z","iopub.execute_input":"2025-12-12T11:38:11.247319Z","iopub.status.idle":"2025-12-12T11:38:11.250914Z","shell.execute_reply.started":"2025-12-12T11:38:11.247289Z","shell.execute_reply":"2025-12-12T11:38:11.249946Z"},"id":"c186240c","trusted":true},"outputs":[],"execution_count":283},{"id":"1","cell_type":"code","source":"# X\n\nimport os\nos.environ[\"TRANSFORMERS_DISABLE_CHAT_TEMPLATES\"] = \"1\"\nos.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\nos.environ[\"TRANSFORMERS_NO_ADDITIONAL_CHAT_TEMPLATES\"] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2025-12-12T11:38:11.255173Z","iopub.execute_input":"2025-12-12T11:38:11.255394Z","iopub.status.idle":"2025-12-12T11:38:11.269582Z","shell.execute_reply.started":"2025-12-12T11:38:11.255377Z","shell.execute_reply":"2025-12-12T11:38:11.268917Z"},"id":"cd8da8ab","trusted":true},"outputs":[],"execution_count":284},{"id":"2","cell_type":"code","source":"import random\nfrom datasets import load_dataset, load_from_disk\nfrom transformers import CanineTokenizer\nfrom peft import LoraConfig, TaskType, get_peft_model\nimport re\nimport string\nfrom collections import Counter\nimport numpy as np\nimport Levenshtein\n\nfrom transformers import TrainingArguments, Trainer, TrainerCallback\nimport json\nfrom huggingface_hub import HfApi, notebook_login, whoami","metadata":{"execution":{"iopub.status.busy":"2025-12-12T11:38:11.347028Z","iopub.execute_input":"2025-12-12T11:38:11.347748Z","iopub.status.idle":"2025-12-12T11:38:11.352152Z","shell.execute_reply.started":"2025-12-12T11:38:11.347707Z","shell.execute_reply":"2025-12-12T11:38:11.351476Z"},"id":"d87eba82","trusted":true},"outputs":[],"execution_count":285},{"id":"3","cell_type":"code","source":"# notebook_login()\n# whoami()","metadata":{"execution":{"iopub.status.busy":"2025-12-12T11:38:11.364531Z","iopub.execute_input":"2025-12-12T11:38:11.365223Z","iopub.status.idle":"2025-12-12T11:38:11.368972Z","shell.execute_reply.started":"2025-12-12T11:38:11.365198Z","shell.execute_reply":"2025-12-12T11:38:11.368243Z"},"id":"0e98cebe-4c08-4850-b3c1-1529564fdb1b","trusted":true},"outputs":[],"execution_count":286},{"id":"4","cell_type":"code","source":"from transformers import CanineTokenizer, CanineForQuestionAnswering\nimport torch\nmodel_name = 'google/canine-s'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n\ntokenizer = CanineTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=False)\nmodel = CanineForQuestionAnswering.from_pretrained(model_name, trust_remote_code=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-12T11:38:11.457646Z","iopub.execute_input":"2025-12-12T11:38:11.457928Z","iopub.status.idle":"2025-12-12T11:38:12.898240Z","shell.execute_reply.started":"2025-12-12T11:38:11.457876Z","shell.execute_reply":"2025-12-12T11:38:12.897620Z"},"id":"f2dd5a40","outputId":"140c30ea-575d-45cd-ea54-7818cdfe6bf5","trusted":true},"outputs":[{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":287},{"id":"f75a7e9b","cell_type":"code","source":"# filter out impossible questions\ndef filter_function(example):\n    return not example['is_impossible']","metadata":{"execution":{"iopub.status.busy":"2025-12-12T11:38:12.899378Z","iopub.execute_input":"2025-12-12T11:38:12.899596Z","iopub.status.idle":"2025-12-12T11:38:12.903391Z","shell.execute_reply.started":"2025-12-12T11:38:12.899579Z","shell.execute_reply":"2025-12-12T11:38:12.902595Z"},"trusted":true},"outputs":[],"execution_count":288},{"id":"5","cell_type":"code","source":"uqa_dataset = load_dataset(\"uqa/UQA\")\n\nuqa_dataset_filtered = uqa_dataset.filter(filter_function)\n\n# testing with filtered dataset, since that's what UQA canonical does...\nuqa_train = uqa_dataset_filtered[\"train\"].shuffle(seed=42).select(range(60000))\nuqa_val = uqa_dataset_filtered[\"validation\"].shuffle(seed=42).select(range(2000))\n\nprint(f\"ğŸ“Š Dataset after filtering:\")\nprint(f\"   Original train size: {len(uqa_dataset['train']):,}\")\nprint(f\"   Filtered train size: {len(uqa_dataset_filtered['train']):,}\")\nprint(f\"   Using for training: {len(uqa_train):,}\")\nprint(f\"   Validation size: {len(uqa_val):,}\")","metadata":{"execution":{"iopub.status.busy":"2025-12-12T11:38:12.904146Z","iopub.execute_input":"2025-12-12T11:38:12.904315Z","iopub.status.idle":"2025-12-12T11:38:13.666811Z","shell.execute_reply.started":"2025-12-12T11:38:12.904301Z","shell.execute_reply":"2025-12-12T11:38:13.666223Z"},"id":"d474e2e8","trusted":true},"outputs":[{"name":"stdout","text":"ğŸ“Š Dataset after filtering:\n   Original train size: 124,745\n   Filtered train size: 83,018\n   Using for training: 60,000\n   Validation size: 2,000\n","output_type":"stream"}],"execution_count":289},{"id":"297e1ff7-52f6-4981-b360-45141788f2f4","cell_type":"code","source":"# Check character-token alignment\nex = uqa_train[444]\ncontext = ex[\"context\"]\ncontext_tokens = tokenizer.encode(ex[\"context\"], add_special_tokens=False)\n\nprint(f\"Context length (characters): {len(context)}\")\nprint(f\"Context length (tokens): {len(context_tokens)}\")\nprint(f\"1:1 mapping: {len(context) == len(context_tokens)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:38:13.668622Z","iopub.execute_input":"2025-12-12T11:38:13.668837Z","iopub.status.idle":"2025-12-12T11:38:13.675508Z","shell.execute_reply.started":"2025-12-12T11:38:13.668820Z","shell.execute_reply":"2025-12-12T11:38:13.674934Z"}},"outputs":[{"name":"stdout","text":"Context length (characters): 1850\nContext length (tokens): 1850\n1:1 mapping: True\n","output_type":"stream"}],"execution_count":290},{"id":"6","cell_type":"code","source":"# Explore raw UQA dataset structure\nprint(\"=\"*80)\nprint(\"UQA DATASET STRUCTURE\")\nprint(\"=\"*80)\nprint(f\"Training set size: {len(uqa_train):,} examples\")\nprint(f\"Validation set size: {len(uqa_val):,} examples\")\nprint(f\"\\nDataset columns: {uqa_train.column_names}\")\nprint(\"\\n\" + \"=\"*80)\n\n# Show a few examples\nprint(\"\\nğŸ“ EXAMPLE 1 - Question with Answer\")\nprint(\"=\"*80)\nex1 = uqa_train[0]\nprint(f\"Question: {ex1['question']}\")\nprint(f\"\\nContext (first 300 chars): {ex1['context'][:300]}...\")\nprint(f\"\\nAnswer: '{ex1['answer']}'\")\nprint(f\"Answer starts at character position: {ex1['answer_start']}\")\n\n# Verify the answer extraction\nif ex1['answer_start'] != -1:\n    extracted = ex1['context'][ex1['answer_start']:ex1['answer_start']+len(ex1['answer'])]\n    print(f\"âœ“ Extracted from context: '{extracted}'\")\n    print(f\"âœ“ Match: {extracted == ex1['answer']}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nğŸ“ EXAMPLE 2 - Another Question\")\nprint(\"=\"*80)\nex2 = uqa_train[100]\nprint(f\"Question: {ex2['question']}\")\nprint(f\"\\nContext length: {len(ex2['context'])} characters\")\nprint(f\"Answer: '{ex2['answer']}'\")\nprint(f\"Answer starts at position: {ex2['answer_start']}\")\n\n# Show answer in context\nif ex2['answer_start'] != -1:\n    start = max(0, ex2['answer_start'] - 50)\n    end = min(len(ex2['context']), ex2['answer_start'] + len(ex2['answer']) + 50)\n    context_snippet = ex2['context'][start:end]\n    answer_pos = ex2['answer_start'] - start\n    print(f\"\\nContext around answer:\")\n    print(f\"...{context_snippet}...\")\n    print(f\"    {' '*answer_pos}{'~'*len(ex2['answer'])} (answer here)\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nğŸ“Š DATASET STATISTICS\")\nprint(\"=\"*80)\n\n# Compute some basic statistics\nimport numpy as np\nquestion_lengths = [len(ex['question']) for ex in uqa_train.select(range(1000))]\ncontext_lengths = [len(ex['context']) for ex in uqa_train.select(range(1000))]\nanswer_lengths = [len(ex['answer']) if ex['answer'] else 0 for ex in uqa_train.select(range(1000))]\nhas_answer = [ex['answer_start'] != -1 for ex in uqa_train.select(range(1000))]\n\nprint(f\"Question length (chars): mean={np.mean(question_lengths):.1f}, max={np.max(question_lengths)}\")\nprint(f\"Context length (chars): mean={np.mean(context_lengths):.1f}, max={np.max(context_lengths)}\")\nprint(f\"Answer length (chars): mean={np.mean(answer_lengths):.1f}, max={np.max(answer_lengths)}\")\nprint(f\"Questions with answers: {sum(has_answer)/len(has_answer)*100:.1f}%\")\nprint(f\"Questions without answers: {(1-sum(has_answer)/len(has_answer))*100:.1f}%\")","metadata":{"execution":{"iopub.status.busy":"2025-12-12T11:38:13.676198Z","iopub.execute_input":"2025-12-12T11:38:13.676441Z","iopub.status.idle":"2025-12-12T11:38:14.070836Z","shell.execute_reply.started":"2025-12-12T11:38:13.676417Z","shell.execute_reply":"2025-12-12T11:38:14.069944Z"},"trusted":true},"outputs":[{"name":"stdout","text":"================================================================================\nUQA DATASET STRUCTURE\n================================================================================\nTraining set size: 60,000 examples\nValidation set size: 2,000 examples\n\nDataset columns: ['id', 'title', 'context', 'question', 'is_impossible', 'answer', 'answer_start']\n\n================================================================================\n\nğŸ“ EXAMPLE 1 - Question with Answer\n================================================================================\nQuestion: Ù†Ø§Ù†Ø¬Ù†Ú¯ Ú©Ø§ Ø§ÛŒÚ¯Ø²ÛŒÚ©Ù¹Ùˆ Ù„ÛŒÚˆØ± Ú©ÙˆÙ† ÛÛ’ØŒ Ø³ÛŒÚ©Ø±Ù¹Ø±ÛŒ Ú©Û’ ØªØ­Øª Ú©Ø§Ù… Ú©Ø± Ø±ÛØ§ ÛÛ’ØŸ\n\nContext (first 300 chars): ÙÛŒ Ø§Ù„Ø­Ø§Ù„ ØŒ Ù†Ø§Ù†Ø¬Ù†Ú¯ Ú©ÛŒ Ø­Ú©ÙˆÙ…Øª Ú©Ø§ Ù…Ú©Ù…Ù„ Ù†Ø§Ù… Ù†Ø§Ù†Ø¬Ù†Ú¯ Ø³Ù¹ÛŒ Ú©ÛŒ Ù¾ÛŒÙ¾Ù„Ø² Ú¯ÙˆØ±Ù†Ù…Ù†Ù¹ ÛÛ’ Ø§ÙˆØ± ÛŒÛ Ø´ÛØ± Ø³ÛŒ Ù¾ÛŒ Ø³ÛŒ Ú©Û’ Ø§ÛŒÚ© Ù¾Ø§Ø±Ù¹ÛŒ Ø­Ú©Ù…Ø±Ø§Ù†ÛŒ Ú©Û’ ØªØ­Øª ÛÛ’ ØŒ Ø¬Ø³ Ù…ÛŒÚº Ø³ÛŒ Ù¾ÛŒ Ø³ÛŒ Ù†Ø§Ù†Ø¬Ù†Ú¯ Ú©Ù…ÛŒÙ¹ÛŒ Ø³ÛŒÚ©Ø±Ù¹Ø±ÛŒ Ø´ÛØ± Ú©Û’ ÚˆÛŒ ÙÛŒÚ©Ù¹Ùˆ Ú¯ÙˆØ±Ù†Ø± Ú©Û’ Ø·ÙˆØ± Ù¾Ø± Ø§ÙˆØ± Ù…ÛŒØ¦Ø± Ø³ÛŒÚ©Ø±Ù¹Ø±ÛŒ Ú©Û’ ØªØ­Øª Ú©Ø§Ù… Ú©Ø±Ù†Û’ ÙˆØ§Ù„ÛŒ Ø­Ú©ÙˆÙ…Øª Ú©Û’ Ø§ÛŒÚ¯Ø²ÛŒÚ©Ù¹Ùˆ Ø³Ø±Ø¨Ø±Ø§Û Ú©Û’ Ø·ÙˆØ± Ù¾Ø± ÛÛ’Û”...\n\nAnswer: 'Ù…ÛŒØ¦Ø±'\nAnswer starts at character position: 196\nâœ“ Extracted from context: 'Ù…ÛŒØ¦Ø±'\nâœ“ Match: True\n\n================================================================================\n\nğŸ“ EXAMPLE 2 - Another Question\n================================================================================\nQuestion: ÛŒÙˆØ±ÛŒÙ†ÛŒÙ… Ú©Ø§ Ú©ÙˆÙ† Ø³Ø§ Ø¢Ø¦Ø³ÙˆÙ¹ÙˆÙ¾ ØªÚ¾ÙˆØ±ÛŒØ¦Ù… Ø³Û’ ØªÛŒØ§Ø± Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ÛÛ’ØŸ\n\nContext length: 849 characters\nAnswer: 'ÛŒÙˆØ±ÛŒÙ†ÛŒÙ… 233'\nAnswer starts at position: 328\n\nContext around answer:\n... Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©ÛŒØ§ Ø¬Ø§Ø³Ú©ØªØ§ ÛÛ’Û” Ø§ÛŒÚ© Ø§ÙˆØ± ÙØ³ÛŒÙ„ÛŒ Ø¢Ø¦Ø³ÙˆÙ¹ÙˆÙ¾ ØŒ ÛŒÙˆØ±ÛŒÙ†ÛŒÙ… 233 ØŒ Ù‚Ø¯Ø±ØªÛŒ ØªÚ¾ÙˆØ±ÛŒØ¦Ù… Ø³Û’ ØªÛŒØ§Ø± Ú©ÛŒØ§ Ø¬Ø§Ø³Ú©ØªØ§ ÛÛ’ Ø§ÙˆØ± Ø¬ÙˆÛØ±ÛŒ Ù¹...\n                                                      ~~~~~~~~~~~ (answer here)\n\n================================================================================\n\nğŸ“Š DATASET STATISTICS\n================================================================================\nQuestion length (chars): mean=54.9, max=154\nContext length (chars): mean=713.8, max=2673\nAnswer length (chars): mean=17.1, max=162\nQuestions with answers: 100.0%\nQuestions without answers: 0.0%\n","output_type":"stream"}],"execution_count":291},{"id":"8","cell_type":"markdown","source":"---","metadata":{"id":"89c472d5"}},{"id":"9","cell_type":"markdown","source":"## Updated preprocessors!\n\nPreviously, we tried to apply the same approach we used in TYDIQA on UQA, the problem was the preprocessors were aligning the answer spans in units of **byte-level spans** instead of **character-level spans**. The calculations were adding byte-level offsets to the answer lengths, and since Urdu characters may be quantified in multiple bytes, the model was being fed the wrong spans -> GIGO!\n\nWe are now testing an updated preprocessor","metadata":{"id":"6e80a8d3"}},{"id":"10","cell_type":"code","source":"\"\"\"\nFIXED preprocessing function for UQA with CANINE-S.\nTyDiQA-style preprocessor adapted for UQA character offsets.\n\nKey fixes applied:\n1. Uses character-level offsets (UQA native format, no byte conversion needed)\n2. Fixed boundary check: uses `<` instead of `<=` for chunk_end\n3. Calculates gold_char_end as inclusive (answer_start + len(answer) - 1)\n4. Dynamic cls_index for no-answer cases\n5. Simplified context_offset calculation\n\nThis preprocessor passed all 200 real-world UQA examples in testing.\n\"\"\"\n\nMAX_SEQ_LENGTH = 384\nDOC_STRIDE = 64  # Using TyDiQA's value for proven results\n\ndef preprocess_uqa(examples, tokenizer, max_length=MAX_SEQ_LENGTH, doc_stride=DOC_STRIDE, model_obj=None, indices=None):\n    \"\"\"\n    TyDiQA-style preprocessor adapted for UQA (character offsets).\n    \n    Args:\n        examples: Batch with question, context, answer, answer_start fields\n        tokenizer: CanineTokenizer instance\n        max_length: Maximum sequence length (default 384)\n        doc_stride: Sliding window overlap (default 64)\n        model_obj: Optional model object (for compatibility)\n        indices: Optional example indices for overflow mapping\n    \n    Returns:\n        Dict with input_ids, attention_mask, token_type_ids, start_positions, \n        end_positions, overflow_to_sample_mapping\n    \"\"\"\n    questions = [q.strip() for q in examples[\"question\"]]\n    contexts = examples[\"context\"]\n    answers = examples[\"answer\"]\n    answer_starts = examples[\"answer_start\"]\n    \n    special_tokens = tokenizer.num_special_tokens_to_add(pair=True)\n    \n    encoded = {\n        \"input_ids\": [],\n        \"attention_mask\": [],\n        \"token_type_ids\": [],\n        \"start_positions\": [],\n        \"end_positions\": [],\n        \"overflow_to_sample_mapping\": [],\n    }\n    \n    for example_idx, (question, context, answer, answer_start) in enumerate(zip(questions, contexts, answers, answer_starts)):\n        question_tokens = tokenizer.encode(question, add_special_tokens=False)\n        context_tokens = tokenizer.encode(context, add_special_tokens=False)\n        \n        max_context_tokens = max_length - len(question_tokens) - special_tokens\n        if max_context_tokens <= 0 or not context_tokens:\n            continue\n        \n        # UQA uses character offsets (not bytes like TyDiQA)\n        if answer and answer_start != -1:\n            start_char = answer_start\n            end_char = answer_start + len(answer) - 1  # Inclusive\n            answer_span = (start_char, end_char)\n        else:\n            answer_span = None\n        \n        stride_tokens = max_context_tokens - doc_stride\n        if stride_tokens <= 0:\n            stride_tokens = max_context_tokens\n        \n        span_start = 0\n        context_length = len(context_tokens)\n        while span_start < context_length:\n            span_end = min(span_start + max_context_tokens, context_length)\n            context_chunk = context_tokens[span_start:span_end]\n            \n            input_ids = tokenizer.build_inputs_with_special_tokens(question_tokens, context_chunk)\n            token_type_ids = tokenizer.create_token_type_ids_from_sequences(question_tokens, context_chunk)\n            attention_mask = [1] * len(input_ids)\n            \n            cls_index = input_ids.index(tokenizer.cls_token_id)\n            context_offset = len(input_ids) - len(context_chunk) - 1\n            \n            if answer_span is None:\n                start_pos = cls_index\n                end_pos = cls_index\n            else:\n                start_char, end_char = answer_span\n                # CRITICAL FIX: Use < instead of <= for exclusive chunk_end\n                answer_in_chunk = start_char >= span_start and end_char < span_end\n                if answer_in_chunk:\n                    start_pos = context_offset + (start_char - span_start)\n                    end_pos = context_offset + (end_char - span_start)\n                else:\n                    start_pos = cls_index\n                    end_pos = cls_index\n            \n            padding = max_length - len(input_ids)\n            if padding > 0:\n                pad_id = tokenizer.pad_token_id\n                input_ids += [pad_id] * padding\n                attention_mask += [0] * padding\n                token_type_ids += [0] * padding\n            else:\n                input_ids = input_ids[:max_length]\n                attention_mask = attention_mask[:max_length]\n                token_type_ids = token_type_ids[:max_length]\n                if start_pos >= max_length or end_pos >= max_length:\n                    start_pos = cls_index\n                    end_pos = cls_index\n            \n            encoded[\"input_ids\"].append(input_ids)\n            encoded[\"attention_mask\"].append(attention_mask)\n            encoded[\"token_type_ids\"].append(token_type_ids)\n            encoded[\"start_positions\"].append(start_pos)\n            encoded[\"end_positions\"].append(end_pos)\n            encoded[\"overflow_to_sample_mapping\"].append(example_idx)\n            \n            if span_end == context_length:\n                break\n            span_start += stride_tokens\n    \n    return encoded\n","metadata":{"execution":{"iopub.status.busy":"2025-12-12T11:38:14.071934Z","iopub.execute_input":"2025-12-12T11:38:14.072203Z","iopub.status.idle":"2025-12-12T11:38:14.086410Z","shell.execute_reply.started":"2025-12-12T11:38:14.072184Z","shell.execute_reply":"2025-12-12T11:38:14.085659Z"},"trusted":true},"outputs":[],"execution_count":292},{"id":"12","cell_type":"code","source":"# LoRA config\nlora_config = LoraConfig(\n    task_type=TaskType.QUESTION_ANS,\n    r=8,   # shadowing tydiqa for now\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"query\", \"value\"], # shadowing tydiqa for now\n    bias=\"none\",\n    modules_to_save=[\"qa_outputs\"],\n)\n\ndef print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-12-12T11:38:14.087368Z","iopub.execute_input":"2025-12-12T11:38:14.087870Z","iopub.status.idle":"2025-12-12T11:38:14.103434Z","shell.execute_reply.started":"2025-12-12T11:38:14.087849Z","shell.execute_reply":"2025-12-12T11:38:14.102748Z"},"id":"a3e95eec","trusted":true},"outputs":[],"execution_count":293},{"id":"0ce2da1b","cell_type":"markdown","source":"### Preprocessing examples...","metadata":{}},{"id":"13","cell_type":"code","source":"\nprint(\"=\"*80)\nprint(\"ğŸ”¬ PREPROCESSING WALKTHROUGH - Single Example\")\nprint(\"=\"*80)\n\n# Take one example\nexample = uqa_train[0]\nprint(f\"\\n1ï¸âƒ£ ORIGINAL DATA\")\nprint(\"-\"*80)\nprint(f\"Question: {example['question']}\")\nprint(f\"Answer: '{example['answer']}'\")\nprint(f\"Answer position: {example['answer_start']}\")\nprint(f\"Context length: {len(example['context'])} characters\")\n\n# Preprocess it\nbatch = {\n    'question': [example['question']],\n    'context': [example['context']],\n    'answer': [example['answer']],\n    'answer_start': [example['answer_start']]\n}\nprocessed = preprocess_uqa(batch, tokenizer, indices=[0])\n\nprint(f\"\\n2ï¸âƒ£ AFTER PREPROCESSING\")\nprint(\"-\"*80)\nprint(f\"Number of chunks created: {len(processed['input_ids'])}\")\nprint(f\"(Sliding window creates multiple chunks per example)\")\n\n# Show first chunk in detail\nchunk_idx = 0\nprint(f\"\\n3ï¸âƒ£ CHUNK {chunk_idx} DETAILS\")\nprint(\"-\"*80)\nprint(f\"Input IDs length: {len(processed['input_ids'][chunk_idx])} tokens\")\nprint(f\"Start position: {processed['start_positions'][chunk_idx]}\")\nprint(f\"End position: {processed['end_positions'][chunk_idx]}\")\nprint(f\"Maps to original example: {processed['overflow_to_sample_mapping'][chunk_idx]}\")\n\n# Decode the inputs to show what the model sees\ninput_ids = processed['input_ids'][chunk_idx]\ndecoded_input = tokenizer.decode(input_ids, skip_special_tokens=False)\nprint(f\"\\n4ï¸âƒ£ DECODED INPUT (first 400 chars, with special tokens)\")\nprint(\"-\"*80)\nprint(decoded_input[:400] + \"...\")\n\n# Decode the labeled answer span\nstart_pos = processed['start_positions'][chunk_idx]\nend_pos = processed['end_positions'][chunk_idx]\ncls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n\nif start_pos == cls_idx and end_pos == cls_idx:\n    labeled_answer = \"[NO ANSWER IN THIS CHUNK]\"\nelse:\n    labeled_answer = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True)\n\nprint(f\"\\n5ï¸âƒ£ LABELED ANSWER SPAN IN THIS CHUNK\")\nprint(\"-\"*80)\nprint(f\"Gold answer: '{example['answer']}'\")\nprint(f\"Labeled span: '{labeled_answer}'\")\nprint(f\"Match: {labeled_answer.strip() == example['answer'].strip()}\")\n\n# Show all chunks for this example\nprint(f\"\\n6ï¸âƒ£ ALL CHUNKS FOR THIS EXAMPLE\")\nprint(\"-\"*80)\nfor i in range(len(processed['input_ids'])):\n    start = processed['start_positions'][i]\n    end = processed['end_positions'][i]\n    if start == cls_idx and end == cls_idx:\n        chunk_answer = \"[NO ANSWER]\"\n    else:\n        chunk_answer = tokenizer.decode(processed['input_ids'][i][start:end+1], skip_special_tokens=True).strip()\n    has_answer = \"âœ…\" if chunk_answer == example['answer'].strip() else \"âŒ\"\n    print(f\"  Chunk {i}: {has_answer} '{chunk_answer[:50]}'\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"execution":{"iopub.status.busy":"2025-12-12T11:38:14.104232Z","iopub.execute_input":"2025-12-12T11:38:14.104540Z","iopub.status.idle":"2025-12-12T11:38:14.126840Z","shell.execute_reply.started":"2025-12-12T11:38:14.104521Z","shell.execute_reply":"2025-12-12T11:38:14.126315Z"},"trusted":true},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ”¬ PREPROCESSING WALKTHROUGH - Single Example\n================================================================================\n\n1ï¸âƒ£ ORIGINAL DATA\n--------------------------------------------------------------------------------\nQuestion: Ù†Ø§Ù†Ø¬Ù†Ú¯ Ú©Ø§ Ø§ÛŒÚ¯Ø²ÛŒÚ©Ù¹Ùˆ Ù„ÛŒÚˆØ± Ú©ÙˆÙ† ÛÛ’ØŒ Ø³ÛŒÚ©Ø±Ù¹Ø±ÛŒ Ú©Û’ ØªØ­Øª Ú©Ø§Ù… Ú©Ø± Ø±ÛØ§ ÛÛ’ØŸ\nAnswer: 'Ù…ÛŒØ¦Ø±'\nAnswer position: 196\nContext length: 268 characters\n\n2ï¸âƒ£ AFTER PREPROCESSING\n--------------------------------------------------------------------------------\nNumber of chunks created: 1\n(Sliding window creates multiple chunks per example)\n\n3ï¸âƒ£ CHUNK 0 DETAILS\n--------------------------------------------------------------------------------\nInput IDs length: 384 tokens\nStart position: 259\nEnd position: 262\nMaps to original example: 0\n\n4ï¸âƒ£ DECODED INPUT (first 400 chars, with special tokens)\n--------------------------------------------------------------------------------\nî€€Ù†Ø§Ù†Ø¬Ù†Ú¯ Ú©Ø§ Ø§ÛŒÚ¯Ø²ÛŒÚ©Ù¹Ùˆ Ù„ÛŒÚˆØ± Ú©ÙˆÙ† ÛÛ’ØŒ Ø³ÛŒÚ©Ø±Ù¹Ø±ÛŒ Ú©Û’ ØªØ­Øª Ú©Ø§Ù… Ú©Ø± Ø±ÛØ§ ÛÛ’ØŸî€ÙÛŒ Ø§Ù„Ø­Ø§Ù„ ØŒ Ù†Ø§Ù†Ø¬Ù†Ú¯ Ú©ÛŒ Ø­Ú©ÙˆÙ…Øª Ú©Ø§ Ù…Ú©Ù…Ù„ Ù†Ø§Ù… Ù†Ø§Ù†Ø¬Ù†Ú¯ Ø³Ù¹ÛŒ Ú©ÛŒ Ù¾ÛŒÙ¾Ù„Ø² Ú¯ÙˆØ±Ù†Ù…Ù†Ù¹ ÛÛ’ Ø§ÙˆØ± ÛŒÛ Ø´ÛØ± Ø³ÛŒ Ù¾ÛŒ Ø³ÛŒ Ú©Û’ Ø§ÛŒÚ© Ù¾Ø§Ø±Ù¹ÛŒ Ø­Ú©Ù…Ø±Ø§Ù†ÛŒ Ú©Û’ ØªØ­Øª ÛÛ’ ØŒ Ø¬Ø³ Ù…ÛŒÚº Ø³ÛŒ Ù¾ÛŒ Ø³ÛŒ Ù†Ø§Ù†Ø¬Ù†Ú¯ Ú©Ù…ÛŒÙ¹ÛŒ Ø³ÛŒÚ©Ø±Ù¹Ø±ÛŒ Ø´ÛØ± Ú©Û’ ÚˆÛŒ ÙÛŒÚ©Ù¹Ùˆ Ú¯ÙˆØ±Ù†Ø± Ú©Û’ Ø·ÙˆØ± Ù¾Ø± Ø§ÙˆØ± Ù…ÛŒØ¦Ø± Ø³ÛŒÚ©Ø±Ù¹Ø±ÛŒ Ú©Û’ ØªØ­Øª Ú©Ø§Ù… Ú©Ø±Ù†Û’ ÙˆØ§Ù„ÛŒ Ø­Ú©ÙˆÙ…Øª Ú©Û’ Ø§ÛŒÚ¯Ø²ÛŒÚ©Ù¹Ùˆ Ø³Ø±Ø¨Ø±Ø§Û Ú©Û’ Ø·ÙˆØ± Ù¾Ø± ÛÛ’Û”î€\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000...\n\n5ï¸âƒ£ LABELED ANSWER SPAN IN THIS CHUNK\n--------------------------------------------------------------------------------\nGold answer: 'Ù…ÛŒØ¦Ø±'\nLabeled span: 'Ù…ÛŒØ¦Ø±'\nMatch: True\n\n6ï¸âƒ£ ALL CHUNKS FOR THIS EXAMPLE\n--------------------------------------------------------------------------------\n  Chunk 0: âœ… 'Ù…ÛŒØ¦Ø±'\n\n================================================================================\n","output_type":"stream"}],"execution_count":294},{"id":"14","cell_type":"markdown","source":"## ğŸ”§ Preprocessing Exploration: Raw Data â†’ Model Input\n\nNow let's see what happens during preprocessing - how we convert text to token IDs and create training labels.","metadata":{}},{"id":"25","cell_type":"code","source":"def normalize_answer(text):\n    text = (text or \"\").lower()\n    def remove_articles(s):\n        return re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n    def remove_punctuation(s):\n        return \"\".join(ch for ch in s if ch not in string.punctuation)\n    def white_space_fix(s):\n        return \" \".join(s.split())\n    return white_space_fix(remove_articles(remove_punctuation(text)))\n\ndef exact_match_score(prediction, ground_truth):\n    return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n\ndef f1_score(prediction, ground_truth):\n    pred_tokens = normalize_answer(prediction).split()\n    gold_tokens = normalize_answer(ground_truth).split()\n    if not gold_tokens:\n        return 1.0 if not pred_tokens else 0.0\n    if not pred_tokens:\n        return 0.0\n    common = Counter(pred_tokens) & Counter(gold_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0.0\n    precision = num_same / len(pred_tokens)\n    recall = num_same / len(gold_tokens)\n    # BUGFIX: Prevent division by zero if both precision and recall are 0\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n\ndef decode_prediction(input_ids, start_idx, end_idx):\n\n    global tokenizer\n    \n    # Dynamic CLS handling\n    cls_index = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n    \n    # No answer case (both point to CLS)\n    if start_idx == cls_index and end_idx == cls_index:\n        return \"\"\n    \n    # Invalid range (start after end) - treat as no answer\n    if start_idx > end_idx:\n        return \"\"\n    \n    # Defensive bounds checking\n    if start_idx < 0 or end_idx < 0:\n        return \"\"\n    if start_idx >= len(input_ids) or end_idx >= len(input_ids):\n        return \"\"\n    \n    # Clamp to valid range (additional safety)\n    start_idx = max(start_idx, 0)\n    end_idx = min(end_idx, len(input_ids) - 1)\n    \n    # Decode with inclusive slicing [start:end+1]\n    text = tokenizer.decode(input_ids[start_idx:end_idx + 1], skip_special_tokens=True)\n    return text.strip()\n\ndef gold_answer(example):\n    if example[\"answer_start\"] == -1:\n        return \"\"\n    return example[\"answer\"]\n\ndef edit_distance_score(prediction, ground_truth):\n    return Levenshtein.ratio(normalize_answer(prediction), normalize_answer(ground_truth))\n\n\n#--- CHANGED TO MATCH TYDIQA APPROACH\ndef evaluate_checkpoint(checkpoint_path=None):\n    \"\"\"\n    EXACT REPLICA of TyDiQA evaluation approach.\n    Loads checkpoint from disk (or uses provided path).\n    \"\"\"\n    # Load base model + trained adapter (TyDiQA approach)\n    base_model = CanineForQuestionAnswering.from_pretrained(\n        model_name, \n        trust_remote_code=False\n    )\n    \n    from peft import PeftModel\n    model = PeftModel.from_pretrained(base_model, checkpoint_path)\n    model.to(device)\n    \n    # Exact TyDiQA eval args\n    eval_args = TrainingArguments(\n        output_dir=\"outputs/canine-s-uqa-filtered\",\n        per_device_eval_batch_size=1,  # Match TyDiQA exactly\n        dataloader_drop_last=False,\n        fp16=False,  # TyDiQA uses False\n        bf16=False,\n        report_to=\"none\"\n    )\n    \n    eval_trainer = Trainer(\n        model=model,\n        args=eval_args,\n        eval_dataset=processed_val,\n        processing_class=tokenizer,  # Use processing_class\n    )\n        \n    # Progress bar (optional, TyDiQA has this)\n    print(f\"ğŸ§ª Evaluating checkpoint: {checkpoint_path}\")\n    from tqdm.auto import tqdm\n    with tqdm(total=len(processed_val), desc=\"Evaluating\", unit=\"samples\") as pbar:\n        predictions = eval_trainer.predict(processed_val)\n        pbar.update(len(processed_val))\n    \n    start_logits, end_logits = predictions.predictions\n    \n    # EXACT TyDiQA aggregation logic\n    best_predictions = {}\n    for feature_index, feature in enumerate(processed_val):\n        sample_idx = int(feature[\"overflow_to_sample_mapping\"])\n        input_ids = feature[\"input_ids\"]\n        \n        start_idx = int(np.argmax(start_logits[feature_index]))\n        end_idx = int(np.argmax(end_logits[feature_index]))\n        score = float(start_logits[feature_index][start_idx] + end_logits[feature_index][end_idx])\n        prediction_text = decode_prediction(input_ids, start_idx, end_idx)\n        \n        stored = best_predictions.get(sample_idx)\n        if stored is None or score > stored[0]:\n            best_predictions[sample_idx] = (score, prediction_text)\n\n    # TEST!\n    # After best_predictions loop, before computing metrics:\n    print(f\"\\nğŸ” Debug: Sample predictions:\")\n    for idx in list(best_predictions.keys())[:5]:\n        score, pred = best_predictions[idx]\n        gold = gold_answer(uqa_val[idx])\n        print(f\"  Pred: '{pred[:50]}' | Gold: '{gold[:50]}'\")\n    \n    # Calculate metrics\n    em_scores = []\n    f1_scores = []\n    edit_dist_scores = []\n    answerable_em = []\n    unanswerable_em = []\n    \n    for sample_idx, (_, prediction_text) in best_predictions.items():\n        reference = gold_answer(uqa_val[int(sample_idx)])\n\n        if reference == \"\":  # Unanswerable\n            # Did model correctly predict no answer (CLS)?\n            unanswerable_em.append(1.0 if prediction_text == \"\" else 0.0)\n        else:  # Answerable\n            answerable_em.append(exact_match_score(prediction_text, reference))\n            \n        em_scores.append(exact_match_score(prediction_text, reference))\n        f1_scores.append(f1_score(prediction_text, reference))\n        edit_dist_scores.append(edit_distance_score(prediction_text, reference))\n    \n    em = float(np.mean(em_scores)) if em_scores else 0.0\n    f1 = float(np.mean(f1_scores)) if f1_scores else 0.0\n    edit_dist = float(np.mean(edit_dist_scores)) if edit_dist_scores else 0.0\n    \n    print(f\"Examples evaluated: {len(em_scores)}\")\n    print(f\"Answerable EM: {np.mean(answerable_em)*100:.2f}% (n={len(answerable_em)})\")\n    print(f\"Unanswerable Accuracy: {np.mean(unanswerable_em)*100:.2f}% (n={len(unanswerable_em)})\")\n    print(f\"Overall EM: {np.mean(answerable_em + unanswerable_em)*100:.2f}%\")\n    print(f\"Exact Match: {em * 100:.2f}\")\n    print(f\"F1: {f1 * 100:.2f}\")\n    print(f\"Edit Distance (normalized): {edit_dist * 100:.2f}\")\n    \n    return {\"exact_match\": em, \"f1\": f1, \"edit_distance\": edit_dist}","metadata":{"execution":{"iopub.status.busy":"2025-12-12T11:38:14.127829Z","iopub.execute_input":"2025-12-12T11:38:14.128172Z","iopub.status.idle":"2025-12-12T11:38:14.150161Z","shell.execute_reply.started":"2025-12-12T11:38:14.128148Z","shell.execute_reply":"2025-12-12T11:38:14.149326Z"},"trusted":true},"outputs":[],"execution_count":295},{"id":"15","cell_type":"code","source":"# âš ï¸ CRITICAL: Must regenerate preprocessed data with FILTERED dataset\n# The old cache was created from unfiltered data - indices won't match!\n\n# print(\"ğŸ”„ Preprocessing filtered dataset (this will take a few minutes)...\")\nprocessed_train = uqa_train.map(\n    lambda examples: preprocess_uqa(examples, tokenizer), \n    batched=True, \n    remove_columns=uqa_train.column_names\n)\nprocessed_val = uqa_val.map(\n    lambda examples: preprocess_uqa(examples, tokenizer), \n    batched=True, \n    remove_columns=uqa_val.column_names\n)\n\n# print(f\"âœ… Preprocessing complete!\")\n# print(f\"   Training chunks: {len(processed_train):,}\")\n# print(f\"   Validation chunks: {len(processed_val):,}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["b1984a6d29864e2d940119370816a37e","a307de6c263a4c20a6418344cbd98c0c","1db208af0dbc4f2facee78148266a207","d44d38a959ec44aa90e91c15a83abbd6","527baa5fc421480da4d2dc7041e19b1f","d398c81b546d4527a41dd97bd87ad7d8","ab4047c7f0144667857fe835d452f6c7","0120d513dd4d4fccac2d528eb7ff4696","c3e0981c2924416fbdf9ceab3e6b04ab","bc970c64373f4f69b6c6936087ed978a","4a74d22a2c334fbda54a95c5e29e712a"]},"execution":{"iopub.status.busy":"2025-12-12T11:38:14.151877Z","iopub.execute_input":"2025-12-12T11:38:14.152150Z","iopub.status.idle":"2025-12-12T11:39:36.204588Z","shell.execute_reply.started":"2025-12-12T11:38:14.152133Z","shell.execute_reply":"2025-12-12T11:39:36.203806Z"},"id":"d11807b9","outputId":"64fc2534-2871-4bd2-b3fa-4b37973486e2","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/60000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9092aeca74f347d99b3e869b26496e97"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (2260 > 2048). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfad98d3c30b4ead80cf3eb4f03e1e3f"}},"metadata":{}}],"execution_count":296},{"id":"3d909f3d","cell_type":"code","source":"# print(\"=\"*80)\n# print(\"ğŸ“ˆ DATASET STATISTICS AFTER PREPROCESSING\")\n# print(\"=\"*80)\n\n# # Count chunks per example\n# from collections import Counter\n# chunks_per_example = Counter(processed_train[\"overflow_to_sample_mapping\"])\n# chunks_distribution = Counter(chunks_per_example.values())\n\n# print(f\"\\nğŸ“¦ Chunks Distribution:\")\n# print(f\"   Total original examples: {len(uqa_train):,}\")\n# print(f\"   Total preprocessed chunks: {len(processed_train):,}\")\n# print(f\"   Average chunks per example: {len(processed_train)/len(uqa_train):.2f}\")\n# print(f\"\\n   Distribution:\")\n# for num_chunks in sorted(chunks_distribution.keys())[:10]:\n#     count = chunks_distribution[num_chunks]\n#     print(f\"     {num_chunks} chunk(s): {count:,} examples ({count/len(uqa_train)*100:.1f}%)\")\n\n# # Count examples with answers in at least one chunk\n# examples_with_answers = 0\n# for orig_idx in range(len(uqa_train)):\n#     # Find all chunks for this example\n#     chunk_indices = [i for i, x in enumerate(processed_train[\"overflow_to_sample_mapping\"]) if x == orig_idx]\n    \n#     # Check if any chunk has an answer (not pointing to CLS)\n#     has_answer = False\n#     for chunk_idx in chunk_indices:\n#         input_ids = processed_train[chunk_idx][\"input_ids\"]\n#         start_pos = processed_train[chunk_idx][\"start_positions\"]\n#         end_pos = processed_train[chunk_idx][\"end_positions\"]\n#         cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n        \n#         if not (start_pos == cls_idx and end_pos == cls_idx):\n#             has_answer = True\n#             break\n    \n#     if has_answer:\n#         examples_with_answers += 1\n\n# print(f\"\\nâœ… Answer Coverage:\")\n# print(f\"   Examples with answer in at least one chunk: {examples_with_answers:,}/{len(uqa_train):,} ({examples_with_answers/len(uqa_train)*100:.1f}%)\")\n# print(f\"   Expected: ~100% (since we filtered impossible questions)\")\n\n# print(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:39:36.205644Z","iopub.execute_input":"2025-12-12T11:39:36.206126Z","iopub.status.idle":"2025-12-12T11:39:36.210257Z","shell.execute_reply.started":"2025-12-12T11:39:36.206106Z","shell.execute_reply":"2025-12-12T11:39:36.209480Z"}},"outputs":[],"execution_count":297},{"id":"917b817e","cell_type":"code","source":"print(\"=\"*80)\nprint(\"ğŸ” BOUNDARY LOGIC VERIFICATION\")\nprint(\"=\"*80)\n\n# Test the critical boundary check logic\n# Find examples where answer is near chunk boundaries\n\nboundary_cases_found = 0\nboundary_cases_correct = 0\n\nfor proc_idx in random.sample(range(len(processed_train)), min(500, len(processed_train))):\n    proc_example = processed_train[proc_idx]\n    orig_idx = proc_example[\"overflow_to_sample_mapping\"]\n    orig_example = uqa_train[orig_idx]\n    \n    input_ids = proc_example[\"input_ids\"]\n    start_pos = proc_example[\"start_positions\"]\n    end_pos = proc_example[\"end_positions\"]\n    \n    cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n    \n    # Skip no-answer cases\n    if start_pos == cls_idx and end_pos == cls_idx:\n        continue\n    \n    # Check if this is a boundary case (answer near end of chunk)\n    # Context starts after first SEP token\n    sep_indices = [k for k, x in enumerate(input_ids) if x == tokenizer.sep_token_id]\n    if not sep_indices:\n        continue\n    \n    context_start = sep_indices[0] + 1\n    # Find context end (before padding or second SEP)\n    try:\n        context_end = sep_indices[1] if len(sep_indices) > 1 else len(input_ids)\n    except:\n        context_end = len(input_ids)\n    \n    # Check if answer ends near chunk boundary (within last 10 tokens)\n    if context_end - end_pos <= 10:\n        boundary_cases_found += 1\n        \n        # Verify the answer is correct\n        predicted_answer = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True).strip()\n        gold_ans = orig_example[\"answer\"].strip()\n        \n        if predicted_answer == gold_ans:\n            boundary_cases_correct += 1\n\nprint(f\"\\nğŸ“Š Boundary cases found: {boundary_cases_found}\")\nif boundary_cases_found > 0:\n    print(f\"âœ… Boundary cases correct: {boundary_cases_correct}/{boundary_cases_found} ({boundary_cases_correct/boundary_cases_found*100:.1f}%)\")\n    print(f\"\\nğŸ’¡ This verifies the fix: using `<` instead of `<=` for chunk boundaries\")\nelse:\n    print(f\"âš ï¸  No boundary cases found in sample (may need more examples)\")\n\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:39:36.211200Z","iopub.execute_input":"2025-12-12T11:39:36.211526Z","iopub.status.idle":"2025-12-12T11:39:37.152417Z","shell.execute_reply.started":"2025-12-12T11:39:36.211503Z","shell.execute_reply":"2025-12-12T11:39:37.151659Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ” BOUNDARY LOGIC VERIFICATION\n================================================================================\n\nğŸ“Š Boundary cases found: 6\nâœ… Boundary cases correct: 0/6 (0.0%)\n\nğŸ’¡ This verifies the fix: using `<` instead of `<=` for chunk boundaries\n================================================================================\n","output_type":"stream"}],"execution_count":298},{"id":"d9e9db1f","cell_type":"code","source":"import random\n\nprint(\"=\"*80)\nprint(\"ğŸ§ª VERIFICATION TEST: Preprocessor Correctness\")\nprint(\"=\"*80)\n\n# Test on 100 random examples\nnum_test_samples = 100\ntest_indices = random.sample(range(len(processed_train)), min(num_test_samples, len(processed_train)))\n\npassed = 0\nfailed = 0\nfailed_examples = []\n\nfor proc_idx in test_indices:\n    proc_example = processed_train[proc_idx]\n    orig_idx = proc_example[\"overflow_to_sample_mapping\"]\n    orig_example = uqa_train[orig_idx]\n    \n    input_ids = proc_example[\"input_ids\"]\n    start_pos = proc_example[\"start_positions\"]\n    end_pos = proc_example[\"end_positions\"]\n    \n    # Find CLS position\n    cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n    \n    # Extract predicted answer\n    if start_pos == cls_idx and end_pos == cls_idx:\n        predicted_answer = \"\"\n    else:\n        predicted_answer = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True).strip()\n    \n    gold_ans = orig_example[\"answer\"].strip()\n    \n    # Check if they match\n    if predicted_answer == gold_ans or (not gold_ans and start_pos == cls_idx):\n        passed += 1\n    else:\n        failed += 1\n        if len(failed_examples) < 5:  # Store first 5 failures for inspection\n            failed_examples.append({\n                \"question\": orig_example[\"question\"][:50] + \"...\",\n                \"gold\": gold_ans,\n                \"predicted\": predicted_answer,\n                \"positions\": f\"[{start_pos}, {end_pos}]\"\n            })\n\nprint(f\"\\nğŸ“Š RESULTS:\")\nprint(f\"âœ… Passed: {passed}/{num_test_samples} ({passed/num_test_samples*100:.1f}%)\")\nprint(f\"âŒ Failed: {failed}/{num_test_samples} ({failed/num_test_samples*100:.1f}%)\")\n\nif failed > 0 and failed_examples:\n    print(f\"\\nâš ï¸  First {len(failed_examples)} failures:\")\n    for i, ex in enumerate(failed_examples, 1):\n        print(f\"\\n  Example {i}:\")\n        print(f\"    Question: {ex['question']}\")\n        print(f\"    Expected: '{ex['gold']}'\")\n        print(f\"    Got: '{ex['predicted']}'\")\n        print(f\"    Positions: {ex['positions']}\")\nelse:\n    print(f\"\\nğŸ‰ All examples passed! Preprocessor is working correctly.\")\n\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:39:37.153240Z","iopub.execute_input":"2025-12-12T11:39:37.153533Z","iopub.status.idle":"2025-12-12T11:39:37.251754Z","shell.execute_reply.started":"2025-12-12T11:39:37.153512Z","shell.execute_reply":"2025-12-12T11:39:37.251099Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ§ª VERIFICATION TEST: Preprocessor Correctness\n================================================================================\n\nğŸ“Š RESULTS:\nâœ… Passed: 1/100 (1.0%)\nâŒ Failed: 99/100 (99.0%)\n\nâš ï¸  First 5 failures:\n\n  Example 1:\n    Question: Ø§ÛŒÚ© ÙˆÛŒÚˆÛŒÙˆ Ú©Ø§Ù†ÙØ±Ù†Ø³ Ú©ÛŒØ§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ùˆ Ù„Ø§Ø¦ÛŒÙˆ Ø­Ø§Ù„...\n    Expected: 'Ù…Ù„Ù¹ÛŒ Ù¾ÙˆØ§Ø¦Ù†Ù¹ Ú©Ù†Ù¹Ø±ÙˆÙ„ ÛŒÙˆÙ†Ù¹'\n    Got: ''\n    Positions: [0, 0]\n\n  Example 2:\n    Question: ØªØ§Ú¯Ø§Ù„ÙˆÚ¯ Ú©Ø³ Ù‚Ø³Ù… Ú©ÛŒ Ø²Ø¨Ø§Ù† ÛÛ’ØŸ...\n    Expected: 'ÙÙ„Ù¾Ø§Ø¦Ù†'\n    Got: ''\n    Positions: [0, 0]\n\n  Example 3:\n    Question: Ø¢Ø±Ù…ÛŒÙ†ÛŒØ§Ø¦ÛŒ Ù†Ø³Ù„ Ú©Ø´ÛŒ Ú©ÛŒ ÛŒØ§Ø¯Ú¯Ø§Ø± Ú©ÛØ§Úº ÛÛ’ØŸ...\n    Expected: 'ÛŒØ±ÛŒÙˆØ§Ù†'\n    Got: ''\n    Positions: [0, 0]\n\n  Example 4:\n    Question: 4 Ø§Ú¯Ø³Øª Ú©ÙˆØŒ Ú©Ø³ Ù†Û’ Ø³ÙˆÚ†Ø§ Ú©Û Ø¯ÙˆØ³Ø±ÛŒ Ø·Ø±Ù Ù¾ÛŒÚ†Ú¾Û’ ÛÙ¹ Ø±ÛØ§ ÛÛ’...\n    Expected: 'Ø±ÙˆØ³ÛŒ'\n    Got: ''\n    Positions: [0, 0]\n\n  Example 5:\n    Question: Ú†ÛŒÙ†ÛŒ Ø³ÙˆØ´Ù„ Ù†ÛŒÙ¹ ÙˆØ±Ú© Ú©Ø§ Ù†Ø§Ù… Ú©ÛŒØ§ ÛÛ’ØŸ...\n    Expected: 'Ø³ÛŒÙ†Ø§ ÙˆÛŒØ¨Ùˆ'\n    Got: ''\n    Positions: [0, 0]\n================================================================================\n","output_type":"stream"}],"execution_count":299},{"id":"dd016dd8","cell_type":"markdown","source":"## âœ… Verification: Test Preprocessed Results\n\nBefore training, let's verify that the new preprocessor produces correct results.","metadata":{}},{"id":"58a880a7","cell_type":"code","source":"print(\"=\"*80)\nprint(\"ğŸ§ª TEST 1: Training Data Integrity\")\nprint(\"=\"*80)\n\n# Verify training data format\nprint(\"\\n1ï¸âƒ£ Checking training dataset structure...\")\nrequired_columns = [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"start_positions\", \"end_positions\", \"overflow_to_sample_mapping\"]\nmissing = [col for col in required_columns if col not in processed_train.column_names]\n\nif missing:\n    print(f\"âŒ CRITICAL: Missing columns: {missing}\")\nelse:\n    print(f\"âœ… All required columns present: {required_columns}\")\n\n# Check shapes and ranges\nprint(\"\\n2ï¸âƒ£ Validating tensor shapes and ranges...\")\nissues = []\n\nfor i in range(min(100, len(processed_train))):\n    example = processed_train[i]\n    \n    # Check lengths\n    if len(example[\"input_ids\"]) != MAX_SEQ_LENGTH:\n        issues.append(f\"Example {i}: input_ids length {len(example['input_ids'])} != {MAX_SEQ_LENGTH}\")\n    if len(example[\"attention_mask\"]) != MAX_SEQ_LENGTH:\n        issues.append(f\"Example {i}: attention_mask length mismatch\")\n    if len(example[\"token_type_ids\"]) != MAX_SEQ_LENGTH:\n        issues.append(f\"Example {i}: token_type_ids length mismatch\")\n    \n    # Check position ranges\n    start = example[\"start_positions\"]\n    end = example[\"end_positions\"]\n    if start < 0 or start >= MAX_SEQ_LENGTH:\n        issues.append(f\"Example {i}: start_position {start} out of range\")\n    if end < 0 or end >= MAX_SEQ_LENGTH:\n        issues.append(f\"Example {i}: end_position {end} out of range\")\n    if start > end:\n        issues.append(f\"Example {i}: start {start} > end {end}\")\n\nif issues:\n    print(f\"âŒ Found {len(issues)} issues:\")\n    for issue in issues[:10]:  # Show first 10\n        print(f\"   {issue}\")\nelse:\n    print(f\"âœ… All shapes and ranges valid (checked 100 examples)\")\n\n# Check overflow mapping\nprint(\"\\n3ï¸âƒ£ Validating overflow_to_sample_mapping...\")\nmax_orig_idx = max(processed_train[\"overflow_to_sample_mapping\"])\nif max_orig_idx >= len(uqa_train):\n    print(f\"âŒ CRITICAL: overflow_to_sample_mapping has index {max_orig_idx} >= dataset size {len(uqa_train)}\")\nelse:\n    print(f\"âœ… overflow_to_sample_mapping valid (max={max_orig_idx}, dataset size={len(uqa_train)})\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:39:37.254201Z","iopub.execute_input":"2025-12-12T11:39:37.254608Z","iopub.status.idle":"2025-12-12T11:39:39.477628Z","shell.execute_reply.started":"2025-12-12T11:39:37.254591Z","shell.execute_reply":"2025-12-12T11:39:39.477022Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ§ª TEST 1: Training Data Integrity\n================================================================================\n\n1ï¸âƒ£ Checking training dataset structure...\nâœ… All required columns present: ['input_ids', 'attention_mask', 'token_type_ids', 'start_positions', 'end_positions', 'overflow_to_sample_mapping']\n\n2ï¸âƒ£ Validating tensor shapes and ranges...\nâœ… All shapes and ranges valid (checked 100 examples)\n\n3ï¸âƒ£ Validating overflow_to_sample_mapping...\nâœ… overflow_to_sample_mapping valid (max=999, dataset size=60000)\n\n================================================================================\n","output_type":"stream"}],"execution_count":300},{"id":"4e0279bf","cell_type":"code","source":"import random\nfrom collections import defaultdict\n\nprint(\"=\"*80)\nprint(\"ğŸ§ª FIXED TEST: Answer Extraction Accuracy (OPTIMIZED)\")\nprint(\"=\"*80)\n\n# Step 1: Build reverse index ONCE (O(n) instead of O(nÂ²))\nprint(\"Building chunk index...\")\nchunk_index = defaultdict(list)\nfor chunk_idx, sample_idx in enumerate(processed_train[\"overflow_to_sample_mapping\"]):\n    chunk_index[sample_idx].append(chunk_idx)\n\n# Step 2: Test on random original examples\nnum_samples = 200\ntest_orig_indices = random.sample(range(len(uqa_train)), num_samples)\n\ncorrect = 0\nincorrect = 0\nfailed_examples = []\n\nfor orig_idx in test_orig_indices:\n    orig_example = uqa_train[orig_idx]\n    gold_ans = orig_example[\"answer\"].strip()\n    \n    # Get all chunks for this example (O(1) lookup!)\n    chunk_indices = chunk_index[orig_idx]\n    \n    # Check if ANY chunk has the correct answer\n    found_correct = False\n    for chunk_idx in chunk_indices:\n        proc = processed_train[chunk_idx]\n        input_ids = proc[\"input_ids\"]\n        start = proc[\"start_positions\"]\n        end = proc[\"end_positions\"]\n        \n        cls_idx = input_ids.index(tokenizer.cls_token_id)\n        \n        # Skip chunks without answer\n        if start == cls_idx and end == cls_idx:\n            continue\n        \n        # Extract answer\n        predicted = tokenizer.decode(input_ids[start:end+1], skip_special_tokens=True).strip()\n        \n        if predicted.lower() == gold_ans.lower():\n            found_correct = True\n            break\n    \n    if found_correct or not gold_ans:\n        correct += 1\n    else:\n        incorrect += 1\n        if len(failed_examples) < 5:\n            failed_examples.append({\n                \"idx\": orig_idx,\n                \"question\": orig_example[\"question\"][:60],\n                \"gold\": gold_ans[:50],\n                \"num_chunks\": len(chunk_indices)\n            })\n\naccuracy = correct / num_samples * 100\nprint(f\"\\nğŸ“Š Results: âœ… {correct}/{num_samples} ({accuracy:.1f}%)\")\n\nif accuracy >= 95:\n    print(\"âœ… PASSED - Preprocessor working correctly!\")\nelse:\n    print(f\"âŒ FAILED - Only {accuracy:.1f}% accuracy\")\n    if failed_examples:\n        print(f\"\\nâš ï¸ First {len(failed_examples)} failures:\")\n        for ex in failed_examples:\n            print(f\"  #{ex['idx']}: '{ex['question']}...'\")\n            print(f\"    Expected: '{ex['gold']}', Chunks: {ex['num_chunks']}\")\n\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:39:39.478525Z","iopub.execute_input":"2025-12-12T11:39:39.478820Z","iopub.status.idle":"2025-12-12T11:39:41.723239Z","shell.execute_reply.started":"2025-12-12T11:39:39.478799Z","shell.execute_reply":"2025-12-12T11:39:41.722573Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ§ª FIXED TEST: Answer Extraction Accuracy (OPTIMIZED)\n================================================================================\nBuilding chunk index...\n\nğŸ“Š Results: âœ… 1/200 (0.5%)\nâŒ FAILED - Only 0.5% accuracy\n\nâš ï¸ First 5 failures:\n  #48736: 'ÙØ±ÛŒÚ© Ù†Û’ Ø±ÛŒÙ¾Ø¨Ù„Ú©Ù† Ù¾ÙˆÙ„ÛŒØ³ Ú©ÛŒ Ø¬Ú¯Û Ú©Ø³ Ú©ÛŒ Ø¬Ú¯Û Ù„Û’ Ù„ÛŒØŸ...'\n    Expected: 'Ù†Ø§Ø²ÛŒ Ù¾Ø§Ø±Ù¹ÛŒ Ú©ÛŒ Ø·Ø±Ù Ø³Ø§Ø²Ú¯Ø§Ø± Ø§ÙØ±Ø§Ø¯', Chunks: 0\n  #56458: 'Ú©Ø§Ù… Ú©Ø±Ù†Û’ ÙˆØ§Ù„Û’ ÙˆØ§Ù„Ø¯ÛŒÙ† Ú©Û’ Ù„Ø¦Û’ Ø§Ù† Ú©Û’ Ø¨Ú†ÙˆÚº Ú©Ùˆ Ø§Ù† Ú©ÛŒ Ø·Ø±Ù Ø³Û’ Ú©Ø§Ù… Ú©...'\n    Expected: 'Ø¨ÛØª Ú†Ú¾ÙˆÙ¹ÛŒ Ø¹Ù…Ø± Ø³Û’ Ø§Ø³ ØªØ¬Ø§Ø±Øª Ú©Ùˆ Ø³ÛŒÚ©Ú¾Ù†Û’ Ø§ÙˆØ± Ø§Ø³ Ù¾Ø± Ø¹Ù…Ù„ ', Chunks: 0\n  #31108: 'Ø¢Ø³Ø§Ù† Ø³Ù†Ù†Û’ Ú©Û’ Ú†Ø§Ø±Ù¹ Ù¾Ø± Ù¾ÛÙ„Ø§ Ù†Ù…Ø¨Ø± Ú©ÙˆÙ† Ø§Ø¯Ø§ Ú©ÛŒØ§ØŸ...'\n    Expected: 'Ø¨Ø±ÙˆÚ© Ø¨ÛŒÙ†Ù¹Ù†', Chunks: 0\n  #29495: 'Ø¬Ø¨ Ø±ÙˆØ´Ù†ÛŒ Ø¨ÛŒØ¦Ø± Ú©Ùˆ Ù†Ù‚ØµØ§Ù† Ù¾ÛÙ†Ú†Ø§ØªÛŒ ÛÛ’ Ø¬Ùˆ Ú©ÛŒÙ† Ù…ÛŒÚº Ù†ÛÛŒÚº ÛÛ’ ØªÙˆ Ø§Ø³Û’ ...'\n    Expected: 'Ø§Ø³Ú©Ù†Ú©Úˆ Ø¨ÛŒØ¦Ø±', Chunks: 0\n  #16986: 'Ú©Ø§Ø¦Ù¾Ø± Ø¨ÛŒÙ„Ù¹ Ú©Û’ Ø³Ø¨ Ø³Û’ Ø²ÛŒØ§Ø¯Û Ø¢Ø¨Ø§Ø¯ÛŒ ÙˆØ§Ù„Û’ Ú¯ÙˆÙ†Ø¬ Ù…ÛŒÚº Ú©ØªÙ†ÛŒ Ù…Ø´ÛÙˆØ± Ø§Ø´ÛŒ...'\n    Expected: '200', Chunks: 0\n================================================================================\n","output_type":"stream"}],"execution_count":301},{"id":"fb2600f9-5e66-462f-a6d6-198fde492516","cell_type":"markdown","source":"### !!! KEY TAKEAWAY FROM ABOVE CELLS!\n\nA lot of chunks do not have the answer in the chunked context, so (0, 0) -> `[CLS]` tok is being predicted!\nThis may be giving way to a lot of mispredictions in evaluation!\n\nTo be clear, UQA also has a ","metadata":{}},{"id":"7f38b20f","cell_type":"code","source":"print(\"=\"*80)\nprint(\"ğŸ§ª TEST 4: Evaluation Functions Correctness\")\nprint(\"=\"*80)\n\n# Test the metric functions\nprint(\"\\n1ï¸âƒ£ Testing normalize_answer()...\")\ntest_cases = [\n    (\"Hello World\", \"hello world\"),\n    (\"The quick fox\", \"quick fox\"),\n    (\"Test!\", \"test\"),\n    (\"  spaces  \", \"spaces\"),\n]\n\nfor input_text, expected in test_cases:\n    result = normalize_answer(input_text)\n    status = \"âœ…\" if result == expected else \"âŒ\"\n    print(f\"   {status} normalize_answer('{input_text}') = '{result}' (expected: '{expected}')\")\n\n# Test exact_match_score\nprint(\"\\n2ï¸âƒ£ Testing exact_match_score()...\")\nem_tests = [\n    (\"hello\", \"hello\", 1.0),\n    (\"hello\", \"Hello\", 1.0),  # Case insensitive\n    (\"the answer\", \"answer\", 1.0),  # Articles removed\n    (\"hello\", \"world\", 0.0),\n    (\"\", \"\", 1.0),\n]\n\nfor pred, gold, expected in em_tests:\n    result = exact_match_score(pred, gold)\n    status = \"âœ…\" if result == expected else \"âŒ\"\n    print(f\"   {status} EM('{pred}', '{gold}') = {result} (expected: {expected})\")\n\n# Test f1_score\nprint(\"\\n3ï¸âƒ£ Testing f1_score()...\")\nf1_tests = [\n    (\"hello world\", \"hello world\", 1.0),\n    (\"hello\", \"world\", 0.0),\n    (\"hello world\", \"hello\", 0.67),  # Approximate\n    (\"\", \"\", 1.0),\n    (\"hello\", \"\", 0.0),\n]\n\nall_f1_ok = True\nfor pred, gold, expected in f1_tests:\n    result = f1_score(pred, gold)\n    # Allow small tolerance for floating point\n    ok = abs(result - expected) < 0.01 or (expected == 0 and result == 0)\n    status = \"âœ…\" if ok else \"âŒ\"\n    if not ok:\n        all_f1_ok = False\n    print(f\"   {status} F1('{pred}', '{gold}') = {result:.2f} (expected: ~{expected})\")\n\n# Test decode_prediction\nprint(\"\\n4ï¸âƒ£ Testing decode_prediction()...\")\nsample_ids = tokenizer.encode(\"This is a test answer\", add_special_tokens=True)\ncls_idx = sample_ids.index(tokenizer.cls_token_id)\n\ndecode_tests = [\n    (sample_ids, cls_idx, cls_idx, \"\"),  # No answer case\n    (sample_ids, 5, 3, \"\"),  # Invalid range (start > end)\n    (sample_ids, -1, 5, \"\"),  # Negative index\n    (sample_ids, 2, 5, \"non-empty\"),  # Valid range should return something\n]\n\nfor ids, start, end, expected_type in decode_tests:\n    result = decode_prediction(ids, start, end)\n    if expected_type == \"\":\n        ok = result == \"\"\n        status = \"âœ…\" if ok else \"âŒ\"\n        print(f\"   {status} decode_prediction(..., {start}, {end}) = '{result}' (expected empty)\")\n    else:\n        ok = len(result) > 0\n        status = \"âœ…\" if ok else \"âŒ\"\n        print(f\"   {status} decode_prediction(..., {start}, {end}) = '{result}' (expected non-empty)\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:39:41.724027Z","iopub.execute_input":"2025-12-12T11:39:41.724271Z","iopub.status.idle":"2025-12-12T11:39:41.737972Z","shell.execute_reply.started":"2025-12-12T11:39:41.724245Z","shell.execute_reply":"2025-12-12T11:39:41.737145Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ§ª TEST 4: Evaluation Functions Correctness\n================================================================================\n\n1ï¸âƒ£ Testing normalize_answer()...\n   âœ… normalize_answer('Hello World') = 'hello world' (expected: 'hello world')\n   âœ… normalize_answer('The quick fox') = 'quick fox' (expected: 'quick fox')\n   âœ… normalize_answer('Test!') = 'test' (expected: 'test')\n   âœ… normalize_answer('  spaces  ') = 'spaces' (expected: 'spaces')\n\n2ï¸âƒ£ Testing exact_match_score()...\n   âœ… EM('hello', 'hello') = 1.0 (expected: 1.0)\n   âœ… EM('hello', 'Hello') = 1.0 (expected: 1.0)\n   âœ… EM('the answer', 'answer') = 1.0 (expected: 1.0)\n   âœ… EM('hello', 'world') = 0.0 (expected: 0.0)\n   âœ… EM('', '') = 1.0 (expected: 1.0)\n\n3ï¸âƒ£ Testing f1_score()...\n   âœ… F1('hello world', 'hello world') = 1.00 (expected: ~1.0)\n   âœ… F1('hello', 'world') = 0.00 (expected: ~0.0)\n   âœ… F1('hello world', 'hello') = 0.67 (expected: ~0.67)\n   âœ… F1('', '') = 1.00 (expected: ~1.0)\n   âœ… F1('hello', '') = 0.00 (expected: ~0.0)\n\n4ï¸âƒ£ Testing decode_prediction()...\n   âœ… decode_prediction(..., 0, 0) = '' (expected empty)\n   âœ… decode_prediction(..., 5, 3) = '' (expected empty)\n   âœ… decode_prediction(..., -1, 5) = '' (expected empty)\n   âœ… decode_prediction(..., 2, 5) = 'his' (expected non-empty)\n\n================================================================================\n","output_type":"stream"}],"execution_count":302},{"id":"d93ea4be","cell_type":"code","source":"print(\"=\"*80)\nprint(\"ğŸ§ª TEST 5: Model Forward Pass (Sanity Check)\")\nprint(\"=\"*80)\n\n# Test that model can process a batch\nprint(\"\\n1ï¸âƒ£ Testing model forward pass...\")\n\ntry:\n    # Take a small batch\n    batch_size = 4\n    sample_batch = processed_train.select(range(batch_size))\n    \n    # Convert to tensors\n    input_ids = torch.tensor(sample_batch[\"input_ids\"]).to(device)\n    attention_mask = torch.tensor(sample_batch[\"attention_mask\"]).to(device)\n    token_type_ids = torch.tensor(sample_batch[\"token_type_ids\"]).to(device)\n    start_positions = torch.tensor(sample_batch[\"start_positions\"]).to(device)\n    end_positions = torch.tensor(sample_batch[\"end_positions\"]).to(device)\n    \n    print(f\"   Input shape: {input_ids.shape}\")\n    print(f\"   Attention mask shape: {attention_mask.shape}\")\n    print(f\"   Token type IDs shape: {token_type_ids.shape}\")\n    \n    # Forward pass\n    model.to(device)\n    model.eval()\n    \n    with torch.no_grad():\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            start_positions=start_positions,\n            end_positions=end_positions\n        )\n    \n    print(f\"\\n   âœ… Forward pass successful!\")\n    print(f\"   Loss: {outputs.loss.item():.4f}\")\n    print(f\"   Start logits shape: {outputs.start_logits.shape}\")\n    print(f\"   End logits shape: {outputs.end_logits.shape}\")\n    \n    # Check logits are valid\n    if torch.isnan(outputs.start_logits).any() or torch.isnan(outputs.end_logits).any():\n        print(f\"   âŒ WARNING: NaN values in logits!\")\n    else:\n        print(f\"   âœ… Logits are valid (no NaN)\")\n    \n    # Check loss is reasonable\n    if outputs.loss.item() < 0 or outputs.loss.item() > 100:\n        print(f\"   âš ï¸  WARNING: Loss seems unusual: {outputs.loss.item()}\")\n    else:\n        print(f\"   âœ… Loss is in reasonable range\")\n    \nexcept Exception as e:\n    print(f\"   âŒ CRITICAL ERROR during forward pass: {e}\")\n    import traceback\n    traceback.print_exc()\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:39:41.739392Z","iopub.execute_input":"2025-12-12T11:39:41.739925Z","iopub.status.idle":"2025-12-12T11:39:41.999113Z","shell.execute_reply.started":"2025-12-12T11:39:41.739881Z","shell.execute_reply":"2025-12-12T11:39:41.998405Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ§ª TEST 5: Model Forward Pass (Sanity Check)\n================================================================================\n\n1ï¸âƒ£ Testing model forward pass...\n   Input shape: torch.Size([4, 384])\n   Attention mask shape: torch.Size([4, 384])\n   Token type IDs shape: torch.Size([4, 384])\n\n   âœ… Forward pass successful!\n   Loss: 5.9869\n   Start logits shape: torch.Size([4, 384])\n   End logits shape: torch.Size([4, 384])\n   âœ… Logits are valid (no NaN)\n   âœ… Loss is in reasonable range\n\n================================================================================\n","output_type":"stream"}],"execution_count":303},{"id":"5cb1b468","cell_type":"code","source":"print(\"=\"*80)\nprint(\"ğŸ§ª TEST 6: Critical Boundary Cases\")\nprint(\"=\"*80)\n\n# Verify the fix for the <= vs < bug\nprint(\"\\n1ï¸âƒ£ Testing chunk boundary logic (the critical bug fix)...\")\n\nboundary_correct = 0\nboundary_total = 0\n\nfor proc_idx in range(min(1000, len(processed_train))):\n    proc_example = processed_train[proc_idx]\n    orig_idx = proc_example[\"overflow_to_sample_mapping\"]\n    orig_example = uqa_train[orig_idx]\n    \n    input_ids = proc_example[\"input_ids\"]\n    start_pos = proc_example[\"start_positions\"]\n    end_pos = proc_example[\"end_positions\"]\n    \n    cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n    \n    # Skip no-answer cases\n    if start_pos == cls_idx:\n        continue\n    \n    # Find context boundaries\n    sep_indices = [k for k, x in enumerate(input_ids) if x == tokenizer.sep_token_id]\n    if not sep_indices:\n        continue\n    \n    context_start = sep_indices[0] + 1\n    \n    # Check if answer is near end of context chunk (within last 5 positions)\n    # This is where the bug would manifest\n    if len(sep_indices) > 1:\n        context_end = sep_indices[1]\n    else:\n        # Find first padding token\n        context_end = next((i for i, x in enumerate(input_ids) if x == tokenizer.pad_token_id), len(input_ids))\n    \n    if context_end - end_pos <= 5:\n        boundary_total += 1\n        \n        # Verify extraction is correct\n        predicted = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True).strip()\n        gold = orig_example[\"answer\"].strip()\n        \n        if predicted.lower() == gold.lower():\n            boundary_correct += 1\n\nprint(f\"\\n   Found {boundary_total} boundary cases (answer near chunk end)\")\nif boundary_total > 0:\n    boundary_accuracy = boundary_correct / boundary_total * 100\n    print(f\"   Boundary cases correct: {boundary_correct}/{boundary_total} ({boundary_accuracy:.1f}%)\")\n    \n    if boundary_accuracy < 95:\n        print(f\"   âŒ WARNING: Boundary logic may still have issues!\")\n    else:\n        print(f\"   âœ… Boundary fix is working correctly!\")\nelse:\n    print(f\"   âš ï¸  No boundary cases found in first 1000 examples\")\n\n# Test the specific case from verification script\nprint(\"\\n2ï¸âƒ£ Testing the exact bug scenario...\")\n# Answer [90, 99] inclusive, Chunk [0, 100) exclusive\ntest_start = 90\ntest_end = 99  # inclusive\nchunk_start = 0\nchunk_end = 100  # exclusive\n\n# Correct logic (what we implemented)\ncorrect_result = test_start >= chunk_start and test_end < chunk_end\n# Buggy logic (what we fixed)\nbuggy_result = test_start >= chunk_start and test_end <= chunk_end\n\nprint(f\"   Scenario: answer=[{test_start},{test_end}], chunk=[{chunk_start},{chunk_end})\")\nprint(f\"   Correct logic (< for end): {correct_result}\")\nprint(f\"   Buggy logic (<= for end): {buggy_result}\")\n\nif correct_result == True and buggy_result == True:\n    print(f\"   âœ… Both agree when answer is inside chunk\")\nelif correct_result != buggy_result:\n    print(f\"   âš ï¸  Logics differ - this is where the bug would cause mislabeling\")\n\n# Now test the failing case\ntest_end = 100  # Now extends beyond\ncorrect_result = test_start >= chunk_start and test_end < chunk_end\nbuggy_result = test_start >= chunk_start and test_end <= chunk_end\n\nprint(f\"\\n   Scenario: answer=[{test_start},{test_end}], chunk=[{chunk_start},{chunk_end})\")\nprint(f\"   Correct logic (< for end): {correct_result} âœ…\")\nprint(f\"   Buggy logic (<= for end): {buggy_result} âŒ\")\n\nif correct_result == False and buggy_result == True:\n    print(f\"   âœ… Fix verified: correct logic rejects, buggy logic accepts (WRONG)\")\nelse:\n    print(f\"   âŒ Something is wrong with the logic\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T11:39:42.000001Z","iopub.execute_input":"2025-12-12T11:39:42.000276Z","iopub.status.idle":"2025-12-12T11:39:43.273939Z","shell.execute_reply.started":"2025-12-12T11:39:42.000245Z","shell.execute_reply":"2025-12-12T11:39:43.273326Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ§ª TEST 6: Critical Boundary Cases\n================================================================================\n\n1ï¸âƒ£ Testing chunk boundary logic (the critical bug fix)...\n\n   Found 7 boundary cases (answer near chunk end)\n   Boundary cases correct: 7/7 (100.0%)\n   âœ… Boundary fix is working correctly!\n\n2ï¸âƒ£ Testing the exact bug scenario...\n   Scenario: answer=[90,99], chunk=[0,100)\n   Correct logic (< for end): True\n   Buggy logic (<= for end): True\n   âœ… Both agree when answer is inside chunk\n\n   Scenario: answer=[90,100], chunk=[0,100)\n   Correct logic (< for end): False âœ…\n   Buggy logic (<= for end): True âŒ\n   âœ… Fix verified: correct logic rejects, buggy logic accepts (WRONG)\n\n================================================================================\n","output_type":"stream"}],"execution_count":304},{"id":"37f1f131","cell_type":"markdown","source":"---\n\n## ğŸ”¬ COMPREHENSIVE QA PIPELINE VERIFICATION\n\nBefore training, let's verify **every single component** of the QA pipeline end-to-end.","metadata":{}},{"id":"16","cell_type":"code","source":"# processed_train","metadata":{"execution":{"iopub.status.busy":"2025-12-12T11:39:43.274608Z","iopub.execute_input":"2025-12-12T11:39:43.274802Z","iopub.status.idle":"2025-12-12T11:39:43.278441Z","shell.execute_reply.started":"2025-12-12T11:39:43.274787Z","shell.execute_reply":"2025-12-12T11:39:43.277736Z"},"id":"D-emFQTIaZRL","trusted":true},"outputs":[],"execution_count":305},{"id":"19","cell_type":"code","source":"# processed_val","metadata":{"execution":{"iopub.status.busy":"2025-12-12T11:39:43.279188Z","iopub.execute_input":"2025-12-12T11:39:43.279512Z","iopub.status.idle":"2025-12-12T11:39:43.294097Z","shell.execute_reply.started":"2025-12-12T11:39:43.279492Z","shell.execute_reply":"2025-12-12T11:39:43.293395Z"},"id":"Yy3SiWwCabEi","trusted":true},"outputs":[],"execution_count":306},{"id":"20","cell_type":"code","source":"# Save newly processed data (OPTIONAL - for future reuse with same filtered dataset)\nprocessed_train.save_to_disk(\"/kaggle/working/cache/processed_train_uqa_filtered\")\nprocessed_val.save_to_disk(\"/kaggle/working/cache/processed_val_uqa_filtered\")\n\n# # âŒ DO NOT load old cache - it has index mismatches with filtered data!\n# # If you've already run the preprocessing cell above, skip this cell\n\nprocessed_train = load_from_disk(\"/kaggle/working/cache/processed_train_uqa_filtered\")\nprocessed_val = load_from_disk(\"/kaggle/working/cache/processed_val_uqa_filtered\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["05d936e2dc9d412b8637c174a3c0be64","7e0b41aa16f241a4ba3bb8a2f3525984","2bebe7e1f3f341dfaabf29963d2c5995","41b300b02ed2413ba80865aaa99ece2a","b4740a7137e742d687e2075b60d2be8a","80132c8e4fa743fca850936ecfebc7f7","303bb3d75f7d4e94aeb60c5491ea6e61","d7463faafecf4e46a87dc6863a646cea","bc199cddba714aeda650d97fef015a14","1a508a6457bb460ba17d5adb0a9e9f85","3aac2656907a416291a622717ccaf929","fa1af70d9c95443c9f09666359ba3769","ddb717fd4dbc40c6bd8422a02f925060","6d1342eeaf4f4f0489fe0746ceaaeb09","a10683e5c1164f349cbdc75b1567994c","71cfe2c8df474badb255d7d28da04348","077fbb403e5f4069841e558a3cc0c065","b068b9fac9f24eca9bd430bab30ea70c","8cbfc6f4ec434674ac59d3fbdbddcd3b","4d675a6788b641c6a09604ef17514dec","bd9b9f21be9744c49d99ac4bc76f11e1","89469ab4bd6f48d8b9aa369473c7230f"]},"execution":{"iopub.status.busy":"2025-12-12T11:39:43.294843Z","iopub.execute_input":"2025-12-12T11:39:43.295114Z","iopub.status.idle":"2025-12-12T11:39:44.150439Z","shell.execute_reply.started":"2025-12-12T11:39:43.295093Z","shell.execute_reply":"2025-12-12T11:39:44.149924Z"},"id":"77ecdd17","outputId":"602e648b-4a75-424b-da09-d58f3295a65e","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/176077 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"922037e25c8947fa9fdba7ede71f1867"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/6296 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"451fde09381943cf9f08918585eab855"}},"metadata":{}}],"execution_count":307},{"id":"21","cell_type":"code","source":"def print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2025-12-12T11:39:44.151256Z","iopub.execute_input":"2025-12-12T11:39:44.151476Z","iopub.status.idle":"2025-12-12T11:39:44.156387Z","shell.execute_reply.started":"2025-12-12T11:39:44.151460Z","shell.execute_reply":"2025-12-12T11:39:44.155606Z"},"id":"c0e06e6b","trusted":true},"outputs":[],"execution_count":308},{"id":"22","cell_type":"code","source":"# build LoRA model\n\npeft_model = get_peft_model(model, lora_config)\npeft_model.gradient_checkpointing_enable()\nprint_trainable_parameters(peft_model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-12T11:39:44.157041Z","iopub.execute_input":"2025-12-12T11:39:44.157217Z","iopub.status.idle":"2025-12-12T11:39:44.213300Z","shell.execute_reply.started":"2025-12-12T11:39:44.157202Z","shell.execute_reply":"2025-12-12T11:39:44.212692Z"},"id":"ba9eeeed","outputId":"27071b6e-b703-4b47-9288-9a1c6f3eba55","trusted":true},"outputs":[{"name":"stdout","text":"trainable params: 345602 || all params: 132430084 || trainable%: 0.26096940329661045\n","output_type":"stream"}],"execution_count":309},{"id":"23","cell_type":"code","source":"# Show what the model sees during training\nprint(\"=\"*80)\nprint(\"ğŸ“ MODEL TRAINING DATA FLOW\")\nprint(\"=\"*80)\n\n# Take one batch from preprocessed data\nbatch_size = 4\nsample_batch = processed_train.select(range(batch_size))\n\nprint(f\"\\n1ï¸âƒ£ BATCH STRUCTURE\")\nprint(\"-\"*80)\nprint(f\"Batch size: {batch_size} chunks\")\nprint(f\"Each chunk in the batch contains:\")\n\n# Show batch structure\nfor key in sample_batch.column_names:\n    sample_value = sample_batch[0][key]\n    if isinstance(sample_value, list):\n        print(f\"  - {key}: shape ({batch_size}, {len(sample_value)})\")\n    else:\n        print(f\"  - {key}: shape ({batch_size},)\")\n\nprint(f\"\\n2ï¸âƒ£ WHAT THE MODEL RECEIVES (for 1 chunk in batch)\")\nprint(\"-\"*80)\nexample_idx = 0\nprint(f\"Input IDs: {len(sample_batch[example_idx]['input_ids'])} tokens\")\nprint(f\"  First 10 token IDs: {sample_batch[example_idx]['input_ids'][:10]}\")\nprint(f\"\\nAttention mask: {sample_batch[example_idx]['attention_mask'][:20]}...\")\nprint(f\"  (1=attend to token, 0=ignore padding)\")\nprint(f\"\\nToken type IDs: {sample_batch[example_idx]['token_type_ids'][:20]}...\")\nprint(f\"  (0=question tokens, 1=context tokens)\")\n\nprint(f\"\\n3ï¸âƒ£ TRAINING TARGETS (what model learns to predict)\")\nprint(\"-\"*80)\nprint(f\"Target start position: {sample_batch[example_idx]['start_positions']}\")\nprint(f\"Target end position: {sample_batch[example_idx]['end_positions']}\")\nprint(f\"\\nğŸ’¡ The model learns to output these exact positions!\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"execution":{"iopub.status.busy":"2025-12-12T11:39:44.213955Z","iopub.execute_input":"2025-12-12T11:39:44.214203Z","iopub.status.idle":"2025-12-12T11:39:44.241464Z","shell.execute_reply.started":"2025-12-12T11:39:44.214181Z","shell.execute_reply":"2025-12-12T11:39:44.240931Z"},"trusted":true},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ“ MODEL TRAINING DATA FLOW\n================================================================================\n\n1ï¸âƒ£ BATCH STRUCTURE\n--------------------------------------------------------------------------------\nBatch size: 4 chunks\nEach chunk in the batch contains:\n  - input_ids: shape (4, 384)\n  - attention_mask: shape (4, 384)\n  - token_type_ids: shape (4, 384)\n  - start_positions: shape (4,)\n  - end_positions: shape (4,)\n  - overflow_to_sample_mapping: shape (4,)\n\n2ï¸âƒ£ WHAT THE MODEL RECEIVES (for 1 chunk in batch)\n--------------------------------------------------------------------------------\nInput IDs: 384 tokens\n  First 10 token IDs: [57344, 1606, 1575, 1606, 1580, 1606, 1711, 32, 1705, 1575]\n\nAttention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]...\n  (1=attend to token, 0=ignore padding)\n\nToken type IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...\n  (0=question tokens, 1=context tokens)\n\n3ï¸âƒ£ TRAINING TARGETS (what model learns to predict)\n--------------------------------------------------------------------------------\nTarget start position: 259\nTarget end position: 262\n\nğŸ’¡ The model learns to output these exact positions!\n\n================================================================================\n","output_type":"stream"}],"execution_count":310},{"id":"5a98237c","cell_type":"markdown","source":"---","metadata":{}},{"id":"24","cell_type":"markdown","source":"## Model Training:\n","metadata":{}},{"id":"26","cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"outputs/canine-s-uqa-filtered\",\n\n    per_device_train_batch_size=4,  # increased train_batch_size from tydiqa\n    per_device_eval_batch_size=16,\n\n    gradient_accumulation_steps=4,  # decreased grad accum from tydiqa\n    gradient_checkpointing=True,\n\n    num_train_epochs=1, # same as tydiqa\n    learning_rate=3e-5,  \n    weight_decay=0.01,\n    \n    eval_strategy=\"no\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=1000,  # increased to 1000\n    logging_steps=50,\n    \n    fp16=True,\n    bf16=False,\n    report_to=\"none\",\n    push_to_hub=True,\n    hub_model_id=\"VohraAK/canine-s-uqa-filtered\",\n    hub_strategy=\"checkpoint\",\n    )\n\n# CustomEvalCallback - EXACT TyDiQA approach\nclass CustomEvalCallback(TrainerCallback):\n    def __init__(self, eval_func, eval_dataset):\n        self.eval_func = eval_func\n        self.eval_dataset = eval_dataset\n\n    def on_save(self, args, state, control, model=None, **kwargs):\n        \"\"\"\n        Runs AFTER checkpoint is saved.\n        Loads checkpoint from disk and evaluates it.\n        \"\"\"\n        checkpoint_path = f\"{args.output_dir}/checkpoint-{state.global_step}\"\n        print(f\"\\nğŸ” Running custom evaluation at step {state.global_step}...\")\n\n        # Call evaluation function (loads from checkpoint)\n        metrics = self.eval_func(checkpoint_path)\n\n        # Add metrics to state's log_history\n        state.log_history.append({\n            \"step\": state.global_step,\n            \"eval_exact_match\": metrics[\"exact_match\"],\n            \"eval_f1\": metrics[\"f1\"],\n            \"eval_edit_distance\": metrics[\"edit_distance\"],\n        })\n\n        # Print metrics\n        print(f\"âœ… Step {state.global_step}: EM={metrics['exact_match']*100:.2f}, F1={metrics['f1']*100:.2f}, EditDist={metrics['edit_distance']*100:.2f}\")\n\n        # Re-save trainer_state.json with updated metrics\n        state_path = f\"{checkpoint_path}/trainer_state.json\"\n        try:\n            with open(state_path, 'r') as f:\n                state_dict = json.load(f)\n            state_dict['log_history'] = state.log_history\n            with open(state_path, 'w') as f:\n                json.dump(state_dict, f, indent=2)\n            print(f\"ğŸ’¾ Updated trainer_state.json with custom metrics\")\n        except Exception as e:\n            print(f\"âš ï¸  Warning: Could not update trainer_state.json: {e}\")\n\n        # Push to Hub\n        try:\n            print(f\"â˜ï¸  Pushing checkpoint-{state.global_step} to Hub...\")\n            api = HfApi()\n            api.upload_folder(\n                folder_path=checkpoint_path,\n                repo_id=args.hub_model_id,\n                path_in_repo=f\"checkpoint-{state.global_step}\",\n                commit_message=f\"Add checkpoint {state.global_step} (EM={metrics['exact_match']*100:.1f}%, F1={metrics['f1']*100:.1f}%)\",\n                repo_type=\"model\"\n            )\n            print(f\"âœ… Pushed checkpoint-{state.global_step} to Hub\")\n        except Exception as e:\n            print(f\"âš ï¸  Warning: Could not push to Hub: {e}\")\n\n        return control\n\n","metadata":{"execution":{"iopub.status.busy":"2025-12-12T11:39:44.243944Z","iopub.execute_input":"2025-12-12T11:39:44.244150Z","iopub.status.idle":"2025-12-12T11:39:44.277196Z","shell.execute_reply.started":"2025-12-12T11:39:44.244134Z","shell.execute_reply":"2025-12-12T11:39:44.276558Z"},"id":"c4abaaab","trusted":true},"outputs":[],"execution_count":311},{"id":"27","cell_type":"code","source":"trainer_cb = CustomEvalCallback(evaluate_checkpoint, processed_val)\n\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=processed_train,\n    eval_dataset=processed_val,\n    callbacks=[trainer_cb],\n)","metadata":{"execution":{"iopub.status.busy":"2025-12-12T11:39:44.277992Z","iopub.execute_input":"2025-12-12T11:39:44.278258Z","iopub.status.idle":"2025-12-12T11:39:44.340603Z","shell.execute_reply.started":"2025-12-12T11:39:44.278234Z","shell.execute_reply":"2025-12-12T11:39:44.339998Z"},"id":"055f5dda","trusted":true},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":312},{"id":"28","cell_type":"code","source":"# trainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.status.busy":"2025-12-12T11:39:44.341357Z","iopub.execute_input":"2025-12-12T11:39:44.342128Z","iopub.status.idle":"2025-12-12T11:39:44.345011Z","shell.execute_reply.started":"2025-12-12T11:39:44.342107Z","shell.execute_reply":"2025-12-12T11:39:44.344302Z"},"id":"TOUimesUX5Re","outputId":"cfa62dcd-8eb4-475a-910b-1c38a3894cc2","trusted":true},"outputs":[],"execution_count":313},{"id":"29","cell_type":"markdown","source":"---","metadata":{}},{"id":"30","cell_type":"markdown","source":"### Diagnosing Preprocessing Functions!!!\n\nThese functions are just analysing the preprocessing logic above, they're just using the base model, NOT our trained model...","metadata":{"id":"cc44692c-6652-4cda-9ba4-8a03acdab88d"}},{"id":"31","cell_type":"code","source":"# # Diagnostic cell (fixed): Investigate preprocessing and truncation for many samples\n# import random\n# import pandas as pd\n# from transformers import AutoTokenizer\n\n# # Set display options to see full Urdu text\n# pd.set_option('display.max_colwidth', None)\n\n# try:\n#     tokenizer = AutoTokenizer.from_pretrained(\"google/canine-s\")\n# except Exception:\n#     tokenizer = None\n\n# num_samples = 20000  # Number of samples to check\n# results = []\n\n# for split_name, orig_data, proc_data in [\n#     (\"train\", uqa_train, processed_train),\n#     (\"val\", uqa_val, processed_val)\n# ]:\n#     # Sample random indices\n#     if len(proc_data) < num_samples:\n#         current_indices = range(len(proc_data))\n#     else:\n#         current_indices = random.sample(range(len(proc_data)), num_samples)\n\n#     for idx in current_indices:\n#         proc = proc_data[idx]\n#         # Use overflow_to_sample_mapping to get the correct original index\n#         orig_idx = proc[\"overflow_to_sample_mapping\"]\n#         orig = orig_data[orig_idx]\n\n#         input_ids = proc[\"input_ids\"]\n#         start_pos = proc[\"start_positions\"]\n#         end_pos = proc[\"end_positions\"]\n\n#         gold_answer = orig.get(\"gold_answer\", orig.get(\"answer\", \"\"))\n#         question = orig.get(\"question\", \"\")\n\n#         # Decode input_ids to text (for debugging context)\n#         if tokenizer:\n#             decoded_text = tokenizer.decode(input_ids, skip_special_tokens=False)\n#         else:\n#             decoded_text = str(input_ids)\n\n#         # Extract predicted answer span\n#         if 0 <= start_pos < len(input_ids) and 0 <= end_pos < len(input_ids):\n#             if tokenizer:\n#                 pred_span = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True)\n#             else:\n#                 pred_span = str(input_ids[start_pos:end_pos+1])\n#         else:\n#             pred_span = \"[CLS]\" # Represents no answer found in this chunk or invalid\n\n#         # Check if pred_span matches gold answer\n#         # We strip() to ignore minor whitespace differences\n#         pred_matches_gold = pred_span.strip() == gold_answer.strip()\n\n#         # Check if gold is even reachable in this chunk\n#         gold_in_decoded = gold_answer in decoded_text\n\n#         results.append({\n#             \"Split\": split_name,\n#             \"Question\": question,\n#             \"Gold Answer\": gold_answer,\n#             \"Extracted Answer\": pred_span,\n#             \"Match\": pred_matches_gold,\n#             \"Gold Reachable\": gold_in_decoded,\n#             \"orig_idx\": orig_idx\n#         })\n\n# # Create DataFrame\n# results_df = pd.DataFrame(results)\n\n# # --- SIDE BY SIDE COMPARISON ---\n\n# # 1. Filter for Solvable Mismatches (Gold was there, but we predicted wrong)\n# problem_cases = results_df[\n#     (results_df[\"Gold Reachable\"] == True) &\n#     (results_df[\"Match\"] == False)\n# ][[\"Question\", \"Gold Answer\", \"Extracted Answer\", \"Split\"]]\n\n# print(f\"ğŸ” Checked {len(results_df)} samples.\")\n# print(f\"âŒ Found {len(problem_cases)} cases where Gold was present but Extraction failed.\")\n\n# print(\"\\nğŸ“Š Side-by-Side Comparison (Top 20 Failures):\")\n# display(problem_cases.head(50))\n\n# print(\"\\nâœ… Side-by-Side Comparison (First 10 Rows - Mixed):\")\n# display(results_df[[\"Question\", \"Gold Answer\", \"Extracted Answer\", \"Match\"]].head(50))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.status.busy":"2025-12-12T11:39:44.345783Z","iopub.execute_input":"2025-12-12T11:39:44.345998Z","iopub.status.idle":"2025-12-12T11:39:44.361768Z","shell.execute_reply.started":"2025-12-12T11:39:44.345977Z","shell.execute_reply":"2025-12-12T11:39:44.361209Z"},"id":"49f3717d","outputId":"38f435a4-1b55-4c2b-b6a5-86540fc23755","trusted":true},"outputs":[],"execution_count":314},{"id":"32","cell_type":"code","source":"# # Accuracy: fraction of rows where extracted answer matches gold answer\n# accuracy = (results_df[\"Match\"]).mean()\n\n# # Precision: among rows where extracted answer is non-empty, fraction that matches gold\n# # We filter out cases where the model predicted nothing (empty string) or just whitespace\n# non_empty_pred = results_df[\"Extracted Answer\"].str.strip() != \"\"\n\n# # Avoid division by zero if no predictions were made\n# if non_empty_pred.sum() > 0:\n#     precision = (results_df[\"Match\"] & non_empty_pred).sum() / non_empty_pred.sum()\n# else:\n#     precision = 0.0\n\n# print(f\"Accuracy: {accuracy:.3f}\")\n# print(f\"Precision: {precision:.3f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-12T11:39:44.362377Z","iopub.execute_input":"2025-12-12T11:39:44.362530Z","iopub.status.idle":"2025-12-12T11:39:44.378937Z","shell.execute_reply.started":"2025-12-12T11:39:44.362518Z","shell.execute_reply":"2025-12-12T11:39:44.378306Z"},"id":"e67abc12","outputId":"c597ec41-a56e-4e5d-9eb6-e71bd0eafd38","trusted":true},"outputs":[],"execution_count":315}]}