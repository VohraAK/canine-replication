{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0","cell_type":"code","source":"# %pip install peft evaluate transformers Levenshtein ipywidgets\n# %pip install protobuf==3.20.3\n# !rm -rf /kaggle/working/cache\n# !rm -rf /kaggle/working/outputs","metadata":{"execution":{"iopub.status.busy":"2025-12-12T10:06:19.737016Z","iopub.execute_input":"2025-12-12T10:06:19.737282Z","iopub.status.idle":"2025-12-12T10:06:19.740840Z","shell.execute_reply.started":"2025-12-12T10:06:19.737262Z","shell.execute_reply":"2025-12-12T10:06:19.739900Z"},"id":"c186240c","trusted":true},"outputs":[],"execution_count":180},{"id":"1","cell_type":"code","source":"# X\n\nimport os\nos.environ[\"TRANSFORMERS_DISABLE_CHAT_TEMPLATES\"] = \"1\"\nos.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\nos.environ[\"TRANSFORMERS_NO_ADDITIONAL_CHAT_TEMPLATES\"] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2025-12-12T10:06:19.745052Z","iopub.execute_input":"2025-12-12T10:06:19.745240Z","iopub.status.idle":"2025-12-12T10:06:19.758970Z","shell.execute_reply.started":"2025-12-12T10:06:19.745225Z","shell.execute_reply":"2025-12-12T10:06:19.758259Z"},"id":"cd8da8ab","trusted":true},"outputs":[],"execution_count":181},{"id":"2","cell_type":"code","source":"import random\nfrom datasets import load_dataset, load_from_disk\nfrom transformers import CanineTokenizer\nfrom peft import LoraConfig, TaskType, get_peft_model\nimport re\nimport string\nfrom collections import Counter\nimport numpy as np\nimport Levenshtein\n\nfrom transformers import TrainingArguments, Trainer, TrainerCallback\nimport json\nfrom huggingface_hub import HfApi, notebook_login, whoami","metadata":{"execution":{"iopub.status.busy":"2025-12-12T10:06:19.760068Z","iopub.execute_input":"2025-12-12T10:06:19.760297Z","iopub.status.idle":"2025-12-12T10:06:19.774974Z","shell.execute_reply.started":"2025-12-12T10:06:19.760272Z","shell.execute_reply":"2025-12-12T10:06:19.774356Z"},"id":"d87eba82","trusted":true},"outputs":[],"execution_count":182},{"id":"3","cell_type":"code","source":"# notebook_login()\n# whoami()","metadata":{"execution":{"iopub.status.busy":"2025-12-12T10:06:19.775707Z","iopub.execute_input":"2025-12-12T10:06:19.776038Z","iopub.status.idle":"2025-12-12T10:06:19.790265Z","shell.execute_reply.started":"2025-12-12T10:06:19.776016Z","shell.execute_reply":"2025-12-12T10:06:19.789666Z"},"id":"0e98cebe-4c08-4850-b3c1-1529564fdb1b","trusted":true},"outputs":[],"execution_count":183},{"id":"4","cell_type":"code","source":"from transformers import CanineTokenizer, CanineForQuestionAnswering\nimport torch\nmodel_name = 'google/canine-s'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n\ntokenizer = CanineTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=False)\nmodel = CanineForQuestionAnswering.from_pretrained(model_name, trust_remote_code=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-12T10:06:19.851658Z","iopub.execute_input":"2025-12-12T10:06:19.851921Z","iopub.status.idle":"2025-12-12T10:06:21.048798Z","shell.execute_reply.started":"2025-12-12T10:06:19.851873Z","shell.execute_reply":"2025-12-12T10:06:21.048194Z"},"id":"f2dd5a40","outputId":"140c30ea-575d-45cd-ea54-7818cdfe6bf5","trusted":true},"outputs":[{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":184},{"id":"f75a7e9b","cell_type":"code","source":"# filter out impossible questions\ndef filter_function(example):\n    return not example['is_impossible']","metadata":{"execution":{"iopub.status.busy":"2025-12-12T10:06:21.049979Z","iopub.execute_input":"2025-12-12T10:06:21.050181Z","iopub.status.idle":"2025-12-12T10:06:21.053743Z","shell.execute_reply.started":"2025-12-12T10:06:21.050164Z","shell.execute_reply":"2025-12-12T10:06:21.053168Z"},"trusted":true},"outputs":[],"execution_count":185},{"id":"5","cell_type":"code","source":"uqa_dataset = load_dataset(\"uqa/UQA\")\n\n# filtering\nuqa_dataset_filtered = uqa_dataset.filter(filter_function)\n\n# trying the full dataset\nuqa_train = uqa_dataset_filtered[\"train\"].shuffle(seed=42)\nuqa_val = uqa_dataset_filtered[\"validation\"].shuffle(seed=42)\n\nuqa_train = uqa_dataset[\"train\"].shuffle(seed=42).select(range(20000))\nuqa_val = uqa_dataset[\"validation\"].shuffle(seed=42).select(range(5000))\n\nprint(f\"ğŸ“Š Dataset after filtering:\")\nprint(f\"   Original train size: {len(uqa_dataset['train']):,}\")\nprint(f\"   Filtered train size: {len(uqa_dataset_filtered['train']):,}\")\nprint(f\"   Using for training: {len(uqa_train):,}\")\nprint(f\"   Validation size: {len(uqa_val):,}\")","metadata":{"execution":{"iopub.status.busy":"2025-12-12T10:06:21.054499Z","iopub.execute_input":"2025-12-12T10:06:21.054714Z","iopub.status.idle":"2025-12-12T10:06:22.064544Z","shell.execute_reply.started":"2025-12-12T10:06:21.054695Z","shell.execute_reply":"2025-12-12T10:06:22.063731Z"},"id":"d474e2e8","trusted":true},"outputs":[{"name":"stdout","text":"ğŸ“Š Dataset after filtering:\n   Original train size: 124,745\n   Filtered train size: 83,018\n   Using for training: 20,000\n   Validation size: 5,000\n","output_type":"stream"}],"execution_count":186},{"id":"297e1ff7-52f6-4981-b360-45141788f2f4","cell_type":"code","source":"# Check character-token alignment\nex = uqa_train[444]\ncontext = ex[\"context\"]\ncontext_tokens = tokenizer.encode(ex[\"context\"], add_special_tokens=False)\n\nprint(f\"Context length (characters): {len(context)}\")\nprint(f\"Context length (tokens): {len(context_tokens)}\")\nprint(f\"1:1 mapping: {len(context) == len(context_tokens)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T10:06:22.065971Z","iopub.execute_input":"2025-12-12T10:06:22.066201Z","iopub.status.idle":"2025-12-12T10:06:22.072491Z","shell.execute_reply.started":"2025-12-12T10:06:22.066185Z","shell.execute_reply":"2025-12-12T10:06:22.071652Z"}},"outputs":[{"name":"stdout","text":"Context length (characters): 749\nContext length (tokens): 749\n1:1 mapping: True\n","output_type":"stream"}],"execution_count":187},{"id":"6","cell_type":"code","source":"# Explore raw UQA dataset structure\nprint(\"=\"*80)\nprint(\"UQA DATASET STRUCTURE\")\nprint(\"=\"*80)\nprint(f\"Training set size: {len(uqa_train):,} examples\")\nprint(f\"Validation set size: {len(uqa_val):,} examples\")\nprint(f\"\\nDataset columns: {uqa_train.column_names}\")\nprint(\"\\n\" + \"=\"*80)\n\n# Show a few examples\nprint(\"\\nğŸ“ EXAMPLE 1 - Question with Answer\")\nprint(\"=\"*80)\nex1 = uqa_train[0]\nprint(f\"Question: {ex1['question']}\")\nprint(f\"\\nContext (first 300 chars): {ex1['context'][:300]}...\")\nprint(f\"\\nAnswer: '{ex1['answer']}'\")\nprint(f\"Answer starts at character position: {ex1['answer_start']}\")\n\n# Verify the answer extraction\nif ex1['answer_start'] != -1:\n    extracted = ex1['context'][ex1['answer_start']:ex1['answer_start']+len(ex1['answer'])]\n    print(f\"âœ“ Extracted from context: '{extracted}'\")\n    print(f\"âœ“ Match: {extracted == ex1['answer']}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nğŸ“ EXAMPLE 2 - Another Question\")\nprint(\"=\"*80)\nex2 = uqa_train[100]\nprint(f\"Question: {ex2['question']}\")\nprint(f\"\\nContext length: {len(ex2['context'])} characters\")\nprint(f\"Answer: '{ex2['answer']}'\")\nprint(f\"Answer starts at position: {ex2['answer_start']}\")\n\n# Show answer in context\nif ex2['answer_start'] != -1:\n    start = max(0, ex2['answer_start'] - 50)\n    end = min(len(ex2['context']), ex2['answer_start'] + len(ex2['answer']) + 50)\n    context_snippet = ex2['context'][start:end]\n    answer_pos = ex2['answer_start'] - start\n    print(f\"\\nContext around answer:\")\n    print(f\"...{context_snippet}...\")\n    print(f\"    {' '*answer_pos}{'~'*len(ex2['answer'])} (answer here)\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nğŸ“Š DATASET STATISTICS\")\nprint(\"=\"*80)\n\n# Compute some basic statistics\nimport numpy as np\nquestion_lengths = [len(ex['question']) for ex in uqa_train.select(range(1000))]\ncontext_lengths = [len(ex['context']) for ex in uqa_train.select(range(1000))]\nanswer_lengths = [len(ex['answer']) if ex['answer'] else 0 for ex in uqa_train.select(range(1000))]\nhas_answer = [ex['answer_start'] != -1 for ex in uqa_train.select(range(1000))]\n\nprint(f\"Question length (chars): mean={np.mean(question_lengths):.1f}, max={np.max(question_lengths)}\")\nprint(f\"Context length (chars): mean={np.mean(context_lengths):.1f}, max={np.max(context_lengths)}\")\nprint(f\"Answer length (chars): mean={np.mean(answer_lengths):.1f}, max={np.max(answer_lengths)}\")\nprint(f\"Questions with answers: {sum(has_answer)/len(has_answer)*100:.1f}%\")\nprint(f\"Questions without answers: {(1-sum(has_answer)/len(has_answer))*100:.1f}%\")","metadata":{"execution":{"iopub.status.busy":"2025-12-12T10:06:22.074331Z","iopub.execute_input":"2025-12-12T10:06:22.074714Z","iopub.status.idle":"2025-12-12T10:06:22.461925Z","shell.execute_reply.started":"2025-12-12T10:06:22.074698Z","shell.execute_reply":"2025-12-12T10:06:22.460944Z"},"trusted":true},"outputs":[{"name":"stdout","text":"================================================================================\nUQA DATASET STRUCTURE\n================================================================================\nTraining set size: 20,000 examples\nValidation set size: 5,000 examples\n\nDataset columns: ['id', 'title', 'context', 'question', 'is_impossible', 'answer', 'answer_start']\n\n================================================================================\n\nğŸ“ EXAMPLE 1 - Question with Answer\n================================================================================\nQuestion: Ø¬Ø¯ÛŒØ¯ ÛÙˆØ§Ø¦ÛŒ Ø¬ÛØ§Ø²ÙˆÚº Ú©ÛŒ ØªØ¹Ù…ÛŒØ± Ù…ÛŒÚº Ú©ÛŒØ§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ØªÚ¾Ø§ØŸ\n\nContext (first 300 chars): 1906 Ù…ÛŒÚº ØŒ Ø§Ù„ÙØ±ÛŒÚˆ ÙˆÙÙ„Ù… Ù†Û’ Ø¨Ø§Ø±Ø´ Ø³Û’ Ø³Ø®Øª ÛÙˆÙ†Û’ ÙˆØ§Ù„Û’ Ù…Ø±Ú©Ø¨ Ø¯Ø±ÛŒØ§ÙØª Ú©ÛŒÛ’Û” Ø¨Ø§Ø±Ø´ Ø³Û’ Ø³Ø®Øª ÛÙˆÙ†Û’ ÙˆØ§Ù„Û’ Ù…Ø±Ú©Ø¨ ØŒ Ø¬ÛŒØ³Û’ Ø§ÛŒÙ„ÙˆÙ…ÛŒÙ†ÛŒÙ… ØŒ Ù¹Ø§Ø¦Ù¹ÛŒÙ†ÛŒÙ… Ø§ÙˆØ± ØªØ§Ù†Ø¨Û’ Ú©Û’ Ú©Ú†Ú¾ Ù…Ø±Ú©Ø¨ ØŒ Ú¯Ø±Ù…ÛŒ Ø³Û’ Ø¹Ù„Ø§Ø¬ Ú©Ø±Ù†Û’ ÙˆØ§Ù„Û’ Ù…Ø±Ú©Ø¨ ÛÛŒÚº Ø¬Ùˆ Ù¹Ú¾Ù†ÚˆØ§ ÛÙˆÙ†Û’ Ù¾Ø± Ù†Ø±Ù… ÛÙˆØ¬Ø§ØªÛ’ ÛÛŒÚº (Ø¬Ù„Ø¯ÛŒ Ø³Û’ Ù¹Ú¾Ù†ÚˆØ§ ÛÙˆØ¬Ø§ØªÛ’ ÛÛŒÚº) ØŒ Ø§ÙˆØ± Ù¾Ú¾Ø± ÙˆÙ‚Øª Ú©Û’ Ø³Ø§ØªÚ¾ Ø³Ø®Øª ÛÙˆØ¬Ø§ØªÛ’ ÛÛŒÚºÛ” Ø§ÛŒÙ„ÙˆÙ…ÛŒÙ†ÛŒÙ… ØŒ ØªØ§Ù†Ø¨Û’ Ø§ÙˆØ± Ù…ÛŒÚ¯Ù†ÛŒ...\n\nAnswer: 'ÚˆÙˆØ±Ø§Ù„ÙˆÙ…ÛŒÙ†'\nAnswer starts at character position: 503\nâœ“ Extracted from context: 'ÚˆÙˆØ±Ø§Ù„ÙˆÙ…ÛŒÙ†'\nâœ“ Match: True\n\n================================================================================\n\nğŸ“ EXAMPLE 2 - Another Question\n================================================================================\nQuestion: Ú©ÙˆÙ† Ø³Ø§ Ù…Ù†Ú¯ Ø´ÛØ²Ø§Ø¯Û Ù†Ø§Ù†Ø¬Ù†Ú¯ Ù…ÛŒÚº ØªØ®Øª Ù†Ø´ÛŒÙ† ÛÙˆØ§ ØªÚ¾Ø§ØŸ\n\nContext length: 447 characters\nAnswer: 'Ø¬Ùˆ ÛŒÙˆ Ø³ÙˆÙ†Ú¯'\nAnswer starts at position: 262\n\nContext around answer:\n...Ø§Ù„ÛŒ Ú†Ù†Ú¯ Ø®Ø§Ù†Ø¯Ø§Ù† Ú©Û’ ÛØ§ØªÚ¾ÙˆÚº Ú¯Ø±Ù†Û’ Ú©Û’ Ø¨Ø¹Ø¯ ØŒ Ù…Ù†Ú¯ Ø´ÛØ²Ø§Ø¯Û Ø¬Ùˆ ÛŒÙˆ Ø³ÙˆÙ†Ú¯ Ú©Ùˆ Ø¬ÙˆÙ† 1644 Ù…ÛŒÚº ÛØ§Ù†Ú¯ Ú¯ÙˆØ§Ù†Ú¯ Ø´ÛÙ†Ø´Ø§Û Ú©Û’ Ø·ÙˆØ± Ù¾Ø± Ù†Ø§Ù†Ø¬Ù†...\n                                                      ~~~~~~~~~~ (answer here)\n\n================================================================================\n\nğŸ“Š DATASET STATISTICS\n================================================================================\nQuestion length (chars): mean=54.1, max=155\nContext length (chars): mean=685.5, max=3179\nAnswer length (chars): mean=11.5, max=142\nQuestions with answers: 64.9%\nQuestions without answers: 35.1%\n","output_type":"stream"}],"execution_count":188},{"id":"8","cell_type":"markdown","source":"---","metadata":{"id":"89c472d5"}},{"id":"9","cell_type":"markdown","source":"## Updated preprocessors!\n\nPreviously, we tried to apply the same approach we used in TYDIQA on UQA, the problem was the preprocessors were aligning the answer spans in units of **byte-level spans** instead of **character-level spans**. The calculations were adding byte-level offsets to the answer lengths, and since Urdu characters may be quantified in multiple bytes, the model was being fed the wrong spans -> GIGO!\n\nWe are now testing an updated preprocessor","metadata":{"id":"6e80a8d3"}},{"id":"10","cell_type":"code","source":"\"\"\"\nFIXED preprocessing function for UQA with CANINE-S.\nTyDiQA-style preprocessor adapted for UQA character offsets.\n\nKey fixes applied:\n1. Uses character-level offsets (UQA native format, no byte conversion needed)\n2. Fixed boundary check: uses `<` instead of `<=` for chunk_end\n3. Calculates gold_char_end as inclusive (answer_start + len(answer) - 1)\n4. Dynamic cls_index for no-answer cases\n5. Simplified context_offset calculation\n\nThis preprocessor passed all 200 real-world UQA examples in testing.\n\"\"\"\n\nMAX_SEQ_LENGTH = 384\nDOC_STRIDE = 64  # Using TyDiQA's value for proven results\n\ndef preprocess_uqa(examples, tokenizer, max_length=MAX_SEQ_LENGTH, doc_stride=DOC_STRIDE, model_obj=None, indices=None):\n    \"\"\"\n    TyDiQA-style preprocessor adapted for UQA (character offsets).\n    \n    Args:\n        examples: Batch with question, context, answer, answer_start fields\n        tokenizer: CanineTokenizer instance\n        max_length: Maximum sequence length (default 384)\n        doc_stride: Sliding window overlap (default 64)\n        model_obj: Optional model object (for compatibility)\n        indices: Optional example indices for overflow mapping\n    \n    Returns:\n        Dict with input_ids, attention_mask, token_type_ids, start_positions, \n        end_positions, overflow_to_sample_mapping\n    \"\"\"\n    questions = [q.strip() for q in examples[\"question\"]]\n    contexts = examples[\"context\"]\n    answers = examples[\"answer\"]\n    answer_starts = examples[\"answer_start\"]\n    \n    special_tokens = tokenizer.num_special_tokens_to_add(pair=True)\n    \n    encoded = {\n        \"input_ids\": [],\n        \"attention_mask\": [],\n        \"token_type_ids\": [],\n        \"start_positions\": [],\n        \"end_positions\": [],\n        \"overflow_to_sample_mapping\": [],\n    }\n    \n    for example_idx, (question, context, answer, answer_start) in enumerate(zip(questions, contexts, answers, answer_starts)):\n        question_tokens = tokenizer.encode(question, add_special_tokens=False)\n        context_tokens = tokenizer.encode(context, add_special_tokens=False)\n        \n        max_context_tokens = max_length - len(question_tokens) - special_tokens\n        if max_context_tokens <= 0 or not context_tokens:\n            continue\n        \n        # UQA uses character offsets (not bytes like TyDiQA)\n        if answer and answer_start != -1:\n            start_char = answer_start\n            end_char = answer_start + len(answer) - 1  # Inclusive\n            answer_span = (start_char, end_char)\n        else:\n            answer_span = None\n        \n        stride_tokens = max_context_tokens - doc_stride\n        if stride_tokens <= 0:\n            stride_tokens = max_context_tokens\n        \n        span_start = 0\n        context_length = len(context_tokens)\n        while span_start < context_length:\n            span_end = min(span_start + max_context_tokens, context_length)\n            context_chunk = context_tokens[span_start:span_end]\n            \n            input_ids = tokenizer.build_inputs_with_special_tokens(question_tokens, context_chunk)\n            token_type_ids = tokenizer.create_token_type_ids_from_sequences(question_tokens, context_chunk)\n            attention_mask = [1] * len(input_ids)\n            \n            cls_index = input_ids.index(tokenizer.cls_token_id)\n            context_offset = len(input_ids) - len(context_chunk) - 1\n            \n            if answer_span is None:\n                start_pos = cls_index\n                end_pos = cls_index\n            else:\n                start_char, end_char = answer_span\n                # CRITICAL FIX: Use < instead of <= for exclusive chunk_end\n                answer_in_chunk = start_char >= span_start and end_char < span_end\n                if answer_in_chunk:\n                    start_pos = context_offset + (start_char - span_start)\n                    end_pos = context_offset + (end_char - span_start)\n                else:\n                    start_pos = cls_index\n                    end_pos = cls_index\n            \n            padding = max_length - len(input_ids)\n            if padding > 0:\n                pad_id = tokenizer.pad_token_id\n                input_ids += [pad_id] * padding\n                attention_mask += [0] * padding\n                token_type_ids += [0] * padding\n            else:\n                input_ids = input_ids[:max_length]\n                attention_mask = attention_mask[:max_length]\n                token_type_ids = token_type_ids[:max_length]\n                if start_pos >= max_length or end_pos >= max_length:\n                    start_pos = cls_index\n                    end_pos = cls_index\n            \n            encoded[\"input_ids\"].append(input_ids)\n            encoded[\"attention_mask\"].append(attention_mask)\n            encoded[\"token_type_ids\"].append(token_type_ids)\n            encoded[\"start_positions\"].append(start_pos)\n            encoded[\"end_positions\"].append(end_pos)\n            encoded[\"overflow_to_sample_mapping\"].append(example_idx if indices is None else indices[example_idx])\n            \n            if span_end == context_length:\n                break\n            span_start += stride_tokens\n    \n    return encoded\n","metadata":{"execution":{"iopub.status.busy":"2025-12-12T10:06:22.462945Z","iopub.execute_input":"2025-12-12T10:06:22.463208Z","iopub.status.idle":"2025-12-12T10:06:22.476564Z","shell.execute_reply.started":"2025-12-12T10:06:22.463190Z","shell.execute_reply":"2025-12-12T10:06:22.475870Z"},"trusted":true},"outputs":[],"execution_count":189},{"id":"12","cell_type":"code","source":"# LoRA config\nlora_config = LoraConfig(\n    task_type=TaskType.QUESTION_ANS,\n    r=8,   # shadowing tydiqa for now\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"query\", \"value\"], # shadowing tydiqa for now\n    bias=\"none\",\n    modules_to_save=[\"qa_outputs\"],\n)\n\ndef print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-12-12T10:06:22.477487Z","iopub.execute_input":"2025-12-12T10:06:22.477821Z","iopub.status.idle":"2025-12-12T10:06:22.494958Z","shell.execute_reply.started":"2025-12-12T10:06:22.477797Z","shell.execute_reply":"2025-12-12T10:06:22.494212Z"},"id":"a3e95eec","trusted":true},"outputs":[],"execution_count":190},{"id":"0ce2da1b","cell_type":"markdown","source":"### Preprocessing examples...","metadata":{}},{"id":"13","cell_type":"code","source":"\nprint(\"=\"*80)\nprint(\"ğŸ”¬ PREPROCESSING WALKTHROUGH - Single Example\")\nprint(\"=\"*80)\n\n# Take one example\nexample = uqa_train[0]\nprint(f\"\\n1ï¸âƒ£ ORIGINAL DATA\")\nprint(\"-\"*80)\nprint(f\"Question: {example['question']}\")\nprint(f\"Answer: '{example['answer']}'\")\nprint(f\"Answer position: {example['answer_start']}\")\nprint(f\"Context length: {len(example['context'])} characters\")\n\n# Preprocess it\nbatch = {\n    'question': [example['question']],\n    'context': [example['context']],\n    'answer': [example['answer']],\n    'answer_start': [example['answer_start']]\n}\nprocessed = preprocess_uqa(batch, tokenizer, indices=[0])\n\nprint(f\"\\n2ï¸âƒ£ AFTER PREPROCESSING\")\nprint(\"-\"*80)\nprint(f\"Number of chunks created: {len(processed['input_ids'])}\")\nprint(f\"(Sliding window creates multiple chunks per example)\")\n\n# Show first chunk in detail\nchunk_idx = 0\nprint(f\"\\n3ï¸âƒ£ CHUNK {chunk_idx} DETAILS\")\nprint(\"-\"*80)\nprint(f\"Input IDs length: {len(processed['input_ids'][chunk_idx])} tokens\")\nprint(f\"Start position: {processed['start_positions'][chunk_idx]}\")\nprint(f\"End position: {processed['end_positions'][chunk_idx]}\")\nprint(f\"Maps to original example: {processed['overflow_to_sample_mapping'][chunk_idx]}\")\n\n# Decode the inputs to show what the model sees\ninput_ids = processed['input_ids'][chunk_idx]\ndecoded_input = tokenizer.decode(input_ids, skip_special_tokens=False)\nprint(f\"\\n4ï¸âƒ£ DECODED INPUT (first 400 chars, with special tokens)\")\nprint(\"-\"*80)\nprint(decoded_input[:400] + \"...\")\n\n# Decode the labeled answer span\nstart_pos = processed['start_positions'][chunk_idx]\nend_pos = processed['end_positions'][chunk_idx]\ncls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n\nif start_pos == cls_idx and end_pos == cls_idx:\n    labeled_answer = \"[NO ANSWER IN THIS CHUNK]\"\nelse:\n    labeled_answer = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True)\n\nprint(f\"\\n5ï¸âƒ£ LABELED ANSWER SPAN IN THIS CHUNK\")\nprint(\"-\"*80)\nprint(f\"Gold answer: '{example['answer']}'\")\nprint(f\"Labeled span: '{labeled_answer}'\")\nprint(f\"Match: {labeled_answer.strip() == example['answer'].strip()}\")\n\n# Show all chunks for this example\nprint(f\"\\n6ï¸âƒ£ ALL CHUNKS FOR THIS EXAMPLE\")\nprint(\"-\"*80)\nfor i in range(len(processed['input_ids'])):\n    start = processed['start_positions'][i]\n    end = processed['end_positions'][i]\n    if start == cls_idx and end == cls_idx:\n        chunk_answer = \"[NO ANSWER]\"\n    else:\n        chunk_answer = tokenizer.decode(processed['input_ids'][i][start:end+1], skip_special_tokens=True).strip()\n    has_answer = \"âœ…\" if chunk_answer == example['answer'].strip() else \"âŒ\"\n    print(f\"  Chunk {i}: {has_answer} '{chunk_answer[:50]}'\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"execution":{"iopub.status.busy":"2025-12-12T10:06:22.495812Z","iopub.execute_input":"2025-12-12T10:06:22.496079Z","iopub.status.idle":"2025-12-12T10:06:22.514796Z","shell.execute_reply.started":"2025-12-12T10:06:22.496058Z","shell.execute_reply":"2025-12-12T10:06:22.514213Z"},"trusted":true},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ”¬ PREPROCESSING WALKTHROUGH - Single Example\n================================================================================\n\n1ï¸âƒ£ ORIGINAL DATA\n--------------------------------------------------------------------------------\nQuestion: Ø¬Ø¯ÛŒØ¯ ÛÙˆØ§Ø¦ÛŒ Ø¬ÛØ§Ø²ÙˆÚº Ú©ÛŒ ØªØ¹Ù…ÛŒØ± Ù…ÛŒÚº Ú©ÛŒØ§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ØªÚ¾Ø§ØŸ\nAnswer: 'ÚˆÙˆØ±Ø§Ù„ÙˆÙ…ÛŒÙ†'\nAnswer position: 503\nContext length: 767 characters\n\n2ï¸âƒ£ AFTER PREPROCESSING\n--------------------------------------------------------------------------------\nNumber of chunks created: 3\n(Sliding window creates multiple chunks per example)\n\n3ï¸âƒ£ CHUNK 0 DETAILS\n--------------------------------------------------------------------------------\nInput IDs length: 384 tokens\nStart position: 0\nEnd position: 0\nMaps to original example: 0\n\n4ï¸âƒ£ DECODED INPUT (first 400 chars, with special tokens)\n--------------------------------------------------------------------------------\nî€€Ø¬Ø¯ÛŒØ¯ ÛÙˆØ§Ø¦ÛŒ Ø¬ÛØ§Ø²ÙˆÚº Ú©ÛŒ ØªØ¹Ù…ÛŒØ± Ù…ÛŒÚº Ú©ÛŒØ§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ§ Ø¬Ø§ØªØ§ ØªÚ¾Ø§ØŸî€1906 Ù…ÛŒÚº ØŒ Ø§Ù„ÙØ±ÛŒÚˆ ÙˆÙÙ„Ù… Ù†Û’ Ø¨Ø§Ø±Ø´ Ø³Û’ Ø³Ø®Øª ÛÙˆÙ†Û’ ÙˆØ§Ù„Û’ Ù…Ø±Ú©Ø¨ Ø¯Ø±ÛŒØ§ÙØª Ú©ÛŒÛ’Û” Ø¨Ø§Ø±Ø´ Ø³Û’ Ø³Ø®Øª ÛÙˆÙ†Û’ ÙˆØ§Ù„Û’ Ù…Ø±Ú©Ø¨ ØŒ Ø¬ÛŒØ³Û’ Ø§ÛŒÙ„ÙˆÙ…ÛŒÙ†ÛŒÙ… ØŒ Ù¹Ø§Ø¦Ù¹ÛŒÙ†ÛŒÙ… Ø§ÙˆØ± ØªØ§Ù†Ø¨Û’ Ú©Û’ Ú©Ú†Ú¾ Ù…Ø±Ú©Ø¨ ØŒ Ú¯Ø±Ù…ÛŒ Ø³Û’ Ø¹Ù„Ø§Ø¬ Ú©Ø±Ù†Û’ ÙˆØ§Ù„Û’ Ù…Ø±Ú©Ø¨ ÛÛŒÚº Ø¬Ùˆ Ù¹Ú¾Ù†ÚˆØ§ ÛÙˆÙ†Û’ Ù¾Ø± Ù†Ø±Ù… ÛÙˆØ¬Ø§ØªÛ’ ÛÛŒÚº (Ø¬Ù„Ø¯ÛŒ Ø³Û’ Ù¹Ú¾Ù†ÚˆØ§ ÛÙˆØ¬Ø§ØªÛ’ ÛÛŒÚº) ØŒ Ø§ÙˆØ± Ù¾Ú¾Ø± ÙˆÙ‚Øª Ú©Û’ Ø³Ø§ØªÚ¾ Ø³Ø®Øª ÛÙˆØ¬Ø§ØªÛ’ ÛÛŒÚºÛ” Ø§ÛŒÙ„ÙˆÙ…ÛŒÙ†ÛŒÙ… ØŒ ØªØ§Ù†Ø¨Û’ Ø§ÙˆØ± Ù…ÛŒÚ¯Ù†ÛŒØ´ÛŒÙ… Ú©Û’ Ù¹Ø±Ù†Ø±ÛŒ Ù…Ø±Ú©Ø¨ Ú©Ùˆ Ù¹Ú¾Ù†Úˆî€...\n\n5ï¸âƒ£ LABELED ANSWER SPAN IN THIS CHUNK\n--------------------------------------------------------------------------------\nGold answer: 'ÚˆÙˆØ±Ø§Ù„ÙˆÙ…ÛŒÙ†'\nLabeled span: '[NO ANSWER IN THIS CHUNK]'\nMatch: False\n\n6ï¸âƒ£ ALL CHUNKS FOR THIS EXAMPLE\n--------------------------------------------------------------------------------\n  Chunk 0: âŒ '[NO ANSWER]'\n  Chunk 1: âœ… 'ÚˆÙˆØ±Ø§Ù„ÙˆÙ…ÛŒÙ†'\n  Chunk 2: âŒ '[NO ANSWER]'\n\n================================================================================\n","output_type":"stream"}],"execution_count":191},{"id":"14","cell_type":"markdown","source":"## ğŸ”§ Preprocessing Exploration: Raw Data â†’ Model Input\n\nNow let's see what happens during preprocessing - how we convert text to token IDs and create training labels.","metadata":{}},{"id":"25","cell_type":"code","source":"def normalize_answer(text):\n    text = (text or \"\").lower()\n    def remove_articles(s):\n        return re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n    def remove_punctuation(s):\n        return \"\".join(ch for ch in s if ch not in string.punctuation)\n    def white_space_fix(s):\n        return \" \".join(s.split())\n    return white_space_fix(remove_articles(remove_punctuation(text)))\n\ndef exact_match_score(prediction, ground_truth):\n    return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n\ndef f1_score(prediction, ground_truth):\n    pred_tokens = normalize_answer(prediction).split()\n    gold_tokens = normalize_answer(ground_truth).split()\n    if not gold_tokens:\n        return 1.0 if not pred_tokens else 0.0\n    if not pred_tokens:\n        return 0.0\n    common = Counter(pred_tokens) & Counter(gold_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0.0\n    precision = num_same / len(pred_tokens)\n    recall = num_same / len(gold_tokens)\n    # BUGFIX: Prevent division by zero if both precision and recall are 0\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n\ndef decode_prediction(input_ids, start_idx, end_idx):\n\n    global tokenizer\n    \n    # Dynamic CLS handling\n    cls_index = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n    \n    # No answer case (both point to CLS)\n    if start_idx == cls_index and end_idx == cls_index:\n        return \"\"\n    \n    # Invalid range (start after end) - treat as no answer\n    if start_idx > end_idx:\n        return \"\"\n    \n    # Defensive bounds checking\n    if start_idx < 0 or end_idx < 0:\n        return \"\"\n    if start_idx >= len(input_ids) or end_idx >= len(input_ids):\n        return \"\"\n    \n    # Clamp to valid range (additional safety)\n    start_idx = max(start_idx, 0)\n    end_idx = min(end_idx, len(input_ids) - 1)\n    \n    # Decode with inclusive slicing [start:end+1]\n    text = tokenizer.decode(input_ids[start_idx:end_idx + 1], skip_special_tokens=True)\n    return text.strip()\n\ndef gold_answer(example):\n    if example[\"answer_start\"] == -1:\n        return \"\"\n    return example[\"answer\"]\n\ndef edit_distance_score(prediction, ground_truth):\n    return Levenshtein.ratio(normalize_answer(prediction), normalize_answer(ground_truth))\n\n\n#--- CHANGED TO MATCH TYDIQA APPROACH\ndef evaluate_checkpoint(checkpoint_path=None):\n    \"\"\"\n    EXACT REPLICA of TyDiQA evaluation approach.\n    Loads checkpoint from disk (or uses provided path).\n    \"\"\"\n    # Load base model + trained adapter (TyDiQA approach)\n    base_model = CanineForQuestionAnswering.from_pretrained(\n        model_name, \n        trust_remote_code=False\n    )\n    \n    from peft import PeftModel\n    model = PeftModel.from_pretrained(base_model, checkpoint_path)\n    model.to(device)\n    \n    # Exact TyDiQA eval args\n    eval_args = TrainingArguments(\n        output_dir=\"outputs/canine-s-uqa-filtered\",\n        per_device_eval_batch_size=1,  # Match TyDiQA exactly\n        dataloader_drop_last=False,\n        fp16=False,  # TyDiQA uses False\n        bf16=False,\n        report_to=\"none\"\n    )\n    \n    eval_trainer = Trainer(\n        model=model,\n        args=eval_args,\n        eval_dataset=processed_val,\n        processing_class=tokenizer,  # Use processing_class\n    )\n    \n    # Progress bar (optional, TyDiQA has this)\n    print(f\"ğŸ§ª Evaluating checkpoint: {checkpoint_path}\")\n    from tqdm.auto import tqdm\n    with tqdm(total=len(processed_val), desc=\"Evaluating\", unit=\"samples\") as pbar:\n        predictions = eval_trainer.predict(processed_val)\n        pbar.update(len(processed_val))\n    \n    start_logits, end_logits = predictions.predictions\n    \n    # EXACT TyDiQA aggregation logic\n    best_predictions = {}\n    for feature_index, feature in enumerate(processed_val):\n        sample_idx = int(feature[\"overflow_to_sample_mapping\"])\n        input_ids = feature[\"input_ids\"]\n        \n        start_idx = int(np.argmax(start_logits[feature_index]))\n        end_idx = int(np.argmax(end_logits[feature_index]))\n        score = float(start_logits[feature_index][start_idx] + end_logits[feature_index][end_idx])\n        prediction_text = decode_prediction(input_ids, start_idx, end_idx)\n        \n        stored = best_predictions.get(sample_idx)\n        if stored is None or score > stored[0]:\n            best_predictions[sample_idx] = (score, prediction_text)\n\n    # TEST!\n    # After best_predictions loop, before computing metrics:\n    print(f\"\\nğŸ” Debug: Sample predictions:\")\n    for idx in list(best_predictions.keys())[:5]:\n        score, pred = best_predictions[idx]\n        gold = gold_answer(uqa_val[idx])\n        print(f\"  Pred: '{pred[:50]}' | Gold: '{gold[:50]}'\")\n    \n    # Calculate metrics\n    em_scores = []\n    f1_scores = []\n    edit_dist_scores = []\n    for sample_idx, (_, prediction_text) in best_predictions.items():\n        reference = gold_answer(uqa_val[int(sample_idx)])\n        em_scores.append(exact_match_score(prediction_text, reference))\n        f1_scores.append(f1_score(prediction_text, reference))\n        edit_dist_scores.append(edit_distance_score(prediction_text, reference))\n    \n    em = float(np.mean(em_scores)) if em_scores else 0.0\n    f1 = float(np.mean(f1_scores)) if f1_scores else 0.0\n    edit_dist = float(np.mean(edit_dist_scores)) if edit_dist_scores else 0.0\n    \n    print(f\"Examples evaluated: {len(em_scores)}\")\n    print(f\"Exact Match: {em * 100:.2f}\")\n    print(f\"F1: {f1 * 100:.2f}\")\n    print(f\"Edit Distance (normalized): {edit_dist * 100:.2f}\")\n    \n    return {\"exact_match\": em, \"f1\": f1, \"edit_distance\": edit_dist}","metadata":{"execution":{"iopub.status.busy":"2025-12-12T10:06:22.515818Z","iopub.execute_input":"2025-12-12T10:06:22.516234Z","iopub.status.idle":"2025-12-12T10:06:22.535799Z","shell.execute_reply.started":"2025-12-12T10:06:22.516218Z","shell.execute_reply":"2025-12-12T10:06:22.535195Z"},"trusted":true},"outputs":[],"execution_count":192},{"id":"15","cell_type":"code","source":"# âš ï¸ CRITICAL: Must regenerate preprocessed data with FILTERED dataset\n# The old cache was created from unfiltered data - indices won't match!\n\n# print(\"ğŸ”„ Preprocessing filtered dataset (this will take a few minutes)...\")\nprocessed_train = uqa_train.map(\n    lambda examples, indices: preprocess_uqa(examples, tokenizer, indices=indices), \n    batched=True, \n    remove_columns=uqa_train.column_names, \n    with_indices=True\n)\nprocessed_val = uqa_val.map(\n    lambda examples, indices: preprocess_uqa(examples, tokenizer, indices=indices), \n    batched=True, \n    remove_columns=uqa_val.column_names, \n    with_indices=True\n)\n\n# print(f\"âœ… Preprocessing complete!\")\n# print(f\"   Training chunks: {len(processed_train):,}\")\n# print(f\"   Validation chunks: {len(processed_val):,}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["b1984a6d29864e2d940119370816a37e","a307de6c263a4c20a6418344cbd98c0c","1db208af0dbc4f2facee78148266a207","d44d38a959ec44aa90e91c15a83abbd6","527baa5fc421480da4d2dc7041e19b1f","d398c81b546d4527a41dd97bd87ad7d8","ab4047c7f0144667857fe835d452f6c7","0120d513dd4d4fccac2d528eb7ff4696","c3e0981c2924416fbdf9ceab3e6b04ab","bc970c64373f4f69b6c6936087ed978a","4a74d22a2c334fbda54a95c5e29e712a"]},"execution":{"iopub.status.busy":"2025-12-12T10:06:22.536579Z","iopub.execute_input":"2025-12-12T10:06:22.536809Z","iopub.status.idle":"2025-12-12T10:06:54.211710Z","shell.execute_reply.started":"2025-12-12T10:06:22.536794Z","shell.execute_reply":"2025-12-12T10:06:54.211130Z"},"id":"d11807b9","outputId":"64fc2534-2871-4bd2-b3fa-4b37973486e2","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"170fd94970ba4a1c93821a0c154e3718"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (3179 > 2048). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6b8ae4228974fa8af165aadf49e7446"}},"metadata":{}}],"execution_count":193},{"id":"3d909f3d","cell_type":"code","source":"# print(\"=\"*80)\n# print(\"ğŸ“ˆ DATASET STATISTICS AFTER PREPROCESSING\")\n# print(\"=\"*80)\n\n# # Count chunks per example\n# from collections import Counter\n# chunks_per_example = Counter(processed_train[\"overflow_to_sample_mapping\"])\n# chunks_distribution = Counter(chunks_per_example.values())\n\n# print(f\"\\nğŸ“¦ Chunks Distribution:\")\n# print(f\"   Total original examples: {len(uqa_train):,}\")\n# print(f\"   Total preprocessed chunks: {len(processed_train):,}\")\n# print(f\"   Average chunks per example: {len(processed_train)/len(uqa_train):.2f}\")\n# print(f\"\\n   Distribution:\")\n# for num_chunks in sorted(chunks_distribution.keys())[:10]:\n#     count = chunks_distribution[num_chunks]\n#     print(f\"     {num_chunks} chunk(s): {count:,} examples ({count/len(uqa_train)*100:.1f}%)\")\n\n# # Count examples with answers in at least one chunk\n# examples_with_answers = 0\n# for orig_idx in range(len(uqa_train)):\n#     # Find all chunks for this example\n#     chunk_indices = [i for i, x in enumerate(processed_train[\"overflow_to_sample_mapping\"]) if x == orig_idx]\n    \n#     # Check if any chunk has an answer (not pointing to CLS)\n#     has_answer = False\n#     for chunk_idx in chunk_indices:\n#         input_ids = processed_train[chunk_idx][\"input_ids\"]\n#         start_pos = processed_train[chunk_idx][\"start_positions\"]\n#         end_pos = processed_train[chunk_idx][\"end_positions\"]\n#         cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n        \n#         if not (start_pos == cls_idx and end_pos == cls_idx):\n#             has_answer = True\n#             break\n    \n#     if has_answer:\n#         examples_with_answers += 1\n\n# print(f\"\\nâœ… Answer Coverage:\")\n# print(f\"   Examples with answer in at least one chunk: {examples_with_answers:,}/{len(uqa_train):,} ({examples_with_answers/len(uqa_train)*100:.1f}%)\")\n# print(f\"   Expected: ~100% (since we filtered impossible questions)\")\n\n# print(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T10:06:54.212761Z","iopub.execute_input":"2025-12-12T10:06:54.213205Z","iopub.status.idle":"2025-12-12T10:06:54.217496Z","shell.execute_reply.started":"2025-12-12T10:06:54.213185Z","shell.execute_reply":"2025-12-12T10:06:54.216770Z"}},"outputs":[],"execution_count":194},{"id":"917b817e","cell_type":"code","source":"print(\"=\"*80)\nprint(\"ğŸ” BOUNDARY LOGIC VERIFICATION\")\nprint(\"=\"*80)\n\n# Test the critical boundary check logic\n# Find examples where answer is near chunk boundaries\n\nboundary_cases_found = 0\nboundary_cases_correct = 0\n\nfor proc_idx in random.sample(range(len(processed_train)), min(500, len(processed_train))):\n    proc_example = processed_train[proc_idx]\n    orig_idx = proc_example[\"overflow_to_sample_mapping\"]\n    orig_example = uqa_train[orig_idx]\n    \n    input_ids = proc_example[\"input_ids\"]\n    start_pos = proc_example[\"start_positions\"]\n    end_pos = proc_example[\"end_positions\"]\n    \n    cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n    \n    # Skip no-answer cases\n    if start_pos == cls_idx and end_pos == cls_idx:\n        continue\n    \n    # Check if this is a boundary case (answer near end of chunk)\n    # Context starts after first SEP token\n    sep_indices = [k for k, x in enumerate(input_ids) if x == tokenizer.sep_token_id]\n    if not sep_indices:\n        continue\n    \n    context_start = sep_indices[0] + 1\n    # Find context end (before padding or second SEP)\n    try:\n        context_end = sep_indices[1] if len(sep_indices) > 1 else len(input_ids)\n    except:\n        context_end = len(input_ids)\n    \n    # Check if answer ends near chunk boundary (within last 10 tokens)\n    if context_end - end_pos <= 10:\n        boundary_cases_found += 1\n        \n        # Verify the answer is correct\n        predicted_answer = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True).strip()\n        gold_ans = orig_example[\"answer\"].strip()\n        \n        if predicted_answer == gold_ans:\n            boundary_cases_correct += 1\n\nprint(f\"\\nğŸ“Š Boundary cases found: {boundary_cases_found}\")\nif boundary_cases_found > 0:\n    print(f\"âœ… Boundary cases correct: {boundary_cases_correct}/{boundary_cases_found} ({boundary_cases_correct/boundary_cases_found*100:.1f}%)\")\n    print(f\"\\nğŸ’¡ This verifies the fix: using `<` instead of `<=` for chunk boundaries\")\nelse:\n    print(f\"âš ï¸  No boundary cases found in sample (may need more examples)\")\n\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T10:06:54.218343Z","iopub.execute_input":"2025-12-12T10:06:54.218588Z","iopub.status.idle":"2025-12-12T10:06:54.814790Z","shell.execute_reply.started":"2025-12-12T10:06:54.218569Z","shell.execute_reply":"2025-12-12T10:06:54.813922Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ” BOUNDARY LOGIC VERIFICATION\n================================================================================\n\nğŸ“Š Boundary cases found: 3\nâœ… Boundary cases correct: 3/3 (100.0%)\n\nğŸ’¡ This verifies the fix: using `<` instead of `<=` for chunk boundaries\n================================================================================\n","output_type":"stream"}],"execution_count":195},{"id":"d9e9db1f","cell_type":"code","source":"import random\n\nprint(\"=\"*80)\nprint(\"ğŸ§ª VERIFICATION TEST: Preprocessor Correctness\")\nprint(\"=\"*80)\n\n# Test on 100 random examples\nnum_test_samples = 100\ntest_indices = random.sample(range(len(processed_train)), min(num_test_samples, len(processed_train)))\n\npassed = 0\nfailed = 0\nfailed_examples = []\n\nfor proc_idx in test_indices:\n    proc_example = processed_train[proc_idx]\n    orig_idx = proc_example[\"overflow_to_sample_mapping\"]\n    orig_example = uqa_train[orig_idx]\n    \n    input_ids = proc_example[\"input_ids\"]\n    start_pos = proc_example[\"start_positions\"]\n    end_pos = proc_example[\"end_positions\"]\n    \n    # Find CLS position\n    cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n    \n    # Extract predicted answer\n    if start_pos == cls_idx and end_pos == cls_idx:\n        predicted_answer = \"\"\n    else:\n        predicted_answer = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True).strip()\n    \n    gold_ans = orig_example[\"answer\"].strip()\n    \n    # Check if they match\n    if predicted_answer == gold_ans or (not gold_ans and start_pos == cls_idx):\n        passed += 1\n    else:\n        failed += 1\n        if len(failed_examples) < 5:  # Store first 5 failures for inspection\n            failed_examples.append({\n                \"question\": orig_example[\"question\"][:50] + \"...\",\n                \"gold\": gold_ans,\n                \"predicted\": predicted_answer,\n                \"positions\": f\"[{start_pos}, {end_pos}]\"\n            })\n\nprint(f\"\\nğŸ“Š RESULTS:\")\nprint(f\"âœ… Passed: {passed}/{num_test_samples} ({passed/num_test_samples*100:.1f}%)\")\nprint(f\"âŒ Failed: {failed}/{num_test_samples} ({failed/num_test_samples*100:.1f}%)\")\n\nif failed > 0 and failed_examples:\n    print(f\"\\nâš ï¸  First {len(failed_examples)} failures:\")\n    for i, ex in enumerate(failed_examples, 1):\n        print(f\"\\n  Example {i}:\")\n        print(f\"    Question: {ex['question']}\")\n        print(f\"    Expected: '{ex['gold']}'\")\n        print(f\"    Got: '{ex['predicted']}'\")\n        print(f\"    Positions: {ex['positions']}\")\nelse:\n    print(f\"\\nğŸ‰ All examples passed! Preprocessor is working correctly.\")\n\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T10:06:54.815714Z","iopub.execute_input":"2025-12-12T10:06:54.816007Z","iopub.status.idle":"2025-12-12T10:06:54.902456Z","shell.execute_reply.started":"2025-12-12T10:06:54.815979Z","shell.execute_reply":"2025-12-12T10:06:54.901918Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ§ª VERIFICATION TEST: Preprocessor Correctness\n================================================================================\n\nğŸ“Š RESULTS:\nâœ… Passed: 61/100 (61.0%)\nâŒ Failed: 39/100 (39.0%)\n\nâš ï¸  First 5 failures:\n\n  Example 1:\n    Question: Ø§Ú¯Ø± ØµØ§Ø±ÙÛŒÙ† Ø§ØµÙ„ Ù¾Ù„Û’ Ø§Ø³Ù¹ÛŒØ´Ù† Ú¯ÛŒÙ…Ø² ÚˆØ§Ø¤Ù† Ù„ÙˆÚˆ Ú©Ø±Ù†Ø§ Ú†Ø§ÛØªÛ’...\n    Expected: 'Ù¾Ù„ÛŒ Ø§Ø³Ù¹ÛŒØ´Ù† Ø§Ø³Ù¹ÙˆØ±'\n    Got: ''\n    Positions: [0, 0]\n\n  Example 2:\n    Question: Ù…Ø­ÛŒ Ø§Ù„Ø¯ÛŒÙ† Ù†Û’ 1967 Ù…ÛŒÚº Ù†ØµÛŒØ± Ú©ÛŒ ØºÙ„Ø·ÛŒÙˆÚº Ú©ÛŒ ÙˆØ¬Û Ú©ÛŒØ§ Ø¨Øª...\n    Expected: 'Ø°ÛŒØ§Ø¨ÛŒØ·Ø³'\n    Got: ''\n    Positions: [0, 0]\n\n  Example 3:\n    Question: Ø±Ù†Ú†Ùˆ ÙˆØ³Ù¹ÙˆØ³Ùˆ Ú©Ø³ Ø´ÛØ± Ù…ÛŒÚº ÙˆØ§Ù‚Ø¹ ÛÛ’ØŸ...\n    Expected: 'Ø§ÙˆØ±Ùˆ ÙˆÛŒÙ„ÛŒ'\n    Got: ''\n    Positions: [0, 0]\n\n  Example 4:\n    Question: ÙÛŒÙ†Ø¨Ø±Ú¯ Ø§Ø³Ú©ÙˆÙ„ Ø¢Ù Ù…ÛŒÚˆÛŒØ³Ù† Ú©Û’ Ú©Ø³ Ú¯Ø±ÛŒØ¬ÙˆÛŒÙ¹ Ú©Û’ Ù†Ø§Ù… Ù¾Ø± Ø±ÙˆØ²...\n    Expected: 'Ù…ÛŒØ±ÛŒ ÛÛŒØ±Ø³ ØªÚ¾Ø§Ù…Ø³Ù†'\n    Got: ''\n    Positions: [0, 0]\n\n  Example 5:\n    Question: Ø§Ø³Ù¹ÙˆØ±ÛŒ Ø¨ÙˆØ±ÚˆØ² Ú©Ùˆ Ú©ÛŒØ§ Ù†ÛÛŒÚº Ø³Ù…Ø¬Ú¾Ø§ Ø¬Ø§ØªØ§ ÛÛ’ØŸ...\n    Expected: 'Ø­ØªÙ…ÛŒ Ù…ØµÙ†ÙˆØ¹'\n    Got: ''\n    Positions: [0, 0]\n================================================================================\n","output_type":"stream"}],"execution_count":196},{"id":"dd016dd8","cell_type":"markdown","source":"## âœ… Verification: Test Preprocessed Results\n\nBefore training, let's verify that the new preprocessor produces correct results.","metadata":{}},{"id":"58a880a7","cell_type":"code","source":"print(\"=\"*80)\nprint(\"ğŸ§ª TEST 1: Training Data Integrity\")\nprint(\"=\"*80)\n\n# Verify training data format\nprint(\"\\n1ï¸âƒ£ Checking training dataset structure...\")\nrequired_columns = [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"start_positions\", \"end_positions\", \"overflow_to_sample_mapping\"]\nmissing = [col for col in required_columns if col not in processed_train.column_names]\n\nif missing:\n    print(f\"âŒ CRITICAL: Missing columns: {missing}\")\nelse:\n    print(f\"âœ… All required columns present: {required_columns}\")\n\n# Check shapes and ranges\nprint(\"\\n2ï¸âƒ£ Validating tensor shapes and ranges...\")\nissues = []\n\nfor i in range(min(100, len(processed_train))):\n    example = processed_train[i]\n    \n    # Check lengths\n    if len(example[\"input_ids\"]) != MAX_SEQ_LENGTH:\n        issues.append(f\"Example {i}: input_ids length {len(example['input_ids'])} != {MAX_SEQ_LENGTH}\")\n    if len(example[\"attention_mask\"]) != MAX_SEQ_LENGTH:\n        issues.append(f\"Example {i}: attention_mask length mismatch\")\n    if len(example[\"token_type_ids\"]) != MAX_SEQ_LENGTH:\n        issues.append(f\"Example {i}: token_type_ids length mismatch\")\n    \n    # Check position ranges\n    start = example[\"start_positions\"]\n    end = example[\"end_positions\"]\n    if start < 0 or start >= MAX_SEQ_LENGTH:\n        issues.append(f\"Example {i}: start_position {start} out of range\")\n    if end < 0 or end >= MAX_SEQ_LENGTH:\n        issues.append(f\"Example {i}: end_position {end} out of range\")\n    if start > end:\n        issues.append(f\"Example {i}: start {start} > end {end}\")\n\nif issues:\n    print(f\"âŒ Found {len(issues)} issues:\")\n    for issue in issues[:10]:  # Show first 10\n        print(f\"   {issue}\")\nelse:\n    print(f\"âœ… All shapes and ranges valid (checked 100 examples)\")\n\n# Check overflow mapping\nprint(\"\\n3ï¸âƒ£ Validating overflow_to_sample_mapping...\")\nmax_orig_idx = max(processed_train[\"overflow_to_sample_mapping\"])\nif max_orig_idx >= len(uqa_train):\n    print(f\"âŒ CRITICAL: overflow_to_sample_mapping has index {max_orig_idx} >= dataset size {len(uqa_train)}\")\nelse:\n    print(f\"âœ… overflow_to_sample_mapping valid (max={max_orig_idx}, dataset size={len(uqa_train)})\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T10:06:54.905269Z","iopub.execute_input":"2025-12-12T10:06:54.905458Z","iopub.status.idle":"2025-12-12T10:06:55.671053Z","shell.execute_reply.started":"2025-12-12T10:06:54.905443Z","shell.execute_reply":"2025-12-12T10:06:55.670224Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ§ª TEST 1: Training Data Integrity\n================================================================================\n\n1ï¸âƒ£ Checking training dataset structure...\nâœ… All required columns present: ['input_ids', 'attention_mask', 'token_type_ids', 'start_positions', 'end_positions', 'overflow_to_sample_mapping']\n\n2ï¸âƒ£ Validating tensor shapes and ranges...\nâœ… All shapes and ranges valid (checked 100 examples)\n\n3ï¸âƒ£ Validating overflow_to_sample_mapping...\nâœ… overflow_to_sample_mapping valid (max=19999, dataset size=20000)\n\n================================================================================\n","output_type":"stream"}],"execution_count":197},{"id":"4e0279bf","cell_type":"code","source":"import random\nfrom collections import defaultdict\n\nprint(\"=\"*80)\nprint(\"ğŸ§ª FIXED TEST: Answer Extraction Accuracy (OPTIMIZED)\")\nprint(\"=\"*80)\n\n# Step 1: Build reverse index ONCE (O(n) instead of O(nÂ²))\nprint(\"Building chunk index...\")\nchunk_index = defaultdict(list)\nfor chunk_idx, sample_idx in enumerate(processed_train[\"overflow_to_sample_mapping\"]):\n    chunk_index[sample_idx].append(chunk_idx)\n\n# Step 2: Test on random original examples\nnum_samples = 200\ntest_orig_indices = random.sample(range(len(uqa_train)), num_samples)\n\ncorrect = 0\nincorrect = 0\nfailed_examples = []\n\nfor orig_idx in test_orig_indices:\n    orig_example = uqa_train[orig_idx]\n    gold_ans = orig_example[\"answer\"].strip()\n    \n    # Get all chunks for this example (O(1) lookup!)\n    chunk_indices = chunk_index[orig_idx]\n    \n    # Check if ANY chunk has the correct answer\n    found_correct = False\n    for chunk_idx in chunk_indices:\n        proc = processed_train[chunk_idx]\n        input_ids = proc[\"input_ids\"]\n        start = proc[\"start_positions\"]\n        end = proc[\"end_positions\"]\n        \n        cls_idx = input_ids.index(tokenizer.cls_token_id)\n        \n        # Skip chunks without answer\n        if start == cls_idx and end == cls_idx:\n            continue\n        \n        # Extract answer\n        predicted = tokenizer.decode(input_ids[start:end+1], skip_special_tokens=True).strip()\n        \n        if predicted.lower() == gold_ans.lower():\n            found_correct = True\n            break\n    \n    if found_correct or not gold_ans:\n        correct += 1\n    else:\n        incorrect += 1\n        if len(failed_examples) < 5:\n            failed_examples.append({\n                \"idx\": orig_idx,\n                \"question\": orig_example[\"question\"][:60],\n                \"gold\": gold_ans[:50],\n                \"num_chunks\": len(chunk_indices)\n            })\n\naccuracy = correct / num_samples * 100\nprint(f\"\\nğŸ“Š Results: âœ… {correct}/{num_samples} ({accuracy:.1f}%)\")\n\nif accuracy >= 95:\n    print(\"âœ… PASSED - Preprocessor working correctly!\")\nelse:\n    print(f\"âŒ FAILED - Only {accuracy:.1f}% accuracy\")\n    if failed_examples:\n        print(f\"\\nâš ï¸ First {len(failed_examples)} failures:\")\n        for ex in failed_examples:\n            print(f\"  #{ex['idx']}: '{ex['question']}...'\")\n            print(f\"    Expected: '{ex['gold']}', Chunks: {ex['num_chunks']}\")\n\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T10:06:55.671981Z","iopub.execute_input":"2025-12-12T10:06:55.672248Z","iopub.status.idle":"2025-12-12T10:06:56.719394Z","shell.execute_reply.started":"2025-12-12T10:06:55.672222Z","shell.execute_reply":"2025-12-12T10:06:56.718756Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ§ª FIXED TEST: Answer Extraction Accuracy (OPTIMIZED)\n================================================================================\nBuilding chunk index...\n\nğŸ“Š Results: âœ… 200/200 (100.0%)\nâœ… PASSED - Preprocessor working correctly!\n================================================================================\n","output_type":"stream"}],"execution_count":198},{"id":"29130506","cell_type":"code","source":"print(\"=\"*80)\nprint(\"ğŸ§ª TEST 3: Validation Data Integrity\")\nprint(\"=\"*80)\n\n# Same checks for validation data\nprint(\"\\n1ï¸âƒ£ Checking validation dataset structure...\")\nmissing_val = [col for col in required_columns if col not in processed_val.column_names]\n\nif missing_val:\n    print(f\"âŒ CRITICAL: Missing columns: {missing_val}\")\nelse:\n    print(f\"âœ… All required columns present\")\n\n# Check validation mapping\nprint(\"\\n2ï¸âƒ£ Validating overflow_to_sample_mapping...\")\nmax_val_idx = max(processed_val[\"overflow_to_sample_mapping\"])\nif max_val_idx >= len(uqa_val):\n    print(f\"âŒ CRITICAL: overflow_to_sample_mapping has index {max_val_idx} >= dataset size {len(uqa_val)}\")\nelse:\n    print(f\"âœ… overflow_to_sample_mapping valid (max={max_val_idx}, dataset size={len(uqa_val)})\")\n\n# Test extraction on validation\nprint(\"\\n3ï¸âƒ£ Testing answer extraction on validation set...\")\nval_correct = 0\nval_incorrect = 0\nval_samples = min(100, len(processed_val))\n\nfor proc_idx in range(val_samples):\n    proc_example = processed_val[proc_idx]\n    orig_idx = proc_example[\"overflow_to_sample_mapping\"]\n    orig_example = uqa_val[orig_idx]\n    \n    input_ids = proc_example[\"input_ids\"]\n    start_pos = proc_example[\"start_positions\"]\n    end_pos = proc_example[\"end_positions\"]\n    \n    cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n    \n    if start_pos == cls_idx and end_pos == cls_idx:\n        predicted_answer = \"\"\n    else:\n        predicted_answer = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True).strip()\n    \n    gold_ans = orig_example[\"answer\"].strip()\n\n    print(f\"GOLD: {gold_ans.lower()}\")\n    print(f\"PREDICTED: {predicted_answer.lower()}\\n\")\n    \n    if predicted_answer.lower() == gold_ans.lower():\n        val_correct += 1\n    else:\n        val_incorrect += 1\n\nval_accuracy = val_correct / val_samples * 100\nprint(f\"   Validation accuracy: {val_correct}/{val_samples} ({val_accuracy:.1f}%)\")\n\nif val_accuracy < 95:\n    print(f\"   âŒ WARNING: Validation accuracy is low!\")\nelse:\n    print(f\"   âœ… Validation data is correct!\")\n\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T10:06:56.720313Z","iopub.execute_input":"2025-12-12T10:06:56.720569Z","iopub.status.idle":"2025-12-12T10:06:56.998875Z","shell.execute_reply.started":"2025-12-12T10:06:56.720552Z","shell.execute_reply":"2025-12-12T10:06:56.998311Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ§ª TEST 3: Validation Data Integrity\n================================================================================\n\n1ï¸âƒ£ Checking validation dataset structure...\nâœ… All required columns present\n\n2ï¸âƒ£ Validating overflow_to_sample_mapping...\nâœ… overflow_to_sample_mapping valid (max=4999, dataset size=5000)\n\n3ï¸âƒ£ Testing answer extraction on validation set...\nGOLD: Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û\nPREDICTED: Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û\n\nGOLD: Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û\nPREDICTED: \n\nGOLD: Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û\nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û\nPREDICTED: ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û\n\nGOLD: ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û\nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: Ú¯Ø§Ù„Ø´ Ù†Ø§Ù… Ø±ÛŒÙˆÙ†ÙˆØ³\nPREDICTED: Ú¯Ø§Ù„Ø´ Ù†Ø§Ù… Ø±ÛŒÙˆÙ†ÙˆØ³\n\nGOLD: Ú¯Ø§Ù„Ø´ Ù†Ø§Ù… Ø±ÛŒÙˆÙ†ÙˆØ³\nPREDICTED: \n\nGOLD: Ú¯Ø§Ù„Ø´ Ù†Ø§Ù… Ø±ÛŒÙˆÙ†ÙˆØ³\nPREDICTED: \n\nGOLD: p\nPREDICTED: \n\nGOLD: p\nPREDICTED: p\n\nGOLD: p\nPREDICTED: \n\nGOLD: Ù†ÛŒÙ¹ ÙˆØ±Ú©Ù†Ú¯ ÙÙˆØ§Ø¦Ø¯ Ú©Ùˆ Ø¨Ú‘Ú¾Ø§Ù†Ø§\nPREDICTED: Ù†ÛŒÙ¹ ÙˆØ±Ú©Ù†Ú¯ ÙÙˆØ§Ø¦Ø¯ Ú©Ùˆ Ø¨Ú‘Ú¾Ø§Ù†Ø§\n\nGOLD: Ù†ÛŒÙ¹ ÙˆØ±Ú©Ù†Ú¯ ÙÙˆØ§Ø¦Ø¯ Ú©Ùˆ Ø¨Ú‘Ú¾Ø§Ù†Ø§\nPREDICTED: \n\nGOLD: ØªÙ‚Ø±ÛŒØ¨Ø§ 20 Ú¯Ú¾Ù†Ù¹ÙˆÚº\nPREDICTED: \n\nGOLD: ØªÙ‚Ø±ÛŒØ¨Ø§ 20 Ú¯Ú¾Ù†Ù¹ÙˆÚº\nPREDICTED: ØªÙ‚Ø±ÛŒØ¨Ø§ 20 Ú¯Ú¾Ù†Ù¹ÙˆÚº\n\nGOLD: ØªÙ‚Ø±ÛŒØ¨Ø§ 20 Ú¯Ú¾Ù†Ù¹ÙˆÚº\nPREDICTED: \n\nGOLD: ØªÙ‚Ø±ÛŒØ¨Ø§ 20 Ú¯Ú¾Ù†Ù¹ÙˆÚº\nPREDICTED: \n\nGOLD: Ø§Ø³Ú©Ø§Ù¹Ø´ Ø¢Ø²Ø§Ø¯ÛŒ\nPREDICTED: \n\nGOLD: Ø§Ø³Ú©Ø§Ù¹Ø´ Ø¢Ø²Ø§Ø¯ÛŒ\nPREDICTED: Ø§Ø³Ú©Ø§Ù¹Ø´ Ø¢Ø²Ø§Ø¯ÛŒ\n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: ÛŒÙ…Ø±Ú† Ø±Ø§Ø¦Ù† Ø¨Ø±Ø¬\nPREDICTED: \n\nGOLD: ÛŒÙ…Ø±Ú† Ø±Ø§Ø¦Ù† Ø¨Ø±Ø¬\nPREDICTED: ÛŒÙ…Ø±Ú† Ø±Ø§Ø¦Ù† Ø¨Ø±Ø¬\n\nGOLD: ÛŒÙ…Ø±Ú† Ø±Ø§Ø¦Ù† Ø¨Ø±Ø¬\nPREDICTED: \n\nGOLD: Ø¹Ø§Ù… Ø®Ø±Ø§Ø¨ÛŒ Ø§ÙˆØ± ÚˆÚ©Ù¹Ø§Ø¦Ù„ Ú©Ú¾ÛŒÙ†Ú†Ù†Û’ Ø§ÙˆØ± Ù¾ØªÙ„Û’ ÛÙˆÙ†Û’ Ú©Û’ Ø°Ø±ÛŒØ¹Û’\nPREDICTED: Ø¹Ø§Ù… Ø®Ø±Ø§Ø¨ÛŒ Ø§ÙˆØ± ÚˆÚ©Ù¹Ø§Ø¦Ù„ Ú©Ú¾ÛŒÙ†Ú†Ù†Û’ Ø§ÙˆØ± Ù¾ØªÙ„Û’ ÛÙˆÙ†Û’ Ú©Û’ Ø°Ø±ÛŒØ¹Û’\n\nGOLD: Ø¹Ø§Ù… Ø®Ø±Ø§Ø¨ÛŒ Ø§ÙˆØ± ÚˆÚ©Ù¹Ø§Ø¦Ù„ Ú©Ú¾ÛŒÙ†Ú†Ù†Û’ Ø§ÙˆØ± Ù¾ØªÙ„Û’ ÛÙˆÙ†Û’ Ú©Û’ Ø°Ø±ÛŒØ¹Û’\nPREDICTED: \n\nGOLD: Ø¹Ø§Ù… Ø®Ø±Ø§Ø¨ÛŒ Ø§ÙˆØ± ÚˆÚ©Ù¹Ø§Ø¦Ù„ Ú©Ú¾ÛŒÙ†Ú†Ù†Û’ Ø§ÙˆØ± Ù¾ØªÙ„Û’ ÛÙˆÙ†Û’ Ú©Û’ Ø°Ø±ÛŒØ¹Û’\nPREDICTED: \n\nGOLD: ÛØ§Ù„ÛŒÙ†Úˆ ØŒ Ù¾Ø±ÙˆØ³ÛŒØ§ Ø§ÙˆØ± Ø¬Ù†ÙˆØ¨ÛŒ Ø§ÙØ±ÛŒÙ‚Û\nPREDICTED: \n\nGOLD: ÛØ§Ù„ÛŒÙ†Úˆ ØŒ Ù¾Ø±ÙˆØ³ÛŒØ§ Ø§ÙˆØ± Ø¬Ù†ÙˆØ¨ÛŒ Ø§ÙØ±ÛŒÙ‚Û\nPREDICTED: ÛØ§Ù„ÛŒÙ†Úˆ ØŒ Ù¾Ø±ÙˆØ³ÛŒØ§ Ø§ÙˆØ± Ø¬Ù†ÙˆØ¨ÛŒ Ø§ÙØ±ÛŒÙ‚Û\n\nGOLD: ÛØ§Ù„ÛŒÙ†Úˆ ØŒ Ù¾Ø±ÙˆØ³ÛŒØ§ Ø§ÙˆØ± Ø¬Ù†ÙˆØ¨ÛŒ Ø§ÙØ±ÛŒÙ‚Û\nPREDICTED: \n\nGOLD: 2001\nPREDICTED: \n\nGOLD: 2001\nPREDICTED: 2001\n\nGOLD: 2001\nPREDICTED: \n\nGOLD: 2001\nPREDICTED: \n\nGOLD: Ú©ÛŒÙ†ÛŒÙ…ÛŒÙ¹Ú© Ù¾ÛŒÙ…Ø§Ø¦Ø´ÙˆÚº\nPREDICTED: Ú©ÛŒÙ†ÛŒÙ…ÛŒÙ¹Ú© Ù¾ÛŒÙ…Ø§Ø¦Ø´ÙˆÚº\n\nGOLD: Ú©ÛŒÙ†ÛŒÙ…ÛŒÙ¹Ú© Ù¾ÛŒÙ…Ø§Ø¦Ø´ÙˆÚº\nPREDICTED: \n\nGOLD: Ú©ÛŒÙ†ÛŒÙ…ÛŒÙ¹Ú© Ù¾ÛŒÙ…Ø§Ø¦Ø´ÙˆÚº\nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: Ø´Ù…Ø§Ù„ÛŒ Ø§Ù…Ø±ÛŒÚ©Û Ù…ÛŒÚº Ø¨Ø±Ø·Ø§Ù†ÙˆÛŒ Ù†Ø§Ú©Ø§Ù…ÛŒÙˆÚº\nPREDICTED: Ø´Ù…Ø§Ù„ÛŒ Ø§Ù…Ø±ÛŒÚ©Û Ù…ÛŒÚº Ø¨Ø±Ø·Ø§Ù†ÙˆÛŒ Ù†Ø§Ú©Ø§Ù…ÛŒÙˆÚº\n\nGOLD: Ø´Ù…Ø§Ù„ÛŒ Ø§Ù…Ø±ÛŒÚ©Û Ù…ÛŒÚº Ø¨Ø±Ø·Ø§Ù†ÙˆÛŒ Ù†Ø§Ú©Ø§Ù…ÛŒÙˆÚº\nPREDICTED: \n\nGOLD: Ø´Ù…Ø§Ù„ÛŒ Ø§Ù…Ø±ÛŒÚ©Û Ù…ÛŒÚº Ø¨Ø±Ø·Ø§Ù†ÙˆÛŒ Ù†Ø§Ú©Ø§Ù…ÛŒÙˆÚº\nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: Ø¬Ø§Ù† ÚˆØ¨Ù„ÛŒÙˆ ÙˆÛŒÚ©Ø³ Ø¨Ø±Ø¬\nPREDICTED: Ø¬Ø§Ù† ÚˆØ¨Ù„ÛŒÙˆ ÙˆÛŒÚ©Ø³ Ø¨Ø±Ø¬\n\nGOLD: Ø¬Ø§Ù† ÚˆØ¨Ù„ÛŒÙˆ ÙˆÛŒÚ©Ø³ Ø¨Ø±Ø¬\nPREDICTED: \n\nGOLD: Ú†Ú¾ Ø³Ø§Ù„ÙˆÚº\nPREDICTED: \n\nGOLD: Ú†Ú¾ Ø³Ø§Ù„ÙˆÚº\nPREDICTED: Ú†Ú¾ Ø³Ø§Ù„ÙˆÚº\n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: Ù¹ÛŒÙ…ÙˆÚ©ÙˆØ§ Ù„ÙˆÚ¯ÙˆÚº\nPREDICTED: Ù¹ÛŒÙ…ÙˆÚ©ÙˆØ§ Ù„ÙˆÚ¯ÙˆÚº\n\nGOLD: Ù¹ÛŒÙ…ÙˆÚ©ÙˆØ§ Ù„ÙˆÚ¯ÙˆÚº\nPREDICTED: \n\nGOLD: Ù¹ÛŒÙ…ÙˆÚ©ÙˆØ§ Ù„ÙˆÚ¯ÙˆÚº\nPREDICTED: \n\nGOLD: Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ù¹Ø³\nPREDICTED: Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ù¹Ø³\n\nGOLD: Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ù¹Ø³\nPREDICTED: \n\nGOLD: Ú©Ú†Ú¾ Ù…Ø³ØªØ­Ú©Ù… Ø¯ÙØ¹Ø§Øª\nPREDICTED: \n\nGOLD: Ú©Ú†Ú¾ Ù…Ø³ØªØ­Ú©Ù… Ø¯ÙØ¹Ø§Øª\nPREDICTED: Ú©Ú†Ú¾ Ù…Ø³ØªØ­Ú©Ù… Ø¯ÙØ¹Ø§Øª\n\nGOLD: Ù‚Ø§Ø¶ÛŒ\nPREDICTED: \n\nGOLD: Ù‚Ø§Ø¶ÛŒ\nPREDICTED: Ù‚Ø§Ø¶ÛŒ\n\nGOLD: Ù‚Ø§Ø¶ÛŒ\nPREDICTED: Ù‚Ø§Ø¶ÛŒ\n\nGOLD: ÙˆÛ Ù…Ø§Ø­ÙˆÙ„ Ø¬Ø³ Ù…ÛŒÚº ÙˆÛ Ø±ÛØªÛ’ ÛÛŒÚº\nPREDICTED: ÙˆÛ Ù…Ø§Ø­ÙˆÙ„ Ø¬Ø³ Ù…ÛŒÚº ÙˆÛ Ø±ÛØªÛ’ ÛÛŒÚº\n\nGOLD: ÙˆÛ Ù…Ø§Ø­ÙˆÙ„ Ø¬Ø³ Ù…ÛŒÚº ÙˆÛ Ø±ÛØªÛ’ ÛÛŒÚº\nPREDICTED: \n\nGOLD: ÙˆÛ Ù…Ø§Ø­ÙˆÙ„ Ø¬Ø³ Ù…ÛŒÚº ÙˆÛ Ø±ÛØªÛ’ ÛÛŒÚº\nPREDICTED: \n\nGOLD: Ø±ÙˆÙ…Ø§Ù†Ù¹Ú© Ø±Ø§Ø¦Ù†\nPREDICTED: \n\nGOLD: Ø±ÙˆÙ…Ø§Ù†Ù¹Ú© Ø±Ø§Ø¦Ù†\nPREDICTED: Ø±ÙˆÙ…Ø§Ù†Ù¹Ú© Ø±Ø§Ø¦Ù†\n\nGOLD: Ø§Ø¹Ù„ÛŒ Ø¹Ø¯Ù… Ù…Ø³Ø§ÙˆØ§Øª\nPREDICTED: \n\nGOLD: Ø§Ø¹Ù„ÛŒ Ø¹Ø¯Ù… Ù…Ø³Ø§ÙˆØ§Øª\nPREDICTED: Ø§Ø¹Ù„ÛŒ Ø¹Ø¯Ù… Ù…Ø³Ø§ÙˆØ§Øª\n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: n Ú©Û’ Ù…Ø±Ø¨Ø¹ Ø¬Ú‘\nPREDICTED: n Ú©Û’ Ù…Ø±Ø¨Ø¹ Ø¬Ú‘\n\nGOLD: n Ú©Û’ Ù…Ø±Ø¨Ø¹ Ø¬Ú‘\nPREDICTED: \n\nGOLD: n Ú©Û’ Ù…Ø±Ø¨Ø¹ Ø¬Ú‘\nPREDICTED: \n\nGOLD: n Ú©Û’ Ù…Ø±Ø¨Ø¹ Ø¬Ú‘\nPREDICTED: \n\nGOLD: ÙØ±Ø§Ù†Ø³\nPREDICTED: ÙØ±Ø§Ù†Ø³\n\nGOLD: ÙØ±Ø§Ù†Ø³\nPREDICTED: \n\nGOLD: ÙØ±Ø§Ù†Ø³\nPREDICTED: \n\nGOLD: ÙØ±Ø§Ù†Ø³\nPREDICTED: \n\nGOLD: ØªØ´Ø¯Ø¯\nPREDICTED: ØªØ´Ø¯Ø¯\n\nGOLD: ØªØ´Ø¯Ø¯\nPREDICTED: \n\nGOLD: ØªØ´Ø¯Ø¯\nPREDICTED: \n\nGOLD: Ù¹ÛŒÙ…ÛŒÚ©ÙˆÙ„Ø§ Ø§ÙˆØ± Ù…ÙˆØ±ÛŒÙ¹Ø§\nPREDICTED: Ù¹ÛŒÙ…ÛŒÚ©ÙˆÙ„Ø§ Ø§ÙˆØ± Ù…ÙˆØ±ÛŒÙ¹Ø§\n\nGOLD: Ù¹ÛŒÙ…ÛŒÚ©ÙˆÙ„Ø§ Ø§ÙˆØ± Ù…ÙˆØ±ÛŒÙ¹Ø§\nPREDICTED: \n\n   Validation accuracy: 55/100 (55.0%)\n   âŒ WARNING: Validation accuracy is low!\n================================================================================\n","output_type":"stream"}],"execution_count":199},{"id":"fb2600f9-5e66-462f-a6d6-198fde492516","cell_type":"markdown","source":"### !!! KEY TAKEAWAY FROM ABOVE CELLS!\n\nA lot of chunks do not have the answer in the chunked context, so (0, 0) -> `[CLS]` tok is being predicted!\nThis may be giving way to a lot of mispredictions in evaluation!","metadata":{}},{"id":"7f38b20f","cell_type":"code","source":"print(\"=\"*80)\nprint(\"ğŸ§ª TEST 4: Evaluation Functions Correctness\")\nprint(\"=\"*80)\n\n# Test the metric functions\nprint(\"\\n1ï¸âƒ£ Testing normalize_answer()...\")\ntest_cases = [\n    (\"Hello World\", \"hello world\"),\n    (\"The quick fox\", \"quick fox\"),\n    (\"Test!\", \"test\"),\n    (\"  spaces  \", \"spaces\"),\n]\n\nfor input_text, expected in test_cases:\n    result = normalize_answer(input_text)\n    status = \"âœ…\" if result == expected else \"âŒ\"\n    print(f\"   {status} normalize_answer('{input_text}') = '{result}' (expected: '{expected}')\")\n\n# Test exact_match_score\nprint(\"\\n2ï¸âƒ£ Testing exact_match_score()...\")\nem_tests = [\n    (\"hello\", \"hello\", 1.0),\n    (\"hello\", \"Hello\", 1.0),  # Case insensitive\n    (\"the answer\", \"answer\", 1.0),  # Articles removed\n    (\"hello\", \"world\", 0.0),\n    (\"\", \"\", 1.0),\n]\n\nfor pred, gold, expected in em_tests:\n    result = exact_match_score(pred, gold)\n    status = \"âœ…\" if result == expected else \"âŒ\"\n    print(f\"   {status} EM('{pred}', '{gold}') = {result} (expected: {expected})\")\n\n# Test f1_score\nprint(\"\\n3ï¸âƒ£ Testing f1_score()...\")\nf1_tests = [\n    (\"hello world\", \"hello world\", 1.0),\n    (\"hello\", \"world\", 0.0),\n    (\"hello world\", \"hello\", 0.67),  # Approximate\n    (\"\", \"\", 1.0),\n    (\"hello\", \"\", 0.0),\n]\n\nall_f1_ok = True\nfor pred, gold, expected in f1_tests:\n    result = f1_score(pred, gold)\n    # Allow small tolerance for floating point\n    ok = abs(result - expected) < 0.01 or (expected == 0 and result == 0)\n    status = \"âœ…\" if ok else \"âŒ\"\n    if not ok:\n        all_f1_ok = False\n    print(f\"   {status} F1('{pred}', '{gold}') = {result:.2f} (expected: ~{expected})\")\n\n# Test decode_prediction\nprint(\"\\n4ï¸âƒ£ Testing decode_prediction()...\")\nsample_ids = tokenizer.encode(\"This is a test answer\", add_special_tokens=True)\ncls_idx = sample_ids.index(tokenizer.cls_token_id)\n\ndecode_tests = [\n    (sample_ids, cls_idx, cls_idx, \"\"),  # No answer case\n    (sample_ids, 5, 3, \"\"),  # Invalid range (start > end)\n    (sample_ids, -1, 5, \"\"),  # Negative index\n    (sample_ids, 2, 5, \"non-empty\"),  # Valid range should return something\n]\n\nfor ids, start, end, expected_type in decode_tests:\n    result = decode_prediction(ids, start, end)\n    if expected_type == \"\":\n        ok = result == \"\"\n        status = \"âœ…\" if ok else \"âŒ\"\n        print(f\"   {status} decode_prediction(..., {start}, {end}) = '{result}' (expected empty)\")\n    else:\n        ok = len(result) > 0\n        status = \"âœ…\" if ok else \"âŒ\"\n        print(f\"   {status} decode_prediction(..., {start}, {end}) = '{result}' (expected non-empty)\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T10:06:56.999867Z","iopub.execute_input":"2025-12-12T10:06:57.000623Z","iopub.status.idle":"2025-12-12T10:06:57.011783Z","shell.execute_reply.started":"2025-12-12T10:06:57.000600Z","shell.execute_reply":"2025-12-12T10:06:57.011049Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ§ª TEST 4: Evaluation Functions Correctness\n================================================================================\n\n1ï¸âƒ£ Testing normalize_answer()...\n   âœ… normalize_answer('Hello World') = 'hello world' (expected: 'hello world')\n   âœ… normalize_answer('The quick fox') = 'quick fox' (expected: 'quick fox')\n   âœ… normalize_answer('Test!') = 'test' (expected: 'test')\n   âœ… normalize_answer('  spaces  ') = 'spaces' (expected: 'spaces')\n\n2ï¸âƒ£ Testing exact_match_score()...\n   âœ… EM('hello', 'hello') = 1.0 (expected: 1.0)\n   âœ… EM('hello', 'Hello') = 1.0 (expected: 1.0)\n   âœ… EM('the answer', 'answer') = 1.0 (expected: 1.0)\n   âœ… EM('hello', 'world') = 0.0 (expected: 0.0)\n   âœ… EM('', '') = 1.0 (expected: 1.0)\n\n3ï¸âƒ£ Testing f1_score()...\n   âœ… F1('hello world', 'hello world') = 1.00 (expected: ~1.0)\n   âœ… F1('hello', 'world') = 0.00 (expected: ~0.0)\n   âœ… F1('hello world', 'hello') = 0.67 (expected: ~0.67)\n   âœ… F1('', '') = 1.00 (expected: ~1.0)\n   âœ… F1('hello', '') = 0.00 (expected: ~0.0)\n\n4ï¸âƒ£ Testing decode_prediction()...\n   âœ… decode_prediction(..., 0, 0) = '' (expected empty)\n   âœ… decode_prediction(..., 5, 3) = '' (expected empty)\n   âœ… decode_prediction(..., -1, 5) = '' (expected empty)\n   âœ… decode_prediction(..., 2, 5) = 'his' (expected non-empty)\n\n================================================================================\n","output_type":"stream"}],"execution_count":200},{"id":"d93ea4be","cell_type":"code","source":"print(\"=\"*80)\nprint(\"ğŸ§ª TEST 5: Model Forward Pass (Sanity Check)\")\nprint(\"=\"*80)\n\n# Test that model can process a batch\nprint(\"\\n1ï¸âƒ£ Testing model forward pass...\")\n\ntry:\n    # Take a small batch\n    batch_size = 4\n    sample_batch = processed_train.select(range(batch_size))\n    \n    # Convert to tensors\n    input_ids = torch.tensor(sample_batch[\"input_ids\"]).to(device)\n    attention_mask = torch.tensor(sample_batch[\"attention_mask\"]).to(device)\n    token_type_ids = torch.tensor(sample_batch[\"token_type_ids\"]).to(device)\n    start_positions = torch.tensor(sample_batch[\"start_positions\"]).to(device)\n    end_positions = torch.tensor(sample_batch[\"end_positions\"]).to(device)\n    \n    print(f\"   Input shape: {input_ids.shape}\")\n    print(f\"   Attention mask shape: {attention_mask.shape}\")\n    print(f\"   Token type IDs shape: {token_type_ids.shape}\")\n    \n    # Forward pass\n    model.to(device)\n    model.eval()\n    \n    with torch.no_grad():\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            start_positions=start_positions,\n            end_positions=end_positions\n        )\n    \n    print(f\"\\n   âœ… Forward pass successful!\")\n    print(f\"   Loss: {outputs.loss.item():.4f}\")\n    print(f\"   Start logits shape: {outputs.start_logits.shape}\")\n    print(f\"   End logits shape: {outputs.end_logits.shape}\")\n    \n    # Check logits are valid\n    if torch.isnan(outputs.start_logits).any() or torch.isnan(outputs.end_logits).any():\n        print(f\"   âŒ WARNING: NaN values in logits!\")\n    else:\n        print(f\"   âœ… Logits are valid (no NaN)\")\n    \n    # Check loss is reasonable\n    if outputs.loss.item() < 0 or outputs.loss.item() > 100:\n        print(f\"   âš ï¸  WARNING: Loss seems unusual: {outputs.loss.item()}\")\n    else:\n        print(f\"   âœ… Loss is in reasonable range\")\n    \nexcept Exception as e:\n    print(f\"   âŒ CRITICAL ERROR during forward pass: {e}\")\n    import traceback\n    traceback.print_exc()\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T10:06:57.012670Z","iopub.execute_input":"2025-12-12T10:06:57.012996Z","iopub.status.idle":"2025-12-12T10:06:57.266186Z","shell.execute_reply.started":"2025-12-12T10:06:57.012979Z","shell.execute_reply":"2025-12-12T10:06:57.265356Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ§ª TEST 5: Model Forward Pass (Sanity Check)\n================================================================================\n\n1ï¸âƒ£ Testing model forward pass...\n   Input shape: torch.Size([4, 384])\n   Attention mask shape: torch.Size([4, 384])\n   Token type IDs shape: torch.Size([4, 384])\n\n   âœ… Forward pass successful!\n   Loss: 5.9048\n   Start logits shape: torch.Size([4, 384])\n   End logits shape: torch.Size([4, 384])\n   âœ… Logits are valid (no NaN)\n   âœ… Loss is in reasonable range\n\n================================================================================\n","output_type":"stream"}],"execution_count":201},{"id":"5cb1b468","cell_type":"code","source":"print(\"=\"*80)\nprint(\"ğŸ§ª TEST 6: Critical Boundary Cases\")\nprint(\"=\"*80)\n\n# Verify the fix for the <= vs < bug\nprint(\"\\n1ï¸âƒ£ Testing chunk boundary logic (the critical bug fix)...\")\n\nboundary_correct = 0\nboundary_total = 0\n\nfor proc_idx in range(min(1000, len(processed_train))):\n    proc_example = processed_train[proc_idx]\n    orig_idx = proc_example[\"overflow_to_sample_mapping\"]\n    orig_example = uqa_train[orig_idx]\n    \n    input_ids = proc_example[\"input_ids\"]\n    start_pos = proc_example[\"start_positions\"]\n    end_pos = proc_example[\"end_positions\"]\n    \n    cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n    \n    # Skip no-answer cases\n    if start_pos == cls_idx:\n        continue\n    \n    # Find context boundaries\n    sep_indices = [k for k, x in enumerate(input_ids) if x == tokenizer.sep_token_id]\n    if not sep_indices:\n        continue\n    \n    context_start = sep_indices[0] + 1\n    \n    # Check if answer is near end of context chunk (within last 5 positions)\n    # This is where the bug would manifest\n    if len(sep_indices) > 1:\n        context_end = sep_indices[1]\n    else:\n        # Find first padding token\n        context_end = next((i for i, x in enumerate(input_ids) if x == tokenizer.pad_token_id), len(input_ids))\n    \n    if context_end - end_pos <= 5:\n        boundary_total += 1\n        \n        # Verify extraction is correct\n        predicted = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True).strip()\n        gold = orig_example[\"answer\"].strip()\n        \n        if predicted.lower() == gold.lower():\n            boundary_correct += 1\n\nprint(f\"\\n   Found {boundary_total} boundary cases (answer near chunk end)\")\nif boundary_total > 0:\n    boundary_accuracy = boundary_correct / boundary_total * 100\n    print(f\"   Boundary cases correct: {boundary_correct}/{boundary_total} ({boundary_accuracy:.1f}%)\")\n    \n    if boundary_accuracy < 95:\n        print(f\"   âŒ WARNING: Boundary logic may still have issues!\")\n    else:\n        print(f\"   âœ… Boundary fix is working correctly!\")\nelse:\n    print(f\"   âš ï¸  No boundary cases found in first 1000 examples\")\n\n# Test the specific case from verification script\nprint(\"\\n2ï¸âƒ£ Testing the exact bug scenario...\")\n# Answer [90, 99] inclusive, Chunk [0, 100) exclusive\ntest_start = 90\ntest_end = 99  # inclusive\nchunk_start = 0\nchunk_end = 100  # exclusive\n\n# Correct logic (what we implemented)\ncorrect_result = test_start >= chunk_start and test_end < chunk_end\n# Buggy logic (what we fixed)\nbuggy_result = test_start >= chunk_start and test_end <= chunk_end\n\nprint(f\"   Scenario: answer=[{test_start},{test_end}], chunk=[{chunk_start},{chunk_end})\")\nprint(f\"   Correct logic (< for end): {correct_result}\")\nprint(f\"   Buggy logic (<= for end): {buggy_result}\")\n\nif correct_result == True and buggy_result == True:\n    print(f\"   âœ… Both agree when answer is inside chunk\")\nelif correct_result != buggy_result:\n    print(f\"   âš ï¸  Logics differ - this is where the bug would cause mislabeling\")\n\n# Now test the failing case\ntest_end = 100  # Now extends beyond\ncorrect_result = test_start >= chunk_start and test_end < chunk_end\nbuggy_result = test_start >= chunk_start and test_end <= chunk_end\n\nprint(f\"\\n   Scenario: answer=[{test_start},{test_end}], chunk=[{chunk_start},{chunk_end})\")\nprint(f\"   Correct logic (< for end): {correct_result} âœ…\")\nprint(f\"   Buggy logic (<= for end): {buggy_result} âŒ\")\n\nif correct_result == False and buggy_result == True:\n    print(f\"   âœ… Fix verified: correct logic rejects, buggy logic accepts (WRONG)\")\nelse:\n    print(f\"   âŒ Something is wrong with the logic\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T10:06:57.267018Z","iopub.execute_input":"2025-12-12T10:06:57.267331Z","iopub.status.idle":"2025-12-12T10:06:58.310161Z","shell.execute_reply.started":"2025-12-12T10:06:57.267313Z","shell.execute_reply":"2025-12-12T10:06:58.309374Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ§ª TEST 6: Critical Boundary Cases\n================================================================================\n\n1ï¸âƒ£ Testing chunk boundary logic (the critical bug fix)...\n\n   Found 2 boundary cases (answer near chunk end)\n   Boundary cases correct: 2/2 (100.0%)\n   âœ… Boundary fix is working correctly!\n\n2ï¸âƒ£ Testing the exact bug scenario...\n   Scenario: answer=[90,99], chunk=[0,100)\n   Correct logic (< for end): True\n   Buggy logic (<= for end): True\n   âœ… Both agree when answer is inside chunk\n\n   Scenario: answer=[90,100], chunk=[0,100)\n   Correct logic (< for end): False âœ…\n   Buggy logic (<= for end): True âŒ\n   âœ… Fix verified: correct logic rejects, buggy logic accepts (WRONG)\n\n================================================================================\n","output_type":"stream"}],"execution_count":202},{"id":"37f1f131","cell_type":"markdown","source":"---\n\n## ğŸ”¬ COMPREHENSIVE QA PIPELINE VERIFICATION\n\nBefore training, let's verify **every single component** of the QA pipeline end-to-end.","metadata":{}},{"id":"16","cell_type":"code","source":"# processed_train","metadata":{"execution":{"iopub.status.busy":"2025-12-12T10:06:58.311014Z","iopub.execute_input":"2025-12-12T10:06:58.311351Z","iopub.status.idle":"2025-12-12T10:06:58.314772Z","shell.execute_reply.started":"2025-12-12T10:06:58.311323Z","shell.execute_reply":"2025-12-12T10:06:58.314081Z"},"id":"D-emFQTIaZRL","trusted":true},"outputs":[],"execution_count":203},{"id":"19","cell_type":"code","source":"# processed_val","metadata":{"execution":{"iopub.status.busy":"2025-12-12T10:06:58.315535Z","iopub.execute_input":"2025-12-12T10:06:58.315754Z","iopub.status.idle":"2025-12-12T10:06:58.328959Z","shell.execute_reply.started":"2025-12-12T10:06:58.315730Z","shell.execute_reply":"2025-12-12T10:06:58.328231Z"},"id":"Yy3SiWwCabEi","trusted":true},"outputs":[],"execution_count":204},{"id":"20","cell_type":"code","source":"# Save newly processed data (OPTIONAL - for future reuse with same filtered dataset)\nprocessed_train.save_to_disk(\"/kaggle/working/cache/processed_train_uqa_filtered\")\nprocessed_val.save_to_disk(\"/kaggle/working/cache/processed_val_uqa_filtered\")\n\n# # âŒ DO NOT load old cache - it has index mismatches with filtered data!\n# # If you've already run the preprocessing cell above, skip this cell\n\nprocessed_train = load_from_disk(\"/kaggle/working/cache/processed_train_uqa_filtered\")\nprocessed_val = load_from_disk(\"/kaggle/working/cache/processed_val_uqa_filtered\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["05d936e2dc9d412b8637c174a3c0be64","7e0b41aa16f241a4ba3bb8a2f3525984","2bebe7e1f3f341dfaabf29963d2c5995","41b300b02ed2413ba80865aaa99ece2a","b4740a7137e742d687e2075b60d2be8a","80132c8e4fa743fca850936ecfebc7f7","303bb3d75f7d4e94aeb60c5491ea6e61","d7463faafecf4e46a87dc6863a646cea","bc199cddba714aeda650d97fef015a14","1a508a6457bb460ba17d5adb0a9e9f85","3aac2656907a416291a622717ccaf929","fa1af70d9c95443c9f09666359ba3769","ddb717fd4dbc40c6bd8422a02f925060","6d1342eeaf4f4f0489fe0746ceaaeb09","a10683e5c1164f349cbdc75b1567994c","71cfe2c8df474badb255d7d28da04348","077fbb403e5f4069841e558a3cc0c065","b068b9fac9f24eca9bd430bab30ea70c","8cbfc6f4ec434674ac59d3fbdbddcd3b","4d675a6788b641c6a09604ef17514dec","bd9b9f21be9744c49d99ac4bc76f11e1","89469ab4bd6f48d8b9aa369473c7230f"]},"execution":{"iopub.status.busy":"2025-12-12T10:06:58.330383Z","iopub.execute_input":"2025-12-12T10:06:58.330646Z","iopub.status.idle":"2025-12-12T10:06:58.590018Z","shell.execute_reply.started":"2025-12-12T10:06:58.330630Z","shell.execute_reply":"2025-12-12T10:06:58.589419Z"},"id":"77ecdd17","outputId":"602e648b-4a75-424b-da09-d58f3295a65e","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/58369 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48176e29aaa34f7f9d566915debd74ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/15786 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"353b0923dc5441a3a980a42f39c75922"}},"metadata":{}}],"execution_count":205},{"id":"21","cell_type":"code","source":"def print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2025-12-12T10:06:58.590868Z","iopub.execute_input":"2025-12-12T10:06:58.591151Z","iopub.status.idle":"2025-12-12T10:06:58.595536Z","shell.execute_reply.started":"2025-12-12T10:06:58.591132Z","shell.execute_reply":"2025-12-12T10:06:58.594931Z"},"id":"c0e06e6b","trusted":true},"outputs":[],"execution_count":206},{"id":"22","cell_type":"code","source":"# build LoRA model\n\npeft_model = get_peft_model(model, lora_config)\npeft_model.gradient_checkpointing_enable()\nprint_trainable_parameters(peft_model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-12T10:06:58.596290Z","iopub.execute_input":"2025-12-12T10:06:58.596582Z","iopub.status.idle":"2025-12-12T10:06:58.651272Z","shell.execute_reply.started":"2025-12-12T10:06:58.596564Z","shell.execute_reply":"2025-12-12T10:06:58.650699Z"},"id":"ba9eeeed","outputId":"27071b6e-b703-4b47-9288-9a1c6f3eba55","trusted":true},"outputs":[{"name":"stdout","text":"trainable params: 345602 || all params: 132430084 || trainable%: 0.26096940329661045\n","output_type":"stream"}],"execution_count":207},{"id":"23","cell_type":"code","source":"# Show what the model sees during training\nprint(\"=\"*80)\nprint(\"ğŸ“ MODEL TRAINING DATA FLOW\")\nprint(\"=\"*80)\n\n# Take one batch from preprocessed data\nbatch_size = 4\nsample_batch = processed_train.select(range(batch_size))\n\nprint(f\"\\n1ï¸âƒ£ BATCH STRUCTURE\")\nprint(\"-\"*80)\nprint(f\"Batch size: {batch_size} chunks\")\nprint(f\"Each chunk in the batch contains:\")\n\n# Show batch structure\nfor key in sample_batch.column_names:\n    sample_value = sample_batch[0][key]\n    if isinstance(sample_value, list):\n        print(f\"  - {key}: shape ({batch_size}, {len(sample_value)})\")\n    else:\n        print(f\"  - {key}: shape ({batch_size},)\")\n\nprint(f\"\\n2ï¸âƒ£ WHAT THE MODEL RECEIVES (for 1 chunk in batch)\")\nprint(\"-\"*80)\nexample_idx = 0\nprint(f\"Input IDs: {len(sample_batch[example_idx]['input_ids'])} tokens\")\nprint(f\"  First 10 token IDs: {sample_batch[example_idx]['input_ids'][:10]}\")\nprint(f\"\\nAttention mask: {sample_batch[example_idx]['attention_mask'][:20]}...\")\nprint(f\"  (1=attend to token, 0=ignore padding)\")\nprint(f\"\\nToken type IDs: {sample_batch[example_idx]['token_type_ids'][:20]}...\")\nprint(f\"  (0=question tokens, 1=context tokens)\")\n\nprint(f\"\\n3ï¸âƒ£ TRAINING TARGETS (what model learns to predict)\")\nprint(\"-\"*80)\nprint(f\"Target start position: {sample_batch[example_idx]['start_positions']}\")\nprint(f\"Target end position: {sample_batch[example_idx]['end_positions']}\")\nprint(f\"\\nğŸ’¡ The model learns to output these exact positions!\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"execution":{"iopub.status.busy":"2025-12-12T10:06:58.652011Z","iopub.execute_input":"2025-12-12T10:06:58.652256Z","iopub.status.idle":"2025-12-12T10:06:58.675227Z","shell.execute_reply.started":"2025-12-12T10:06:58.652234Z","shell.execute_reply":"2025-12-12T10:06:58.674612Z"},"trusted":true},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ“ MODEL TRAINING DATA FLOW\n================================================================================\n\n1ï¸âƒ£ BATCH STRUCTURE\n--------------------------------------------------------------------------------\nBatch size: 4 chunks\nEach chunk in the batch contains:\n  - input_ids: shape (4, 384)\n  - attention_mask: shape (4, 384)\n  - token_type_ids: shape (4, 384)\n  - start_positions: shape (4,)\n  - end_positions: shape (4,)\n  - overflow_to_sample_mapping: shape (4,)\n\n2ï¸âƒ£ WHAT THE MODEL RECEIVES (for 1 chunk in batch)\n--------------------------------------------------------------------------------\nInput IDs: 384 tokens\n  First 10 token IDs: [57344, 1580, 1583, 1740, 1583, 32, 1729, 1608, 1575, 1574]\n\nAttention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]...\n  (1=attend to token, 0=ignore padding)\n\nToken type IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...\n  (0=question tokens, 1=context tokens)\n\n3ï¸âƒ£ TRAINING TARGETS (what model learns to predict)\n--------------------------------------------------------------------------------\nTarget start position: 0\nTarget end position: 0\n\nğŸ’¡ The model learns to output these exact positions!\n\n================================================================================\n","output_type":"stream"}],"execution_count":208},{"id":"5a98237c","cell_type":"markdown","source":"---","metadata":{}},{"id":"24","cell_type":"markdown","source":"## Model Training:\n","metadata":{}},{"id":"26","cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"outputs/canine-s-uqa-filtered\",\n\n    per_device_train_batch_size=4,  # increased train_batch_size from tydiqa\n    per_device_eval_batch_size=16,\n\n    gradient_accumulation_steps=4,  # decreased grad accum from tydiqa\n    gradient_checkpointing=True,\n\n    num_train_epochs=1, # same as tydiqa\n    learning_rate=3e-5,  \n    weight_decay=0.01,\n    \n    eval_strategy=\"no\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=1000,  # increased to 1000\n    logging_steps=50,\n    \n    fp16=True,\n    bf16=False,\n    report_to=\"none\",\n    push_to_hub=True,\n    hub_model_id=\"VohraAK/canine-s-uqa-filtered\",\n    hub_strategy=\"checkpoint\",\n    )\n\n# CustomEvalCallback - EXACT TyDiQA approach\nclass CustomEvalCallback(TrainerCallback):\n    def __init__(self, eval_func, eval_dataset):\n        self.eval_func = eval_func\n        self.eval_dataset = eval_dataset\n\n    def on_save(self, args, state, control, model=None, **kwargs):\n        \"\"\"\n        Runs AFTER checkpoint is saved.\n        Loads checkpoint from disk and evaluates it.\n        \"\"\"\n        checkpoint_path = f\"{args.output_dir}/checkpoint-{state.global_step}\"\n        print(f\"\\nğŸ” Running custom evaluation at step {state.global_step}...\")\n\n        # Call evaluation function (loads from checkpoint)\n        metrics = self.eval_func(checkpoint_path)\n\n        # Add metrics to state's log_history\n        state.log_history.append({\n            \"step\": state.global_step,\n            \"eval_exact_match\": metrics[\"exact_match\"],\n            \"eval_f1\": metrics[\"f1\"],\n            \"eval_edit_distance\": metrics[\"edit_distance\"],\n        })\n\n        # Print metrics\n        print(f\"âœ… Step {state.global_step}: EM={metrics['exact_match']*100:.2f}, F1={metrics['f1']*100:.2f}, EditDist={metrics['edit_distance']*100:.2f}\")\n\n        # Re-save trainer_state.json with updated metrics\n        state_path = f\"{checkpoint_path}/trainer_state.json\"\n        try:\n            with open(state_path, 'r') as f:\n                state_dict = json.load(f)\n            state_dict['log_history'] = state.log_history\n            with open(state_path, 'w') as f:\n                json.dump(state_dict, f, indent=2)\n            print(f\"ğŸ’¾ Updated trainer_state.json with custom metrics\")\n        except Exception as e:\n            print(f\"âš ï¸  Warning: Could not update trainer_state.json: {e}\")\n\n        # Push to Hub\n        try:\n            print(f\"â˜ï¸  Pushing checkpoint-{state.global_step} to Hub...\")\n            api = HfApi()\n            api.upload_folder(\n                folder_path=checkpoint_path,\n                repo_id=args.hub_model_id,\n                path_in_repo=f\"checkpoint-{state.global_step}\",\n                commit_message=f\"Add checkpoint {state.global_step} (EM={metrics['exact_match']*100:.1f}%, F1={metrics['f1']*100:.1f}%)\",\n                repo_type=\"model\"\n            )\n            print(f\"âœ… Pushed checkpoint-{state.global_step} to Hub\")\n        except Exception as e:\n            print(f\"âš ï¸  Warning: Could not push to Hub: {e}\")\n\n        return control\n\n","metadata":{"execution":{"iopub.status.busy":"2025-12-12T10:06:58.676032Z","iopub.execute_input":"2025-12-12T10:06:58.676301Z","iopub.status.idle":"2025-12-12T10:06:58.708805Z","shell.execute_reply.started":"2025-12-12T10:06:58.676279Z","shell.execute_reply":"2025-12-12T10:06:58.708240Z"},"id":"c4abaaab","trusted":true},"outputs":[],"execution_count":209},{"id":"27","cell_type":"code","source":"trainer_cb = CustomEvalCallback(evaluate_checkpoint, processed_val)\n\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=processed_train,\n    eval_dataset=processed_val,\n    callbacks=[trainer_cb],\n)","metadata":{"execution":{"iopub.status.busy":"2025-12-12T10:06:58.709506Z","iopub.execute_input":"2025-12-12T10:06:58.709766Z","iopub.status.idle":"2025-12-12T10:06:58.770829Z","shell.execute_reply.started":"2025-12-12T10:06:58.709748Z","shell.execute_reply":"2025-12-12T10:06:58.770286Z"},"id":"055f5dda","trusted":true},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":210},{"id":"28","cell_type":"code","source":"trainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.status.busy":"2025-12-12T10:06:58.771544Z","iopub.execute_input":"2025-12-12T10:06:58.771829Z"},"id":"TOUimesUX5Re","outputId":"cfa62dcd-8eb4-475a-910b-1c38a3894cc2","trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3001' max='3649' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3001/3649 23:25 < 05:03, 2.13 it/s, Epoch 0.82/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>5.939700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>5.894100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>5.843800</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>5.809200</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>5.752200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>5.720200</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>5.680400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>5.648900</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>5.599100</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>5.580700</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>5.561900</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>5.511800</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>5.482300</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>5.445800</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>5.388300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>5.386800</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>5.377100</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>5.339100</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>5.313000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>5.289900</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>5.276200</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>5.230000</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>5.216800</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>5.196900</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>5.158100</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>5.168700</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>5.176900</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>5.085900</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>5.076600</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>5.101100</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>5.081500</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>5.037900</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>4.988100</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>4.973900</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>4.972400</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>4.994600</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>4.941400</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>4.896400</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>4.915900</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>4.903600</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>4.862800</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>4.843700</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>4.854200</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>4.871300</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>4.891200</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>4.883700</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>4.786100</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>4.812200</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>4.738900</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>4.815400</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>4.804300</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>4.722100</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>4.840400</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>4.737100</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>4.790400</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>4.796800</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>4.762200</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>4.714200</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>4.731900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 1000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ§ª Evaluating checkpoint: outputs/canine-s-uqa-filtered/checkpoint-1000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/15786 [00:00<?, ?samples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf815909fedd4b12a931b30c2ec95e69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ” Debug: Sample predictions:\n  Pred: '' | Gold: 'Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û'\n  Pred: 'ARPANET Ø³Ø§Ø¯Û Ø§Ù„ÙØ§Ø¸ Ù…ÛŒÚº Ú©ÛŒØ§ Ú©Ø±ØªØ§ ÛÛ’ØŸ1965 Ù…ÛŒÚº Ø´Ø±ÙˆØ¹ Ú©' | Gold: ''\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: 'ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û'\n  Pred: '' | Gold: ''\nExamples evaluated: 5000\nExact Match: 25.98\nF1: 26.62\nEdit Distance (normalized): 27.98\nâœ… Step 1000: EM=25.98, F1=26.62, EditDist=27.98\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-1000 to Hub...\nâœ… Pushed checkpoint-1000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 2000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ§ª Evaluating checkpoint: outputs/canine-s-uqa-filtered/checkpoint-2000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/15786 [00:00<?, ?samples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"211e78c5cf3a4ea4be6328f96dd44255"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nğŸ” Debug: Sample predictions:\n  Pred: '' | Gold: 'Ù…ØºØ±Ø¨ÛŒ Ú©ÛŒÙ¾ ØµÙˆØ¨Û'\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: 'ÛÛŒÙˆÚ¯Ø³ Ù…ÙØ±ÙˆØ¶Û'\n  Pred: '' | Gold: ''\nExamples evaluated: 5000\nExact Match: 28.80\nF1: 29.28\nEdit Distance (normalized): 30.21\nâœ… Step 2000: EM=28.80, F1=29.28, EditDist=30.21\nğŸ’¾ Updated trainer_state.json with custom metrics\nâ˜ï¸  Pushing checkpoint-2000 to Hub...\nâœ… Pushed checkpoint-2000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ” Running custom evaluation at step 3000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ§ª Evaluating checkpoint: outputs/canine-s-uqa-filtered/checkpoint-3000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/15786 [00:00<?, ?samples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"820da7b044dc45b9ae32587d56bdd8f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10407' max='15786' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10407/15786 04:38 < 02:24, 37.31 it/s]\n    </div>\n    "},"metadata":{}}],"execution_count":null},{"id":"29","cell_type":"markdown","source":"---","metadata":{}},{"id":"30","cell_type":"markdown","source":"### Diagnosing Preprocessing Functions!!!\n\nThese functions are just analysing the preprocessing logic above, they're just using the base model, NOT our trained model...","metadata":{"id":"cc44692c-6652-4cda-9ba4-8a03acdab88d"}},{"id":"31","cell_type":"code","source":"# # Diagnostic cell (fixed): Investigate preprocessing and truncation for many samples\n# import random\n# import pandas as pd\n# from transformers import AutoTokenizer\n\n# # Set display options to see full Urdu text\n# pd.set_option('display.max_colwidth', None)\n\n# try:\n#     tokenizer = AutoTokenizer.from_pretrained(\"google/canine-s\")\n# except Exception:\n#     tokenizer = None\n\n# num_samples = 20000  # Number of samples to check\n# results = []\n\n# for split_name, orig_data, proc_data in [\n#     (\"train\", uqa_train, processed_train),\n#     (\"val\", uqa_val, processed_val)\n# ]:\n#     # Sample random indices\n#     if len(proc_data) < num_samples:\n#         current_indices = range(len(proc_data))\n#     else:\n#         current_indices = random.sample(range(len(proc_data)), num_samples)\n\n#     for idx in current_indices:\n#         proc = proc_data[idx]\n#         # Use overflow_to_sample_mapping to get the correct original index\n#         orig_idx = proc[\"overflow_to_sample_mapping\"]\n#         orig = orig_data[orig_idx]\n\n#         input_ids = proc[\"input_ids\"]\n#         start_pos = proc[\"start_positions\"]\n#         end_pos = proc[\"end_positions\"]\n\n#         gold_answer = orig.get(\"gold_answer\", orig.get(\"answer\", \"\"))\n#         question = orig.get(\"question\", \"\")\n\n#         # Decode input_ids to text (for debugging context)\n#         if tokenizer:\n#             decoded_text = tokenizer.decode(input_ids, skip_special_tokens=False)\n#         else:\n#             decoded_text = str(input_ids)\n\n#         # Extract predicted answer span\n#         if 0 <= start_pos < len(input_ids) and 0 <= end_pos < len(input_ids):\n#             if tokenizer:\n#                 pred_span = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True)\n#             else:\n#                 pred_span = str(input_ids[start_pos:end_pos+1])\n#         else:\n#             pred_span = \"[CLS]\" # Represents no answer found in this chunk or invalid\n\n#         # Check if pred_span matches gold answer\n#         # We strip() to ignore minor whitespace differences\n#         pred_matches_gold = pred_span.strip() == gold_answer.strip()\n\n#         # Check if gold is even reachable in this chunk\n#         gold_in_decoded = gold_answer in decoded_text\n\n#         results.append({\n#             \"Split\": split_name,\n#             \"Question\": question,\n#             \"Gold Answer\": gold_answer,\n#             \"Extracted Answer\": pred_span,\n#             \"Match\": pred_matches_gold,\n#             \"Gold Reachable\": gold_in_decoded,\n#             \"orig_idx\": orig_idx\n#         })\n\n# # Create DataFrame\n# results_df = pd.DataFrame(results)\n\n# # --- SIDE BY SIDE COMPARISON ---\n\n# # 1. Filter for Solvable Mismatches (Gold was there, but we predicted wrong)\n# problem_cases = results_df[\n#     (results_df[\"Gold Reachable\"] == True) &\n#     (results_df[\"Match\"] == False)\n# ][[\"Question\", \"Gold Answer\", \"Extracted Answer\", \"Split\"]]\n\n# print(f\"ğŸ” Checked {len(results_df)} samples.\")\n# print(f\"âŒ Found {len(problem_cases)} cases where Gold was present but Extraction failed.\")\n\n# print(\"\\nğŸ“Š Side-by-Side Comparison (Top 20 Failures):\")\n# display(problem_cases.head(50))\n\n# print(\"\\nâœ… Side-by-Side Comparison (First 10 Rows - Mixed):\")\n# display(results_df[[\"Question\", \"Gold Answer\", \"Extracted Answer\", \"Match\"]].head(50))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"49f3717d","outputId":"38f435a4-1b55-4c2b-b6a5-86540fc23755","trusted":true},"outputs":[],"execution_count":null},{"id":"32","cell_type":"code","source":"# # Accuracy: fraction of rows where extracted answer matches gold answer\n# accuracy = (results_df[\"Match\"]).mean()\n\n# # Precision: among rows where extracted answer is non-empty, fraction that matches gold\n# # We filter out cases where the model predicted nothing (empty string) or just whitespace\n# non_empty_pred = results_df[\"Extracted Answer\"].str.strip() != \"\"\n\n# # Avoid division by zero if no predictions were made\n# if non_empty_pred.sum() > 0:\n#     precision = (results_df[\"Match\"] & non_empty_pred).sum() / non_empty_pred.sum()\n# else:\n#     precision = 0.0\n\n# print(f\"Accuracy: {accuracy:.3f}\")\n# print(f\"Precision: {precision:.3f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e67abc12","outputId":"c597ec41-a56e-4e5d-9eb6-e71bd0eafd38","trusted":true},"outputs":[],"execution_count":null}]}