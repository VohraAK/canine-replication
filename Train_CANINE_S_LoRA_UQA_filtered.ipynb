{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0","cell_type":"code","source":"# %pip install peft evaluate transformers Levenshtein ipywidgets\n# %pip install protobuf==3.20.3\n# !rm -rf /kaggle/working/cache\n# !rm -rf /kaggle/working/outputs","metadata":{"execution":{"iopub.status.busy":"2025-12-12T13:52:15.034974Z","iopub.execute_input":"2025-12-12T13:52:15.035568Z","iopub.status.idle":"2025-12-12T13:52:15.039057Z","shell.execute_reply.started":"2025-12-12T13:52:15.035543Z","shell.execute_reply":"2025-12-12T13:52:15.038290Z"},"id":"c186240c","trusted":true},"outputs":[],"execution_count":7},{"id":"1","cell_type":"code","source":"# X\n\nimport os\nos.environ[\"TRANSFORMERS_DISABLE_CHAT_TEMPLATES\"] = \"1\"\nos.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\nos.environ[\"TRANSFORMERS_NO_ADDITIONAL_CHAT_TEMPLATES\"] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2025-12-12T13:52:15.040331Z","iopub.execute_input":"2025-12-12T13:52:15.040641Z","iopub.status.idle":"2025-12-12T13:52:15.053754Z","shell.execute_reply.started":"2025-12-12T13:52:15.040606Z","shell.execute_reply":"2025-12-12T13:52:15.053021Z"},"id":"cd8da8ab","trusted":true},"outputs":[],"execution_count":8},{"id":"2","cell_type":"code","source":"import random\nfrom datasets import load_dataset, load_from_disk\nfrom transformers import CanineTokenizer\nfrom peft import LoraConfig, TaskType, get_peft_model\nimport re\nimport string\nfrom collections import Counter\nimport numpy as np\nimport Levenshtein\n\nfrom transformers import TrainingArguments, Trainer, TrainerCallback\nimport json\nfrom huggingface_hub import HfApi, notebook_login, whoami","metadata":{"execution":{"iopub.status.busy":"2025-12-12T13:52:15.097584Z","iopub.execute_input":"2025-12-12T13:52:15.098194Z","iopub.status.idle":"2025-12-12T13:52:15.103051Z","shell.execute_reply.started":"2025-12-12T13:52:15.098171Z","shell.execute_reply":"2025-12-12T13:52:15.102099Z"},"id":"d87eba82","trusted":true},"outputs":[],"execution_count":9},{"id":"3","cell_type":"code","source":"# notebook_login()\n# whoami()","metadata":{"execution":{"iopub.status.busy":"2025-12-12T13:52:15.104405Z","iopub.execute_input":"2025-12-12T13:52:15.104851Z","iopub.status.idle":"2025-12-12T13:52:15.123099Z","shell.execute_reply.started":"2025-12-12T13:52:15.104822Z","shell.execute_reply":"2025-12-12T13:52:15.122319Z"},"id":"0e98cebe-4c08-4850-b3c1-1529564fdb1b","trusted":true},"outputs":[],"execution_count":10},{"id":"4","cell_type":"code","source":"from transformers import CanineTokenizer, CanineForQuestionAnswering\nimport torch\nmodel_name = 'google/canine-s'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n\ntokenizer = CanineTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=False)\nmodel = CanineForQuestionAnswering.from_pretrained(model_name, trust_remote_code=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-12T13:52:15.123798Z","iopub.execute_input":"2025-12-12T13:52:15.124095Z","iopub.status.idle":"2025-12-12T13:52:20.760221Z","shell.execute_reply.started":"2025-12-12T13:52:15.124072Z","shell.execute_reply":"2025-12-12T13:52:20.759617Z"},"id":"f2dd5a40","outputId":"140c30ea-575d-45cd-ea54-7818cdfe6bf5","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/854 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc16395f1d054e9ead2923f517e7767f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/657 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e1d4e2fd86b4e4c9c475a05f61f7d74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/670 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90f2e729abd74b50921d425b5c8fd097"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/528M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a9aba94389a4e489589952732a6f646"}},"metadata":{}},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":11},{"id":"f75a7e9b","cell_type":"code","source":"# funtion to  filter out impossible questions\ndef filter_function(example):\n    return not example['is_impossible']","metadata":{"execution":{"iopub.status.busy":"2025-12-12T13:52:20.761114Z","iopub.execute_input":"2025-12-12T13:52:20.761392Z","iopub.status.idle":"2025-12-12T13:52:20.765408Z","shell.execute_reply.started":"2025-12-12T13:52:20.761365Z","shell.execute_reply":"2025-12-12T13:52:20.764637Z"},"trusted":true},"outputs":[],"execution_count":12},{"id":"5","cell_type":"code","source":"uqa_dataset = load_dataset(\"uqa/UQA\")\n\n# filtering\nuqa_dataset_filtered = uqa_dataset.filter(filter_function)\n\n# uqa_train = uqa_dataset_filtered[\"train\"].shuffle(seed=42)\n# uqa_val = uqa_dataset_filtered[\"validation\"].shuffle(seed=42)\n\n# trying the full dataset for now\nuqa_train = uqa_dataset[\"train\"].shuffle(seed=42).select(range(80000))\nuqa_val = uqa_dataset[\"validation\"].shuffle(seed=42).select(range(8000))\n\nprint(f\"üìä Dataset after filtering:\")\nprint(f\"   Original train size: {len(uqa_dataset['train']):,}\")\nprint(f\"   Filtered train size: {len(uqa_dataset_filtered['train']):,}\")\nprint(f\"   Using for training: {len(uqa_train):,}\")\nprint(f\"   Validation size: {len(uqa_val):,}\")","metadata":{"execution":{"iopub.status.busy":"2025-12-12T13:52:20.767382Z","iopub.execute_input":"2025-12-12T13:52:20.767619Z","iopub.status.idle":"2025-12-12T13:52:25.949008Z","shell.execute_reply.started":"2025-12-12T13:52:20.767606Z","shell.execute_reply":"2025-12-12T13:52:25.948202Z"},"id":"d474e2e8","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/898 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0849ad169ff84150bbf30b297a8c6c1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001-bac007e8ca7192(‚Ä¶):   0%|          | 0.00/30.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd1c2832bbf14795880870735e522f0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001-cf8a6960d(‚Ä¶):   0%|          | 0.00/2.92M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52b5accff7c14d7491496f7de2ca6a5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/124745 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be91a792d3a649f9bff90b2785715602"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/16824 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee6f9d4c047d4528840d7fbee0273160"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/124745 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68d360d0376f4289ae07ed118b891815"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/16824 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72bfc6d3d7da467d9666050c4ad73cbc"}},"metadata":{}},{"name":"stdout","text":"üìä Dataset after filtering:\n   Original train size: 124,745\n   Filtered train size: 83,018\n   Using for training: 80,000\n   Validation size: 8,000\n","output_type":"stream"}],"execution_count":13},{"id":"297e1ff7-52f6-4981-b360-45141788f2f4","cell_type":"code","source":"# Check character-token alignment\nex = uqa_train[444]\ncontext = ex[\"context\"]\ncontext_tokens = tokenizer.encode(ex[\"context\"], add_special_tokens=False)\n\nprint(f\"Context length (characters): {len(context)}\")\nprint(f\"Context length (tokens): {len(context_tokens)}\")\nprint(f\"1:1 mapping: {len(context) == len(context_tokens)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:52:25.949743Z","iopub.execute_input":"2025-12-12T13:52:25.949947Z","iopub.status.idle":"2025-12-12T13:52:25.956627Z","shell.execute_reply.started":"2025-12-12T13:52:25.949931Z","shell.execute_reply":"2025-12-12T13:52:25.955926Z"}},"outputs":[{"name":"stdout","text":"Context length (characters): 749\nContext length (tokens): 749\n1:1 mapping: True\n","output_type":"stream"}],"execution_count":14},{"id":"6","cell_type":"code","source":"# Explore raw UQA dataset structure\nprint(\"=\"*80)\nprint(\"UQA DATASET STRUCTURE\")\nprint(\"=\"*80)\nprint(f\"Training set size: {len(uqa_train):,} examples\")\nprint(f\"Validation set size: {len(uqa_val):,} examples\")\nprint(f\"\\nDataset columns: {uqa_train.column_names}\")\nprint(\"\\n\" + \"=\"*80)\n\n# Show a few examples\nprint(\"\\nüìù EXAMPLE 1 - Question with Answer\")\nprint(\"=\"*80)\nex1 = uqa_train[0]\nprint(f\"Question: {ex1['question']}\")\nprint(f\"\\nContext (first 300 chars): {ex1['context'][:300]}...\")\nprint(f\"\\nAnswer: '{ex1['answer']}'\")\nprint(f\"Answer starts at character position: {ex1['answer_start']}\")\n\n# Verify the answer extraction\nif ex1['answer_start'] != -1:\n    extracted = ex1['context'][ex1['answer_start']:ex1['answer_start']+len(ex1['answer'])]\n    print(f\"‚úì Extracted from context: '{extracted}'\")\n    print(f\"‚úì Match: {extracted == ex1['answer']}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nüìù EXAMPLE 2 - Another Question\")\nprint(\"=\"*80)\nex2 = uqa_train[100]\nprint(f\"Question: {ex2['question']}\")\nprint(f\"\\nContext length: {len(ex2['context'])} characters\")\nprint(f\"Answer: '{ex2['answer']}'\")\nprint(f\"Answer starts at position: {ex2['answer_start']}\")\n\n# Show answer in context\nif ex2['answer_start'] != -1:\n    start = max(0, ex2['answer_start'] - 50)\n    end = min(len(ex2['context']), ex2['answer_start'] + len(ex2['answer']) + 50)\n    context_snippet = ex2['context'][start:end]\n    answer_pos = ex2['answer_start'] - start\n    print(f\"\\nContext around answer:\")\n    print(f\"...{context_snippet}...\")\n    print(f\"    {' '*answer_pos}{'~'*len(ex2['answer'])} (answer here)\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nüìä DATASET STATISTICS\")\nprint(\"=\"*80)\n\n# Compute some basic statistics\nimport numpy as np\nquestion_lengths = [len(ex['question']) for ex in uqa_train.select(range(1000))]\ncontext_lengths = [len(ex['context']) for ex in uqa_train.select(range(1000))]\nanswer_lengths = [len(ex['answer']) if ex['answer'] else 0 for ex in uqa_train.select(range(1000))]\nhas_answer = [ex['answer_start'] != -1 for ex in uqa_train.select(range(1000))]\n\nprint(f\"Question length (chars): mean={np.mean(question_lengths):.1f}, max={np.max(question_lengths)}\")\nprint(f\"Context length (chars): mean={np.mean(context_lengths):.1f}, max={np.max(context_lengths)}\")\nprint(f\"Answer length (chars): mean={np.mean(answer_lengths):.1f}, max={np.max(answer_lengths)}\")\nprint(f\"Questions with answers: {sum(has_answer)/len(has_answer)*100:.1f}%\")\nprint(f\"Questions without answers: {(1-sum(has_answer)/len(has_answer))*100:.1f}%\")","metadata":{"execution":{"iopub.status.busy":"2025-12-12T13:52:25.957412Z","iopub.execute_input":"2025-12-12T13:52:25.957603Z","iopub.status.idle":"2025-12-12T13:52:26.363579Z","shell.execute_reply.started":"2025-12-12T13:52:25.957588Z","shell.execute_reply":"2025-12-12T13:52:26.362791Z"},"trusted":true},"outputs":[{"name":"stdout","text":"================================================================================\nUQA DATASET STRUCTURE\n================================================================================\nTraining set size: 80,000 examples\nValidation set size: 8,000 examples\n\nDataset columns: ['id', 'title', 'context', 'question', 'is_impossible', 'answer', 'answer_start']\n\n================================================================================\n\nüìù EXAMPLE 1 - Question with Answer\n================================================================================\nQuestion: ÿ¨ÿØ€åÿØ €ÅŸàÿßÿ¶€å ÿ¨€Åÿßÿ≤Ÿà⁄∫ ⁄©€å ÿ™ÿπŸÖ€åÿ± ŸÖ€å⁄∫ ⁄©€åÿß ÿßÿ≥ÿ™ÿπŸÖÿßŸÑ ⁄©€åÿß ÿ¨ÿßÿ™ÿß ÿ™⁄æÿßÿü\n\nContext (first 300 chars): 1906 ŸÖ€å⁄∫ ÿå ÿßŸÑŸÅÿ±€å⁄à ŸàŸêŸÑŸÖ ŸÜ€í ÿ®ÿßÿ±ÿ¥ ÿ≥€í ÿ≥ÿÆÿ™ €ÅŸàŸÜ€í ŸàÿßŸÑ€í ŸÖÿ±⁄©ÿ® ÿØÿ±€åÿßŸÅÿ™ ⁄©€å€í€î ÿ®ÿßÿ±ÿ¥ ÿ≥€í ÿ≥ÿÆÿ™ €ÅŸàŸÜ€í ŸàÿßŸÑ€í ŸÖÿ±⁄©ÿ® ÿå ÿ¨€åÿ≥€í ÿß€åŸÑŸàŸÖ€åŸÜ€åŸÖ ÿå Ÿπÿßÿ¶Ÿπ€åŸÜ€åŸÖ ÿßŸàÿ± ÿ™ÿßŸÜÿ®€í ⁄©€í ⁄©⁄Ü⁄æ ŸÖÿ±⁄©ÿ® ÿå ⁄Øÿ±ŸÖ€å ÿ≥€í ÿπŸÑÿßÿ¨ ⁄©ÿ±ŸÜ€í ŸàÿßŸÑ€í ŸÖÿ±⁄©ÿ® €Å€å⁄∫ ÿ¨Ÿà Ÿπ⁄æŸÜ⁄àÿß €ÅŸàŸÜ€í Ÿæÿ± ŸÜÿ±ŸÖ €ÅŸàÿ¨ÿßÿ™€í €Å€å⁄∫ (ÿ¨ŸÑÿØ€å ÿ≥€í Ÿπ⁄æŸÜ⁄àÿß €ÅŸàÿ¨ÿßÿ™€í €Å€å⁄∫) ÿå ÿßŸàÿ± Ÿæ⁄æÿ± ŸàŸÇÿ™ ⁄©€í ÿ≥ÿßÿ™⁄æ ÿ≥ÿÆÿ™ €ÅŸàÿ¨ÿßÿ™€í €Å€å⁄∫€î ÿß€åŸÑŸàŸÖ€åŸÜ€åŸÖ ÿå ÿ™ÿßŸÜÿ®€í ÿßŸàÿ± ŸÖ€å⁄ØŸÜ€å...\n\nAnswer: '⁄àŸàÿ±ÿßŸÑŸàŸÖ€åŸÜ'\nAnswer starts at character position: 503\n‚úì Extracted from context: '⁄àŸàÿ±ÿßŸÑŸàŸÖ€åŸÜ'\n‚úì Match: True\n\n================================================================================\n\nüìù EXAMPLE 2 - Another Question\n================================================================================\nQuestion: ⁄©ŸàŸÜ ÿ≥ÿß ŸÖŸÜ⁄Ø ÿ¥€Åÿ≤ÿßÿØ€Å ŸÜÿßŸÜÿ¨ŸÜ⁄Ø ŸÖ€å⁄∫ ÿ™ÿÆÿ™ ŸÜÿ¥€åŸÜ €ÅŸàÿß ÿ™⁄æÿßÿü\n\nContext length: 447 characters\nAnswer: 'ÿ¨Ÿà €åŸà ÿ≥ŸàŸÜ⁄Ø'\nAnswer starts at position: 262\n\nContext around answer:\n...ÿßŸÑ€å ⁄ÜŸÜ⁄Ø ÿÆÿßŸÜÿØÿßŸÜ ⁄©€í €Åÿßÿ™⁄æŸà⁄∫ ⁄Øÿ±ŸÜ€í ⁄©€í ÿ®ÿπÿØ ÿå ŸÖŸÜ⁄Ø ÿ¥€Åÿ≤ÿßÿØ€Å ÿ¨Ÿà €åŸà ÿ≥ŸàŸÜ⁄Ø ⁄©Ÿà ÿ¨ŸàŸÜ 1644 ŸÖ€å⁄∫ €ÅÿßŸÜ⁄Ø ⁄ØŸàÿßŸÜ⁄Ø ÿ¥€ÅŸÜÿ¥ÿß€Å ⁄©€í ÿ∑Ÿàÿ± Ÿæÿ± ŸÜÿßŸÜÿ¨ŸÜ...\n                                                      ~~~~~~~~~~ (answer here)\n\n================================================================================\n\nüìä DATASET STATISTICS\n================================================================================\nQuestion length (chars): mean=54.1, max=155\nContext length (chars): mean=685.5, max=3179\nAnswer length (chars): mean=11.5, max=142\nQuestions with answers: 64.9%\nQuestions without answers: 35.1%\n","output_type":"stream"}],"execution_count":15},{"id":"8","cell_type":"markdown","source":"---","metadata":{"id":"89c472d5"}},{"id":"9","cell_type":"markdown","source":"## Updated preprocessors!\n\nPreviously, we tried to apply the same approach we used in TYDIQA on UQA, the problem was the preprocessors were aligning the answer spans in units of **byte-level spans** instead of **character-level spans**. The calculations were adding byte-level offsets to the answer lengths, and since Urdu characters may be quantified in multiple bytes, the model was being fed the wrong spans -> GIGO!\n\nWe are now testing an updated preprocessor","metadata":{"id":"6e80a8d3"}},{"id":"10","cell_type":"code","source":"\"\"\"\nFIXED preprocessing function for UQA with CANINE-S.\nTyDiQA-style preprocessor adapted for UQA character offsets.\n\nKey fixes applied:\n1. Uses character-level offsets (UQA native format, no byte conversion needed)\n2. Fixed boundary check: uses `<` instead of `<=` for chunk_end\n3. Calculates gold_char_end as inclusive (answer_start + len(answer) - 1)\n4. Dynamic cls_index for no-answer cases\n5. Simplified context_offset calculation\n\nThis preprocessor passed all 200 real-world UQA examples in testing.\n\"\"\"\n\nMAX_SEQ_LENGTH = 384\nDOC_STRIDE = 64  # Using TyDiQA's value for proven results\n\ndef preprocess_uqa(examples, tokenizer, max_length=MAX_SEQ_LENGTH, doc_stride=DOC_STRIDE, model_obj=None, indices=None):\n    \"\"\"\n    TyDiQA-style preprocessor adapted for UQA (character offsets).\n    \n    Args:\n        examples: Batch with question, context, answer, answer_start fields\n        tokenizer: CanineTokenizer instance\n        max_length: Maximum sequence length (default 384)\n        doc_stride: Sliding window overlap (default 64)\n        model_obj: Optional model object (for compatibility)\n        indices: Optional example indices for overflow mapping\n    \n    Returns:\n        Dict with input_ids, attention_mask, token_type_ids, start_positions, \n        end_positions, overflow_to_sample_mapping\n    \"\"\"\n    questions = [q.strip() for q in examples[\"question\"]]\n    contexts = examples[\"context\"]\n    answers = examples[\"answer\"]\n    answer_starts = examples[\"answer_start\"]\n    \n    special_tokens = tokenizer.num_special_tokens_to_add(pair=True)\n    \n    encoded = {\n        \"input_ids\": [],\n        \"attention_mask\": [],\n        \"token_type_ids\": [],\n        \"start_positions\": [],\n        \"end_positions\": [],\n        \"overflow_to_sample_mapping\": [],\n    }\n    \n    for example_idx, (question, context, answer, answer_start) in enumerate(zip(questions, contexts, answers, answer_starts)):\n        question_tokens = tokenizer.encode(question, add_special_tokens=False)\n        context_tokens = tokenizer.encode(context, add_special_tokens=False)\n        \n        max_context_tokens = max_length - len(question_tokens) - special_tokens\n        if max_context_tokens <= 0 or not context_tokens:\n            continue\n        \n        # UQA uses character offsets (not bytes like TyDiQA)\n        if answer and answer_start != -1:\n            start_char = answer_start\n            end_char = answer_start + len(answer) - 1  # Inclusive\n            answer_span = (start_char, end_char)\n        else:\n            answer_span = None\n        \n        stride_tokens = max_context_tokens - doc_stride\n        if stride_tokens <= 0:\n            stride_tokens = max_context_tokens\n        \n        span_start = 0\n        context_length = len(context_tokens)\n        while span_start < context_length:\n            span_end = min(span_start + max_context_tokens, context_length)\n            context_chunk = context_tokens[span_start:span_end]\n            \n            input_ids = tokenizer.build_inputs_with_special_tokens(question_tokens, context_chunk)\n            token_type_ids = tokenizer.create_token_type_ids_from_sequences(question_tokens, context_chunk)\n            attention_mask = [1] * len(input_ids)\n            \n            cls_index = input_ids.index(tokenizer.cls_token_id)\n            context_offset = len(input_ids) - len(context_chunk) - 1\n            \n            if answer_span is None:\n                start_pos = cls_index\n                end_pos = cls_index\n            else:\n                start_char, end_char = answer_span\n                # CRITICAL FIX: Use < instead of <= for exclusive chunk_end\n                answer_in_chunk = start_char >= span_start and end_char < span_end\n                if answer_in_chunk:\n                    start_pos = context_offset + (start_char - span_start)\n                    end_pos = context_offset + (end_char - span_start)\n                else:\n                    start_pos = cls_index\n                    end_pos = cls_index\n            \n            padding = max_length - len(input_ids)\n            if padding > 0:\n                pad_id = tokenizer.pad_token_id\n                input_ids += [pad_id] * padding\n                attention_mask += [0] * padding\n                token_type_ids += [0] * padding\n            else:\n                input_ids = input_ids[:max_length]\n                attention_mask = attention_mask[:max_length]\n                token_type_ids = token_type_ids[:max_length]\n                if start_pos >= max_length or end_pos >= max_length:\n                    start_pos = cls_index\n                    end_pos = cls_index\n            \n            encoded[\"input_ids\"].append(input_ids)\n            encoded[\"attention_mask\"].append(attention_mask)\n            encoded[\"token_type_ids\"].append(token_type_ids)\n            encoded[\"start_positions\"].append(start_pos)\n            encoded[\"end_positions\"].append(end_pos)\n            encoded[\"overflow_to_sample_mapping\"].append(example_idx if indices is None else indices[example_idx])\n            \n            if span_end == context_length:\n                break\n            span_start += stride_tokens\n    \n    return encoded\n","metadata":{"execution":{"iopub.status.busy":"2025-12-12T13:52:26.364403Z","iopub.execute_input":"2025-12-12T13:52:26.364675Z","iopub.status.idle":"2025-12-12T13:52:26.378985Z","shell.execute_reply.started":"2025-12-12T13:52:26.364654Z","shell.execute_reply":"2025-12-12T13:52:26.378217Z"},"trusted":true},"outputs":[],"execution_count":16},{"id":"12","cell_type":"code","source":"# LoRA config\nlora_config = LoraConfig(\n    task_type=TaskType.QUESTION_ANS,\n    r=8,   # shadowing tydiqa for now\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"query\", \"value\"], # shadowing tydiqa for now\n    bias=\"none\",\n    modules_to_save=[\"qa_outputs\"],\n)\n\ndef print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-12-12T13:52:26.379876Z","iopub.execute_input":"2025-12-12T13:52:26.380135Z","iopub.status.idle":"2025-12-12T13:52:26.396805Z","shell.execute_reply.started":"2025-12-12T13:52:26.380114Z","shell.execute_reply":"2025-12-12T13:52:26.396102Z"},"id":"a3e95eec","trusted":true},"outputs":[],"execution_count":17},{"id":"0ce2da1b","cell_type":"markdown","source":"### Preprocessing examples...","metadata":{}},{"id":"13","cell_type":"code","source":"\nprint(\"=\"*80)\nprint(\"üî¨ PREPROCESSING WALKTHROUGH - Single Example\")\nprint(\"=\"*80)\n\n# Take one example\nexample = uqa_train[0]\nprint(f\"\\n1Ô∏è‚É£ ORIGINAL DATA\")\nprint(\"-\"*80)\nprint(f\"Question: {example['question']}\")\nprint(f\"Answer: '{example['answer']}'\")\nprint(f\"Answer position: {example['answer_start']}\")\nprint(f\"Context length: {len(example['context'])} characters\")\n\n# Preprocess it\nbatch = {\n    'question': [example['question']],\n    'context': [example['context']],\n    'answer': [example['answer']],\n    'answer_start': [example['answer_start']]\n}\nprocessed = preprocess_uqa(batch, tokenizer, indices=[0])\n\nprint(f\"\\n2Ô∏è‚É£ AFTER PREPROCESSING\")\nprint(\"-\"*80)\nprint(f\"Number of chunks created: {len(processed['input_ids'])}\")\nprint(f\"(Sliding window creates multiple chunks per example)\")\n\n# Show first chunk in detail\nchunk_idx = 0\nprint(f\"\\n3Ô∏è‚É£ CHUNK {chunk_idx} DETAILS\")\nprint(\"-\"*80)\nprint(f\"Input IDs length: {len(processed['input_ids'][chunk_idx])} tokens\")\nprint(f\"Start position: {processed['start_positions'][chunk_idx]}\")\nprint(f\"End position: {processed['end_positions'][chunk_idx]}\")\nprint(f\"Maps to original example: {processed['overflow_to_sample_mapping'][chunk_idx]}\")\n\n# Decode the inputs to show what the model sees\ninput_ids = processed['input_ids'][chunk_idx]\ndecoded_input = tokenizer.decode(input_ids, skip_special_tokens=False)\nprint(f\"\\n4Ô∏è‚É£ DECODED INPUT (first 400 chars, with special tokens)\")\nprint(\"-\"*80)\nprint(decoded_input[:400] + \"...\")\n\n# Decode the labeled answer span\nstart_pos = processed['start_positions'][chunk_idx]\nend_pos = processed['end_positions'][chunk_idx]\ncls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n\nif start_pos == cls_idx and end_pos == cls_idx:\n    labeled_answer = \"[NO ANSWER IN THIS CHUNK]\"\nelse:\n    labeled_answer = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True)\n\nprint(f\"\\n5Ô∏è‚É£ LABELED ANSWER SPAN IN THIS CHUNK\")\nprint(\"-\"*80)\nprint(f\"Gold answer: '{example['answer']}'\")\nprint(f\"Labeled span: '{labeled_answer}'\")\nprint(f\"Match: {labeled_answer.strip() == example['answer'].strip()}\")\n\n# Show all chunks for this example\nprint(f\"\\n6Ô∏è‚É£ ALL CHUNKS FOR THIS EXAMPLE\")\nprint(\"-\"*80)\nfor i in range(len(processed['input_ids'])):\n    start = processed['start_positions'][i]\n    end = processed['end_positions'][i]\n    if start == cls_idx and end == cls_idx:\n        chunk_answer = \"[NO ANSWER]\"\n    else:\n        chunk_answer = tokenizer.decode(processed['input_ids'][i][start:end+1], skip_special_tokens=True).strip()\n    has_answer = \"‚úÖ\" if chunk_answer == example['answer'].strip() else \"‚ùå\"\n    print(f\"  Chunk {i}: {has_answer} '{chunk_answer[:50]}'\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"execution":{"iopub.status.busy":"2025-12-12T13:52:26.397600Z","iopub.execute_input":"2025-12-12T13:52:26.397923Z","iopub.status.idle":"2025-12-12T13:52:26.418040Z","shell.execute_reply.started":"2025-12-12T13:52:26.397905Z","shell.execute_reply":"2025-12-12T13:52:26.417356Z"},"trusted":true},"outputs":[{"name":"stdout","text":"================================================================================\nüî¨ PREPROCESSING WALKTHROUGH - Single Example\n================================================================================\n\n1Ô∏è‚É£ ORIGINAL DATA\n--------------------------------------------------------------------------------\nQuestion: ÿ¨ÿØ€åÿØ €ÅŸàÿßÿ¶€å ÿ¨€Åÿßÿ≤Ÿà⁄∫ ⁄©€å ÿ™ÿπŸÖ€åÿ± ŸÖ€å⁄∫ ⁄©€åÿß ÿßÿ≥ÿ™ÿπŸÖÿßŸÑ ⁄©€åÿß ÿ¨ÿßÿ™ÿß ÿ™⁄æÿßÿü\nAnswer: '⁄àŸàÿ±ÿßŸÑŸàŸÖ€åŸÜ'\nAnswer position: 503\nContext length: 767 characters\n\n2Ô∏è‚É£ AFTER PREPROCESSING\n--------------------------------------------------------------------------------\nNumber of chunks created: 3\n(Sliding window creates multiple chunks per example)\n\n3Ô∏è‚É£ CHUNK 0 DETAILS\n--------------------------------------------------------------------------------\nInput IDs length: 384 tokens\nStart position: 0\nEnd position: 0\nMaps to original example: 0\n\n4Ô∏è‚É£ DECODED INPUT (first 400 chars, with special tokens)\n--------------------------------------------------------------------------------\nÓÄÄÿ¨ÿØ€åÿØ €ÅŸàÿßÿ¶€å ÿ¨€Åÿßÿ≤Ÿà⁄∫ ⁄©€å ÿ™ÿπŸÖ€åÿ± ŸÖ€å⁄∫ ⁄©€åÿß ÿßÿ≥ÿ™ÿπŸÖÿßŸÑ ⁄©€åÿß ÿ¨ÿßÿ™ÿß ÿ™⁄æÿßÿüÓÄÅ1906 ŸÖ€å⁄∫ ÿå ÿßŸÑŸÅÿ±€å⁄à ŸàŸêŸÑŸÖ ŸÜ€í ÿ®ÿßÿ±ÿ¥ ÿ≥€í ÿ≥ÿÆÿ™ €ÅŸàŸÜ€í ŸàÿßŸÑ€í ŸÖÿ±⁄©ÿ® ÿØÿ±€åÿßŸÅÿ™ ⁄©€å€í€î ÿ®ÿßÿ±ÿ¥ ÿ≥€í ÿ≥ÿÆÿ™ €ÅŸàŸÜ€í ŸàÿßŸÑ€í ŸÖÿ±⁄©ÿ® ÿå ÿ¨€åÿ≥€í ÿß€åŸÑŸàŸÖ€åŸÜ€åŸÖ ÿå Ÿπÿßÿ¶Ÿπ€åŸÜ€åŸÖ ÿßŸàÿ± ÿ™ÿßŸÜÿ®€í ⁄©€í ⁄©⁄Ü⁄æ ŸÖÿ±⁄©ÿ® ÿå ⁄Øÿ±ŸÖ€å ÿ≥€í ÿπŸÑÿßÿ¨ ⁄©ÿ±ŸÜ€í ŸàÿßŸÑ€í ŸÖÿ±⁄©ÿ® €Å€å⁄∫ ÿ¨Ÿà Ÿπ⁄æŸÜ⁄àÿß €ÅŸàŸÜ€í Ÿæÿ± ŸÜÿ±ŸÖ €ÅŸàÿ¨ÿßÿ™€í €Å€å⁄∫ (ÿ¨ŸÑÿØ€å ÿ≥€í Ÿπ⁄æŸÜ⁄àÿß €ÅŸàÿ¨ÿßÿ™€í €Å€å⁄∫) ÿå ÿßŸàÿ± Ÿæ⁄æÿ± ŸàŸÇÿ™ ⁄©€í ÿ≥ÿßÿ™⁄æ ÿ≥ÿÆÿ™ €ÅŸàÿ¨ÿßÿ™€í €Å€å⁄∫€î ÿß€åŸÑŸàŸÖ€åŸÜ€åŸÖ ÿå ÿ™ÿßŸÜÿ®€í ÿßŸàÿ± ŸÖ€å⁄ØŸÜ€åÿ¥€åŸÖ ⁄©€í Ÿπÿ±ŸÜÿ±€å ŸÖÿ±⁄©ÿ® ⁄©Ÿà Ÿπ⁄æŸÜ⁄àÓÄÅ...\n\n5Ô∏è‚É£ LABELED ANSWER SPAN IN THIS CHUNK\n--------------------------------------------------------------------------------\nGold answer: '⁄àŸàÿ±ÿßŸÑŸàŸÖ€åŸÜ'\nLabeled span: '[NO ANSWER IN THIS CHUNK]'\nMatch: False\n\n6Ô∏è‚É£ ALL CHUNKS FOR THIS EXAMPLE\n--------------------------------------------------------------------------------\n  Chunk 0: ‚ùå '[NO ANSWER]'\n  Chunk 1: ‚úÖ '⁄àŸàÿ±ÿßŸÑŸàŸÖ€åŸÜ'\n  Chunk 2: ‚ùå '[NO ANSWER]'\n\n================================================================================\n","output_type":"stream"}],"execution_count":18},{"id":"14","cell_type":"markdown","source":"## üîß Preprocessing Exploration: Raw Data ‚Üí Model Input\n\nNow let's see what happens during preprocessing - how we convert text to token IDs and create training labels.","metadata":{}},{"id":"25","cell_type":"code","source":"def normalize_answer(text):\n    text = (text or \"\").lower()\n    def remove_articles(s):\n        return re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n    def remove_punctuation(s):\n        return \"\".join(ch for ch in s if ch not in string.punctuation)\n    def white_space_fix(s):\n        return \" \".join(s.split())\n    return white_space_fix(remove_articles(remove_punctuation(text)))\n\ndef exact_match_score(prediction, ground_truth):\n    return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n\ndef f1_score(prediction, ground_truth):\n    pred_tokens = normalize_answer(prediction).split()\n    gold_tokens = normalize_answer(ground_truth).split()\n    if not gold_tokens:\n        return 1.0 if not pred_tokens else 0.0\n    if not pred_tokens:\n        return 0.0\n    common = Counter(pred_tokens) & Counter(gold_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0.0\n    precision = num_same / len(pred_tokens)\n    recall = num_same / len(gold_tokens)\n    # BUGFIX: Prevent division by zero if both precision and recall are 0\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n\ndef decode_prediction(input_ids, start_idx, end_idx):\n\n    global tokenizer\n    \n    # Dynamic CLS handling\n    cls_index = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n    \n    # No answer case (both point to CLS)\n    if start_idx == cls_index and end_idx == cls_index:\n        return \"\"\n    \n    # Invalid range (start after end) - treat as no answer\n    if start_idx > end_idx:\n        return \"\"\n    \n    # Defensive bounds checking\n    if start_idx < 0 or end_idx < 0:\n        return \"\"\n    if start_idx >= len(input_ids) or end_idx >= len(input_ids):\n        return \"\"\n    \n    # Clamp to valid range (additional safety)\n    start_idx = max(start_idx, 0)\n    end_idx = min(end_idx, len(input_ids) - 1)\n    \n    # Decode with inclusive slicing [start:end+1]\n    text = tokenizer.decode(input_ids[start_idx:end_idx + 1], skip_special_tokens=True)\n    return text.strip()\n\ndef gold_answer(example):\n    if example[\"answer_start\"] == -1:\n        return \"\"\n    return example[\"answer\"]\n\ndef edit_distance_score(prediction, ground_truth):\n    return Levenshtein.ratio(normalize_answer(prediction), normalize_answer(ground_truth))\n\n\n#--- CHANGED TO MATCH TYDIQA APPROACH\ndef evaluate_checkpoint(checkpoint_path=None):\n    \"\"\"\n    EXACT REPLICA of TyDiQA evaluation approach.\n    Loads checkpoint from disk (or uses provided path).\n    \"\"\"\n    # Load base model + trained adapter (TyDiQA approach)\n    base_model = CanineForQuestionAnswering.from_pretrained(\n        model_name, \n        trust_remote_code=False\n    )\n    \n    from peft import PeftModel\n    model = PeftModel.from_pretrained(base_model, checkpoint_path)\n    model.to(device)\n    \n    # Exact TyDiQA eval args\n    eval_args = TrainingArguments(\n        output_dir=\"outputs/canine-s-uqa-filtered\",\n        per_device_eval_batch_size=1,  # Match TyDiQA exactly\n        dataloader_drop_last=False,\n        fp16=False,  # TyDiQA uses False\n        bf16=False,\n        report_to=\"none\"\n    )\n    \n    eval_trainer = Trainer(\n        model=model,\n        args=eval_args,\n        eval_dataset=processed_val,\n        processing_class=tokenizer,  # Use processing_class\n    )\n    \n    # Progress bar (optional, TyDiQA has this)\n    print(f\"üß™ Evaluating checkpoint: {checkpoint_path}\")\n    from tqdm.auto import tqdm\n    with tqdm(total=len(processed_val), desc=\"Evaluating\", unit=\"samples\") as pbar:\n        predictions = eval_trainer.predict(processed_val)\n        pbar.update(len(processed_val))\n    \n    start_logits, end_logits = predictions.predictions\n    \n    # EXACT TyDiQA aggregation logic\n    best_predictions = {}\n    for feature_index, feature in enumerate(processed_val):\n        sample_idx = int(feature[\"overflow_to_sample_mapping\"])\n        input_ids = feature[\"input_ids\"]\n        \n        start_idx = int(np.argmax(start_logits[feature_index]))\n        end_idx = int(np.argmax(end_logits[feature_index]))\n        score = float(start_logits[feature_index][start_idx] + end_logits[feature_index][end_idx])\n        prediction_text = decode_prediction(input_ids, start_idx, end_idx)\n        \n        stored = best_predictions.get(sample_idx)\n        if stored is None or score > stored[0]:\n            best_predictions[sample_idx] = (score, prediction_text)\n\n    # TEST!\n    # After best_predictions loop, before computing metrics:\n    print(f\"\\nüîç Debug: Sample predictions:\")\n    for idx in list(best_predictions.keys())[:5]:\n        score, pred = best_predictions[idx]\n        gold = gold_answer(uqa_val[idx])\n        print(f\"  Pred: '{pred[:50]}' | Gold: '{gold[:50]}'\")\n    \n    # Calculate metrics\n    em_scores = []\n    f1_scores = []\n    edit_dist_scores = []\n    for sample_idx, (_, prediction_text) in best_predictions.items():\n        reference = gold_answer(uqa_val[int(sample_idx)])\n        em_scores.append(exact_match_score(prediction_text, reference))\n        f1_scores.append(f1_score(prediction_text, reference))\n        edit_dist_scores.append(edit_distance_score(prediction_text, reference))\n    \n    em = float(np.mean(em_scores)) if em_scores else 0.0\n    f1 = float(np.mean(f1_scores)) if f1_scores else 0.0\n    edit_dist = float(np.mean(edit_dist_scores)) if edit_dist_scores else 0.0\n    \n    print(f\"Examples evaluated: {len(em_scores)}\")\n    print(f\"Exact Match: {em * 100:.2f}\")\n    print(f\"F1: {f1 * 100:.2f}\")\n    print(f\"Edit Distance (normalized): {edit_dist * 100:.2f}\")\n    \n    return {\"exact_match\": em, \"f1\": f1, \"edit_distance\": edit_dist}","metadata":{"execution":{"iopub.status.busy":"2025-12-12T13:52:26.418874Z","iopub.execute_input":"2025-12-12T13:52:26.419150Z","iopub.status.idle":"2025-12-12T13:52:26.439870Z","shell.execute_reply.started":"2025-12-12T13:52:26.419125Z","shell.execute_reply":"2025-12-12T13:52:26.439127Z"},"trusted":true},"outputs":[],"execution_count":19},{"id":"15","cell_type":"code","source":"# ‚ö†Ô∏è CRITICAL: Must regenerate preprocessed data with FILTERED dataset\n# The old cache was created from unfiltered data - indices won't match!\n\n# print(\"üîÑ Preprocessing filtered dataset (this will take a few minutes)...\")\nprocessed_train = uqa_train.map(\n    lambda examples, indices: preprocess_uqa(examples, tokenizer, indices=indices), \n    batched=True, \n    remove_columns=uqa_train.column_names, \n    with_indices=True\n)\nprocessed_val = uqa_val.map(\n    lambda examples, indices: preprocess_uqa(examples, tokenizer, indices=indices), \n    batched=True, \n    remove_columns=uqa_val.column_names, \n    with_indices=True\n)\n\n# print(f\"‚úÖ Preprocessing complete!\")\n# print(f\"   Training chunks: {len(processed_train):,}\")\n# print(f\"   Validation chunks: {len(processed_val):,}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["b1984a6d29864e2d940119370816a37e","a307de6c263a4c20a6418344cbd98c0c","1db208af0dbc4f2facee78148266a207","d44d38a959ec44aa90e91c15a83abbd6","527baa5fc421480da4d2dc7041e19b1f","d398c81b546d4527a41dd97bd87ad7d8","ab4047c7f0144667857fe835d452f6c7","0120d513dd4d4fccac2d528eb7ff4696","c3e0981c2924416fbdf9ceab3e6b04ab","bc970c64373f4f69b6c6936087ed978a","4a74d22a2c334fbda54a95c5e29e712a"]},"execution":{"iopub.status.busy":"2025-12-12T13:52:26.440752Z","iopub.execute_input":"2025-12-12T13:52:26.441006Z","iopub.status.idle":"2025-12-12T13:54:25.904017Z","shell.execute_reply.started":"2025-12-12T13:52:26.440989Z","shell.execute_reply":"2025-12-12T13:54:25.903135Z"},"id":"d11807b9","outputId":"64fc2534-2871-4bd2-b3fa-4b37973486e2","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/80000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40c8f78e9894453a9715d203c49bfb93"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (3179 > 2048). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83b28e6e4ff5470aadb4cedff872deea"}},"metadata":{}}],"execution_count":20},{"id":"3d909f3d","cell_type":"code","source":"# print(\"=\"*80)\n# print(\"üìà DATASET STATISTICS AFTER PREPROCESSING\")\n# print(\"=\"*80)\n\n# # Count chunks per example\n# from collections import Counter\n# chunks_per_example = Counter(processed_train[\"overflow_to_sample_mapping\"])\n# chunks_distribution = Counter(chunks_per_example.values())\n\n# print(f\"\\nüì¶ Chunks Distribution:\")\n# print(f\"   Total original examples: {len(uqa_train):,}\")\n# print(f\"   Total preprocessed chunks: {len(processed_train):,}\")\n# print(f\"   Average chunks per example: {len(processed_train)/len(uqa_train):.2f}\")\n# print(f\"\\n   Distribution:\")\n# for num_chunks in sorted(chunks_distribution.keys())[:10]:\n#     count = chunks_distribution[num_chunks]\n#     print(f\"     {num_chunks} chunk(s): {count:,} examples ({count/len(uqa_train)*100:.1f}%)\")\n\n# # Count examples with answers in at least one chunk\n# examples_with_answers = 0\n# for orig_idx in range(len(uqa_train)):\n#     # Find all chunks for this example\n#     chunk_indices = [i for i, x in enumerate(processed_train[\"overflow_to_sample_mapping\"]) if x == orig_idx]\n    \n#     # Check if any chunk has an answer (not pointing to CLS)\n#     has_answer = False\n#     for chunk_idx in chunk_indices:\n#         input_ids = processed_train[chunk_idx][\"input_ids\"]\n#         start_pos = processed_train[chunk_idx][\"start_positions\"]\n#         end_pos = processed_train[chunk_idx][\"end_positions\"]\n#         cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n        \n#         if not (start_pos == cls_idx and end_pos == cls_idx):\n#             has_answer = True\n#             break\n    \n#     if has_answer:\n#         examples_with_answers += 1\n\n# print(f\"\\n‚úÖ Answer Coverage:\")\n# print(f\"   Examples with answer in at least one chunk: {examples_with_answers:,}/{len(uqa_train):,} ({examples_with_answers/len(uqa_train)*100:.1f}%)\")\n# print(f\"   Expected: ~100% (since we filtered impossible questions)\")\n\n# print(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:54:25.905603Z","iopub.execute_input":"2025-12-12T13:54:25.905909Z","iopub.status.idle":"2025-12-12T13:54:25.910445Z","shell.execute_reply.started":"2025-12-12T13:54:25.905889Z","shell.execute_reply":"2025-12-12T13:54:25.909574Z"}},"outputs":[],"execution_count":21},{"id":"917b817e","cell_type":"code","source":"print(\"=\"*80)\nprint(\"üîç BOUNDARY LOGIC VERIFICATION\")\nprint(\"=\"*80)\n\n# Test the critical boundary check logic\n# Find examples where answer is near chunk boundaries\n\nboundary_cases_found = 0\nboundary_cases_correct = 0\n\nfor proc_idx in random.sample(range(len(processed_train)), min(500, len(processed_train))):\n    proc_example = processed_train[proc_idx]\n    orig_idx = proc_example[\"overflow_to_sample_mapping\"]\n    orig_example = uqa_train[orig_idx]\n    \n    input_ids = proc_example[\"input_ids\"]\n    start_pos = proc_example[\"start_positions\"]\n    end_pos = proc_example[\"end_positions\"]\n    \n    cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n    \n    # Skip no-answer cases\n    if start_pos == cls_idx and end_pos == cls_idx:\n        continue\n    \n    # Check if this is a boundary case (answer near end of chunk)\n    # Context starts after first SEP token\n    sep_indices = [k for k, x in enumerate(input_ids) if x == tokenizer.sep_token_id]\n    if not sep_indices:\n        continue\n    \n    context_start = sep_indices[0] + 1\n    # Find context end (before padding or second SEP)\n    try:\n        context_end = sep_indices[1] if len(sep_indices) > 1 else len(input_ids)\n    except:\n        context_end = len(input_ids)\n    \n    # Check if answer ends near chunk boundary (within last 10 tokens)\n    if context_end - end_pos <= 10:\n        boundary_cases_found += 1\n        \n        # Verify the answer is correct\n        predicted_answer = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True).strip()\n        gold_ans = orig_example[\"answer\"].strip()\n        \n        if predicted_answer == gold_ans:\n            boundary_cases_correct += 1\n\nprint(f\"\\nüìä Boundary cases found: {boundary_cases_found}\")\nif boundary_cases_found > 0:\n    print(f\"‚úÖ Boundary cases correct: {boundary_cases_correct}/{boundary_cases_found} ({boundary_cases_correct/boundary_cases_found*100:.1f}%)\")\nelse:\n    print(f\"‚ö†Ô∏è  No boundary cases found in sample (may need more examples)\")\n\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:54:25.913490Z","iopub.execute_input":"2025-12-12T13:54:25.913750Z","iopub.status.idle":"2025-12-12T13:54:26.639308Z","shell.execute_reply.started":"2025-12-12T13:54:25.913706Z","shell.execute_reply":"2025-12-12T13:54:26.638540Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüîç BOUNDARY LOGIC VERIFICATION\n================================================================================\n\nüìä Boundary cases found: 6\n‚úÖ Boundary cases correct: 6/6 (100.0%)\n================================================================================\n","output_type":"stream"}],"execution_count":22},{"id":"dd016dd8","cell_type":"markdown","source":"## ‚úÖ Verification: Test Preprocessed Results\n\nBefore training, let's verify that the new preprocessor produces correct results.","metadata":{}},{"id":"58a880a7","cell_type":"code","source":"print(\"=\"*80)\nprint(\"üß™ TEST 1: Training Data Integrity\")\nprint(\"=\"*80)\n\n# Verify training data format\nprint(\"\\n1Ô∏è‚É£ Checking training dataset structure...\")\nrequired_columns = [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"start_positions\", \"end_positions\", \"overflow_to_sample_mapping\"]\nmissing = [col for col in required_columns if col not in processed_train.column_names]\n\nif missing:\n    print(f\"‚ùå CRITICAL: Missing columns: {missing}\")\nelse:\n    print(f\"‚úÖ All required columns present: {required_columns}\")\n\n# Check shapes and ranges\nprint(\"\\n2Ô∏è‚É£ Validating tensor shapes and ranges...\")\nissues = []\n\nfor i in range(min(100, len(processed_train))):\n    example = processed_train[i]\n    \n    # Check lengths\n    if len(example[\"input_ids\"]) != MAX_SEQ_LENGTH:\n        issues.append(f\"Example {i}: input_ids length {len(example['input_ids'])} != {MAX_SEQ_LENGTH}\")\n    if len(example[\"attention_mask\"]) != MAX_SEQ_LENGTH:\n        issues.append(f\"Example {i}: attention_mask length mismatch\")\n    if len(example[\"token_type_ids\"]) != MAX_SEQ_LENGTH:\n        issues.append(f\"Example {i}: token_type_ids length mismatch\")\n    \n    # Check position ranges\n    start = example[\"start_positions\"]\n    end = example[\"end_positions\"]\n    if start < 0 or start >= MAX_SEQ_LENGTH:\n        issues.append(f\"Example {i}: start_position {start} out of range\")\n    if end < 0 or end >= MAX_SEQ_LENGTH:\n        issues.append(f\"Example {i}: end_position {end} out of range\")\n    if start > end:\n        issues.append(f\"Example {i}: start {start} > end {end}\")\n\nif issues:\n    print(f\"‚ùå Found {len(issues)} issues:\")\n    for issue in issues[:10]:  # Show first 10\n        print(f\"   {issue}\")\nelse:\n    print(f\"‚úÖ All shapes and ranges valid (checked 100 examples)\")\n\n# Check overflow mapping\nprint(\"\\n3Ô∏è‚É£ Validating overflow_to_sample_mapping...\")\nmax_orig_idx = max(processed_train[\"overflow_to_sample_mapping\"])\nif max_orig_idx >= len(uqa_train):\n    print(f\"‚ùå CRITICAL: overflow_to_sample_mapping has index {max_orig_idx} >= dataset size {len(uqa_train)}\")\nelse:\n    print(f\"‚úÖ overflow_to_sample_mapping valid (max={max_orig_idx}, dataset size={len(uqa_train)})\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:54:26.640020Z","iopub.execute_input":"2025-12-12T13:54:26.640197Z","iopub.status.idle":"2025-12-12T13:54:29.669413Z","shell.execute_reply.started":"2025-12-12T13:54:26.640184Z","shell.execute_reply":"2025-12-12T13:54:29.668521Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüß™ TEST 1: Training Data Integrity\n================================================================================\n\n1Ô∏è‚É£ Checking training dataset structure...\n‚úÖ All required columns present: ['input_ids', 'attention_mask', 'token_type_ids', 'start_positions', 'end_positions', 'overflow_to_sample_mapping']\n\n2Ô∏è‚É£ Validating tensor shapes and ranges...\n‚úÖ All shapes and ranges valid (checked 100 examples)\n\n3Ô∏è‚É£ Validating overflow_to_sample_mapping...\n‚úÖ overflow_to_sample_mapping valid (max=79999, dataset size=80000)\n\n================================================================================\n","output_type":"stream"}],"execution_count":23},{"id":"4e0279bf","cell_type":"code","source":"import random\nfrom collections import defaultdict\n\nprint(\"=\"*80)\nprint(\"üß™ FIXED TEST: Answer Extraction Accuracy (OPTIMIZED)\")\nprint(\"=\"*80)\n\n# Step 1: Build reverse index ONCE (O(n) instead of O(n¬≤))\nprint(\"Building chunk index...\")\nchunk_index = defaultdict(list)\nfor chunk_idx, sample_idx in enumerate(processed_train[\"overflow_to_sample_mapping\"]):\n    chunk_index[sample_idx].append(chunk_idx)\n\n# Step 2: Test on random original examples\nnum_samples = 200\ntest_orig_indices = random.sample(range(len(uqa_train)), num_samples)\n\ncorrect = 0\nincorrect = 0\nfailed_examples = []\n\nfor orig_idx in test_orig_indices:\n    orig_example = uqa_train[orig_idx]\n    gold_ans = orig_example[\"answer\"].strip()\n    \n    # Get all chunks for this example (O(1) lookup!)\n    chunk_indices = chunk_index[orig_idx]\n    \n    # Check if ANY chunk has the correct answer\n    found_correct = False\n    for chunk_idx in chunk_indices:\n        proc = processed_train[chunk_idx]\n        input_ids = proc[\"input_ids\"]\n        start = proc[\"start_positions\"]\n        end = proc[\"end_positions\"]\n        \n        cls_idx = input_ids.index(tokenizer.cls_token_id)\n        \n        # Skip chunks without answer\n        if start == cls_idx and end == cls_idx:\n            continue\n        \n        # Extract answer\n        predicted = tokenizer.decode(input_ids[start:end+1], skip_special_tokens=True).strip()\n        \n        if predicted.lower() == gold_ans.lower():\n            found_correct = True\n            break\n    \n    if found_correct or not gold_ans:\n        correct += 1\n    else:\n        incorrect += 1\n        if len(failed_examples) < 5:\n            failed_examples.append({\n                \"idx\": orig_idx,\n                \"question\": orig_example[\"question\"][:60],\n                \"gold\": gold_ans[:50],\n                \"num_chunks\": len(chunk_indices)\n            })\n\naccuracy = correct / num_samples * 100\nprint(f\"\\nüìä Results: ‚úÖ {correct}/{num_samples} ({accuracy:.1f}%)\")\n\nif accuracy >= 95:\n    print(\"‚úÖ PASSED - Preprocessor working correctly!\")\nelse:\n    print(f\"‚ùå FAILED - Only {accuracy:.1f}% accuracy\")\n    if failed_examples:\n        print(f\"\\n‚ö†Ô∏è First {len(failed_examples)} failures:\")\n        for ex in failed_examples:\n            print(f\"  #{ex['idx']}: '{ex['question']}...'\")\n            print(f\"    Expected: '{ex['gold']}', Chunks: {ex['num_chunks']}\")\n\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:54:29.670375Z","iopub.execute_input":"2025-12-12T13:54:29.670773Z","iopub.status.idle":"2025-12-12T13:54:33.006852Z","shell.execute_reply.started":"2025-12-12T13:54:29.670746Z","shell.execute_reply":"2025-12-12T13:54:33.005841Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüß™ FIXED TEST: Answer Extraction Accuracy (OPTIMIZED)\n================================================================================\nBuilding chunk index...\n\nüìä Results: ‚úÖ 200/200 (100.0%)\n‚úÖ PASSED - Preprocessor working correctly!\n================================================================================\n","output_type":"stream"}],"execution_count":24},{"id":"29130506","cell_type":"code","source":"print(\"=\"*80)\nprint(\"üß™ TEST 3: Validation Data Integrity\")\nprint(\"=\"*80)\n\n# Same checks for validation data\nprint(\"\\n1Ô∏è‚É£ Checking validation dataset structure...\")\nmissing_val = [col for col in required_columns if col not in processed_val.column_names]\n\nif missing_val:\n    print(f\"‚ùå CRITICAL: Missing columns: {missing_val}\")\nelse:\n    print(f\"‚úÖ All required columns present\")\n\n# Check validation mapping\nprint(\"\\n2Ô∏è‚É£ Validating overflow_to_sample_mapping...\")\nmax_val_idx = max(processed_val[\"overflow_to_sample_mapping\"])\nif max_val_idx >= len(uqa_val):\n    print(f\"‚ùå CRITICAL: overflow_to_sample_mapping has index {max_val_idx} >= dataset size {len(uqa_val)}\")\nelse:\n    print(f\"‚úÖ overflow_to_sample_mapping valid (max={max_val_idx}, dataset size={len(uqa_val)})\")\n\n# Test extraction on validation\nprint(\"\\n3Ô∏è‚É£ Testing answer extraction on validation set...\")\nval_correct = 0\nval_incorrect = 0\nval_samples = min(100, len(processed_val))\n\nfor proc_idx in range(val_samples):\n    proc_example = processed_val[proc_idx]\n    orig_idx = proc_example[\"overflow_to_sample_mapping\"]\n    orig_example = uqa_val[orig_idx]\n    \n    input_ids = proc_example[\"input_ids\"]\n    start_pos = proc_example[\"start_positions\"]\n    end_pos = proc_example[\"end_positions\"]\n    \n    cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n    \n    if start_pos == cls_idx and end_pos == cls_idx:\n        predicted_answer = \"\"\n    else:\n        predicted_answer = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True).strip()\n    \n    gold_ans = orig_example[\"answer\"].strip()\n\n    print(f\"GOLD: {gold_ans.lower()}\")\n    print(f\"PREDICTED: {predicted_answer.lower()}\\n\")\n    \n    if predicted_answer.lower() == gold_ans.lower():\n        val_correct += 1\n    else:\n        val_incorrect += 1\n\nval_accuracy = val_correct / val_samples * 100\nprint(f\"   Validation accuracy: {val_correct}/{val_samples} ({val_accuracy:.1f}%)\")\n\nif val_accuracy < 95:\n    print(f\"   ‚ùå WARNING: Validation accuracy is low!\")\nelse:\n    print(f\"   ‚úÖ Validation data is correct!\")\n\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:54:33.007675Z","iopub.execute_input":"2025-12-12T13:54:33.007987Z","iopub.status.idle":"2025-12-12T13:54:33.434829Z","shell.execute_reply.started":"2025-12-12T13:54:33.007954Z","shell.execute_reply":"2025-12-12T13:54:33.434031Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüß™ TEST 3: Validation Data Integrity\n================================================================================\n\n1Ô∏è‚É£ Checking validation dataset structure...\n‚úÖ All required columns present\n\n2Ô∏è‚É£ Validating overflow_to_sample_mapping...\n‚úÖ overflow_to_sample_mapping valid (max=7999, dataset size=8000)\n\n3Ô∏è‚É£ Testing answer extraction on validation set...\nGOLD: ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å\nPREDICTED: ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å\n\nGOLD: ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å\nPREDICTED: \n\nGOLD: ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å\nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: €Å€åŸà⁄Øÿ≥ ŸÖŸÅÿ±Ÿàÿ∂€Å\nPREDICTED: €Å€åŸà⁄Øÿ≥ ŸÖŸÅÿ±Ÿàÿ∂€Å\n\nGOLD: €Å€åŸà⁄Øÿ≥ ŸÖŸÅÿ±Ÿàÿ∂€Å\nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: ⁄ØÿßŸÑÿ¥ ŸÜÿßŸÖ ÿ±€åŸàŸÜŸàÿ≥\nPREDICTED: ⁄ØÿßŸÑÿ¥ ŸÜÿßŸÖ ÿ±€åŸàŸÜŸàÿ≥\n\nGOLD: ⁄ØÿßŸÑÿ¥ ŸÜÿßŸÖ ÿ±€åŸàŸÜŸàÿ≥\nPREDICTED: \n\nGOLD: ⁄ØÿßŸÑÿ¥ ŸÜÿßŸÖ ÿ±€åŸàŸÜŸàÿ≥\nPREDICTED: \n\nGOLD: p\nPREDICTED: \n\nGOLD: p\nPREDICTED: p\n\nGOLD: p\nPREDICTED: \n\nGOLD: ŸÜ€åŸπ Ÿàÿ±⁄©ŸÜ⁄Ø ŸÅŸàÿßÿ¶ÿØ ⁄©Ÿà ÿ®⁄ë⁄æÿßŸÜÿß\nPREDICTED: ŸÜ€åŸπ Ÿàÿ±⁄©ŸÜ⁄Ø ŸÅŸàÿßÿ¶ÿØ ⁄©Ÿà ÿ®⁄ë⁄æÿßŸÜÿß\n\nGOLD: ŸÜ€åŸπ Ÿàÿ±⁄©ŸÜ⁄Ø ŸÅŸàÿßÿ¶ÿØ ⁄©Ÿà ÿ®⁄ë⁄æÿßŸÜÿß\nPREDICTED: \n\nGOLD: ÿ™ŸÇÿ±€åÿ®ÿß 20 ⁄Ø⁄æŸÜŸπŸà⁄∫\nPREDICTED: \n\nGOLD: ÿ™ŸÇÿ±€åÿ®ÿß 20 ⁄Ø⁄æŸÜŸπŸà⁄∫\nPREDICTED: ÿ™ŸÇÿ±€åÿ®ÿß 20 ⁄Ø⁄æŸÜŸπŸà⁄∫\n\nGOLD: ÿ™ŸÇÿ±€åÿ®ÿß 20 ⁄Ø⁄æŸÜŸπŸà⁄∫\nPREDICTED: \n\nGOLD: ÿ™ŸÇÿ±€åÿ®ÿß 20 ⁄Ø⁄æŸÜŸπŸà⁄∫\nPREDICTED: \n\nGOLD: ÿßÿ≥⁄©ÿßŸπÿ¥ ÿ¢ÿ≤ÿßÿØ€å\nPREDICTED: \n\nGOLD: ÿßÿ≥⁄©ÿßŸπÿ¥ ÿ¢ÿ≤ÿßÿØ€å\nPREDICTED: ÿßÿ≥⁄©ÿßŸπÿ¥ ÿ¢ÿ≤ÿßÿØ€å\n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: €åŸÖÿ±⁄Ü ÿ±ÿßÿ¶ŸÜ ÿ®ÿ±ÿ¨\nPREDICTED: \n\nGOLD: €åŸÖÿ±⁄Ü ÿ±ÿßÿ¶ŸÜ ÿ®ÿ±ÿ¨\nPREDICTED: €åŸÖÿ±⁄Ü ÿ±ÿßÿ¶ŸÜ ÿ®ÿ±ÿ¨\n\nGOLD: €åŸÖÿ±⁄Ü ÿ±ÿßÿ¶ŸÜ ÿ®ÿ±ÿ¨\nPREDICTED: \n\nGOLD: ÿπÿßŸÖ ÿÆÿ±ÿßÿ®€å ÿßŸàÿ± ⁄à⁄©Ÿπÿßÿ¶ŸÑ ⁄©⁄æ€åŸÜ⁄ÜŸÜ€í ÿßŸàÿ± Ÿæÿ™ŸÑ€í €ÅŸàŸÜ€í ⁄©€í ÿ∞ÿ±€åÿπ€í\nPREDICTED: ÿπÿßŸÖ ÿÆÿ±ÿßÿ®€å ÿßŸàÿ± ⁄à⁄©Ÿπÿßÿ¶ŸÑ ⁄©⁄æ€åŸÜ⁄ÜŸÜ€í ÿßŸàÿ± Ÿæÿ™ŸÑ€í €ÅŸàŸÜ€í ⁄©€í ÿ∞ÿ±€åÿπ€í\n\nGOLD: ÿπÿßŸÖ ÿÆÿ±ÿßÿ®€å ÿßŸàÿ± ⁄à⁄©Ÿπÿßÿ¶ŸÑ ⁄©⁄æ€åŸÜ⁄ÜŸÜ€í ÿßŸàÿ± Ÿæÿ™ŸÑ€í €ÅŸàŸÜ€í ⁄©€í ÿ∞ÿ±€åÿπ€í\nPREDICTED: \n\nGOLD: ÿπÿßŸÖ ÿÆÿ±ÿßÿ®€å ÿßŸàÿ± ⁄à⁄©Ÿπÿßÿ¶ŸÑ ⁄©⁄æ€åŸÜ⁄ÜŸÜ€í ÿßŸàÿ± Ÿæÿ™ŸÑ€í €ÅŸàŸÜ€í ⁄©€í ÿ∞ÿ±€åÿπ€í\nPREDICTED: \n\nGOLD: €ÅÿßŸÑ€åŸÜ⁄à ÿå Ÿæÿ±Ÿàÿ≥€åÿß ÿßŸàÿ± ÿ¨ŸÜŸàÿ®€å ÿßŸÅÿ±€åŸÇ€Å\nPREDICTED: \n\nGOLD: €ÅÿßŸÑ€åŸÜ⁄à ÿå Ÿæÿ±Ÿàÿ≥€åÿß ÿßŸàÿ± ÿ¨ŸÜŸàÿ®€å ÿßŸÅÿ±€åŸÇ€Å\nPREDICTED: €ÅÿßŸÑ€åŸÜ⁄à ÿå Ÿæÿ±Ÿàÿ≥€åÿß ÿßŸàÿ± ÿ¨ŸÜŸàÿ®€å ÿßŸÅÿ±€åŸÇ€Å\n\nGOLD: €ÅÿßŸÑ€åŸÜ⁄à ÿå Ÿæÿ±Ÿàÿ≥€åÿß ÿßŸàÿ± ÿ¨ŸÜŸàÿ®€å ÿßŸÅÿ±€åŸÇ€Å\nPREDICTED: \n\nGOLD: 2001\nPREDICTED: \n\nGOLD: 2001\nPREDICTED: 2001\n\nGOLD: 2001\nPREDICTED: \n\nGOLD: 2001\nPREDICTED: \n\nGOLD: ⁄©€åŸÜ€åŸÖ€åŸπ⁄© Ÿæ€åŸÖÿßÿ¶ÿ¥Ÿà⁄∫\nPREDICTED: ⁄©€åŸÜ€åŸÖ€åŸπ⁄© Ÿæ€åŸÖÿßÿ¶ÿ¥Ÿà⁄∫\n\nGOLD: ⁄©€åŸÜ€åŸÖ€åŸπ⁄© Ÿæ€åŸÖÿßÿ¶ÿ¥Ÿà⁄∫\nPREDICTED: \n\nGOLD: ⁄©€åŸÜ€åŸÖ€åŸπ⁄© Ÿæ€åŸÖÿßÿ¶ÿ¥Ÿà⁄∫\nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: ÿ¥ŸÖÿßŸÑ€å ÿßŸÖÿ±€å⁄©€Å ŸÖ€å⁄∫ ÿ®ÿ±ÿ∑ÿßŸÜŸà€å ŸÜÿß⁄©ÿßŸÖ€åŸà⁄∫\nPREDICTED: ÿ¥ŸÖÿßŸÑ€å ÿßŸÖÿ±€å⁄©€Å ŸÖ€å⁄∫ ÿ®ÿ±ÿ∑ÿßŸÜŸà€å ŸÜÿß⁄©ÿßŸÖ€åŸà⁄∫\n\nGOLD: ÿ¥ŸÖÿßŸÑ€å ÿßŸÖÿ±€å⁄©€Å ŸÖ€å⁄∫ ÿ®ÿ±ÿ∑ÿßŸÜŸà€å ŸÜÿß⁄©ÿßŸÖ€åŸà⁄∫\nPREDICTED: \n\nGOLD: ÿ¥ŸÖÿßŸÑ€å ÿßŸÖÿ±€å⁄©€Å ŸÖ€å⁄∫ ÿ®ÿ±ÿ∑ÿßŸÜŸà€å ŸÜÿß⁄©ÿßŸÖ€åŸà⁄∫\nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: ÿ¨ÿßŸÜ ⁄àÿ®ŸÑ€åŸà Ÿà€å⁄©ÿ≥ ÿ®ÿ±ÿ¨\nPREDICTED: ÿ¨ÿßŸÜ ⁄àÿ®ŸÑ€åŸà Ÿà€å⁄©ÿ≥ ÿ®ÿ±ÿ¨\n\nGOLD: ÿ¨ÿßŸÜ ⁄àÿ®ŸÑ€åŸà Ÿà€å⁄©ÿ≥ ÿ®ÿ±ÿ¨\nPREDICTED: \n\nGOLD: ⁄Ü⁄æ ÿ≥ÿßŸÑŸà⁄∫\nPREDICTED: \n\nGOLD: ⁄Ü⁄æ ÿ≥ÿßŸÑŸà⁄∫\nPREDICTED: ⁄Ü⁄æ ÿ≥ÿßŸÑŸà⁄∫\n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: Ÿπ€åŸÖŸà⁄©Ÿàÿß ŸÑŸà⁄ØŸà⁄∫\nPREDICTED: Ÿπ€åŸÖŸà⁄©Ÿàÿß ŸÑŸà⁄ØŸà⁄∫\n\nGOLD: Ÿπ€åŸÖŸà⁄©Ÿàÿß ŸÑŸà⁄ØŸà⁄∫\nPREDICTED: \n\nGOLD: Ÿπ€åŸÖŸà⁄©Ÿàÿß ŸÑŸà⁄ØŸà⁄∫\nPREDICTED: \n\nGOLD: ÿ¢ÿ±⁄©€åŸπ€å⁄©Ÿπÿ≥\nPREDICTED: ÿ¢ÿ±⁄©€åŸπ€å⁄©Ÿπÿ≥\n\nGOLD: ÿ¢ÿ±⁄©€åŸπ€å⁄©Ÿπÿ≥\nPREDICTED: \n\nGOLD: ⁄©⁄Ü⁄æ ŸÖÿ≥ÿ™ÿ≠⁄©ŸÖ ÿØŸÅÿπÿßÿ™\nPREDICTED: \n\nGOLD: ⁄©⁄Ü⁄æ ŸÖÿ≥ÿ™ÿ≠⁄©ŸÖ ÿØŸÅÿπÿßÿ™\nPREDICTED: ⁄©⁄Ü⁄æ ŸÖÿ≥ÿ™ÿ≠⁄©ŸÖ ÿØŸÅÿπÿßÿ™\n\nGOLD: ŸÇÿßÿ∂€å\nPREDICTED: \n\nGOLD: ŸÇÿßÿ∂€å\nPREDICTED: ŸÇÿßÿ∂€å\n\nGOLD: ŸÇÿßÿ∂€å\nPREDICTED: ŸÇÿßÿ∂€å\n\nGOLD: Ÿà€Å ŸÖÿßÿ≠ŸàŸÑ ÿ¨ÿ≥ ŸÖ€å⁄∫ Ÿà€Å ÿ±€Åÿ™€í €Å€å⁄∫\nPREDICTED: Ÿà€Å ŸÖÿßÿ≠ŸàŸÑ ÿ¨ÿ≥ ŸÖ€å⁄∫ Ÿà€Å ÿ±€Åÿ™€í €Å€å⁄∫\n\nGOLD: Ÿà€Å ŸÖÿßÿ≠ŸàŸÑ ÿ¨ÿ≥ ŸÖ€å⁄∫ Ÿà€Å ÿ±€Åÿ™€í €Å€å⁄∫\nPREDICTED: \n\nGOLD: Ÿà€Å ŸÖÿßÿ≠ŸàŸÑ ÿ¨ÿ≥ ŸÖ€å⁄∫ Ÿà€Å ÿ±€Åÿ™€í €Å€å⁄∫\nPREDICTED: \n\nGOLD: ÿ±ŸàŸÖÿßŸÜŸπ⁄© ÿ±ÿßÿ¶ŸÜ\nPREDICTED: \n\nGOLD: ÿ±ŸàŸÖÿßŸÜŸπ⁄© ÿ±ÿßÿ¶ŸÜ\nPREDICTED: ÿ±ŸàŸÖÿßŸÜŸπ⁄© ÿ±ÿßÿ¶ŸÜ\n\nGOLD: ÿßÿπŸÑ€å ÿπÿØŸÖ ŸÖÿ≥ÿßŸàÿßÿ™\nPREDICTED: \n\nGOLD: ÿßÿπŸÑ€å ÿπÿØŸÖ ŸÖÿ≥ÿßŸàÿßÿ™\nPREDICTED: ÿßÿπŸÑ€å ÿπÿØŸÖ ŸÖÿ≥ÿßŸàÿßÿ™\n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: \nPREDICTED: \n\nGOLD: n ⁄©€í ŸÖÿ±ÿ®ÿπ ÿ¨⁄ë\nPREDICTED: n ⁄©€í ŸÖÿ±ÿ®ÿπ ÿ¨⁄ë\n\nGOLD: n ⁄©€í ŸÖÿ±ÿ®ÿπ ÿ¨⁄ë\nPREDICTED: \n\nGOLD: n ⁄©€í ŸÖÿ±ÿ®ÿπ ÿ¨⁄ë\nPREDICTED: \n\nGOLD: n ⁄©€í ŸÖÿ±ÿ®ÿπ ÿ¨⁄ë\nPREDICTED: \n\nGOLD: ŸÅÿ±ÿßŸÜÿ≥\nPREDICTED: ŸÅÿ±ÿßŸÜÿ≥\n\nGOLD: ŸÅÿ±ÿßŸÜÿ≥\nPREDICTED: \n\nGOLD: ŸÅÿ±ÿßŸÜÿ≥\nPREDICTED: \n\nGOLD: ŸÅÿ±ÿßŸÜÿ≥\nPREDICTED: \n\nGOLD: ÿ™ÿ¥ÿØÿØ\nPREDICTED: ÿ™ÿ¥ÿØÿØ\n\nGOLD: ÿ™ÿ¥ÿØÿØ\nPREDICTED: \n\nGOLD: ÿ™ÿ¥ÿØÿØ\nPREDICTED: \n\nGOLD: Ÿπ€åŸÖ€å⁄©ŸàŸÑÿß ÿßŸàÿ± ŸÖŸàÿ±€åŸπÿß\nPREDICTED: Ÿπ€åŸÖ€å⁄©ŸàŸÑÿß ÿßŸàÿ± ŸÖŸàÿ±€åŸπÿß\n\nGOLD: Ÿπ€åŸÖ€å⁄©ŸàŸÑÿß ÿßŸàÿ± ŸÖŸàÿ±€åŸπÿß\nPREDICTED: \n\n   Validation accuracy: 55/100 (55.0%)\n   ‚ùå WARNING: Validation accuracy is low!\n================================================================================\n","output_type":"stream"}],"execution_count":25},{"id":"fb2600f9-5e66-462f-a6d6-198fde492516","cell_type":"markdown","source":"### !!! KEY TAKEAWAY FROM ABOVE CELLS!\n\nA lot of chunks do not have the answer in the chunked context, so (0, 0) -> `[CLS]` tok is being predicted!\nThis may be giving way to a lot of mispredictions in evaluation!","metadata":{}},{"id":"7f38b20f","cell_type":"code","source":"print(\"=\"*80)\nprint(\"üß™ TEST 4: Evaluation Functions Correctness\")\nprint(\"=\"*80)\n\n# Test the metric functions\nprint(\"\\n1Ô∏è‚É£ Testing normalize_answer()...\")\ntest_cases = [\n    (\"Hello World\", \"hello world\"),\n    (\"The quick fox\", \"quick fox\"),\n    (\"Test!\", \"test\"),\n    (\"  spaces  \", \"spaces\"),\n]\n\nfor input_text, expected in test_cases:\n    result = normalize_answer(input_text)\n    status = \"‚úÖ\" if result == expected else \"‚ùå\"\n    print(f\"   {status} normalize_answer('{input_text}') = '{result}' (expected: '{expected}')\")\n\n# Test exact_match_score\nprint(\"\\n2Ô∏è‚É£ Testing exact_match_score()...\")\nem_tests = [\n    (\"hello\", \"hello\", 1.0),\n    (\"hello\", \"Hello\", 1.0),  # Case insensitive\n    (\"the answer\", \"answer\", 1.0),  # Articles removed\n    (\"hello\", \"world\", 0.0),\n    (\"\", \"\", 1.0),\n]\n\nfor pred, gold, expected in em_tests:\n    result = exact_match_score(pred, gold)\n    status = \"‚úÖ\" if result == expected else \"‚ùå\"\n    print(f\"   {status} EM('{pred}', '{gold}') = {result} (expected: {expected})\")\n\n# Test f1_score\nprint(\"\\n3Ô∏è‚É£ Testing f1_score()...\")\nf1_tests = [\n    (\"hello world\", \"hello world\", 1.0),\n    (\"hello\", \"world\", 0.0),\n    (\"hello world\", \"hello\", 0.67),  # Approximate\n    (\"\", \"\", 1.0),\n    (\"hello\", \"\", 0.0),\n]\n\nall_f1_ok = True\nfor pred, gold, expected in f1_tests:\n    result = f1_score(pred, gold)\n    # Allow small tolerance for floating point\n    ok = abs(result - expected) < 0.01 or (expected == 0 and result == 0)\n    status = \"‚úÖ\" if ok else \"‚ùå\"\n    if not ok:\n        all_f1_ok = False\n    print(f\"   {status} F1('{pred}', '{gold}') = {result:.2f} (expected: ~{expected})\")\n\n# Test decode_prediction\nprint(\"\\n4Ô∏è‚É£ Testing decode_prediction()...\")\nsample_ids = tokenizer.encode(\"This is a test answer\", add_special_tokens=True)\ncls_idx = sample_ids.index(tokenizer.cls_token_id)\n\ndecode_tests = [\n    (sample_ids, cls_idx, cls_idx, \"\"),  # No answer case\n    (sample_ids, 5, 3, \"\"),  # Invalid range (start > end)\n    (sample_ids, -1, 5, \"\"),  # Negative index\n    (sample_ids, 2, 5, \"non-empty\"),  # Valid range should return something\n]\n\nfor ids, start, end, expected_type in decode_tests:\n    result = decode_prediction(ids, start, end)\n    if expected_type == \"\":\n        ok = result == \"\"\n        status = \"‚úÖ\" if ok else \"‚ùå\"\n        print(f\"   {status} decode_prediction(..., {start}, {end}) = '{result}' (expected empty)\")\n    else:\n        ok = len(result) > 0\n        status = \"‚úÖ\" if ok else \"‚ùå\"\n        print(f\"   {status} decode_prediction(..., {start}, {end}) = '{result}' (expected non-empty)\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:54:33.435664Z","iopub.execute_input":"2025-12-12T13:54:33.435921Z","iopub.status.idle":"2025-12-12T13:54:33.449489Z","shell.execute_reply.started":"2025-12-12T13:54:33.435903Z","shell.execute_reply":"2025-12-12T13:54:33.448755Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüß™ TEST 4: Evaluation Functions Correctness\n================================================================================\n\n1Ô∏è‚É£ Testing normalize_answer()...\n   ‚úÖ normalize_answer('Hello World') = 'hello world' (expected: 'hello world')\n   ‚úÖ normalize_answer('The quick fox') = 'quick fox' (expected: 'quick fox')\n   ‚úÖ normalize_answer('Test!') = 'test' (expected: 'test')\n   ‚úÖ normalize_answer('  spaces  ') = 'spaces' (expected: 'spaces')\n\n2Ô∏è‚É£ Testing exact_match_score()...\n   ‚úÖ EM('hello', 'hello') = 1.0 (expected: 1.0)\n   ‚úÖ EM('hello', 'Hello') = 1.0 (expected: 1.0)\n   ‚úÖ EM('the answer', 'answer') = 1.0 (expected: 1.0)\n   ‚úÖ EM('hello', 'world') = 0.0 (expected: 0.0)\n   ‚úÖ EM('', '') = 1.0 (expected: 1.0)\n\n3Ô∏è‚É£ Testing f1_score()...\n   ‚úÖ F1('hello world', 'hello world') = 1.00 (expected: ~1.0)\n   ‚úÖ F1('hello', 'world') = 0.00 (expected: ~0.0)\n   ‚úÖ F1('hello world', 'hello') = 0.67 (expected: ~0.67)\n   ‚úÖ F1('', '') = 1.00 (expected: ~1.0)\n   ‚úÖ F1('hello', '') = 0.00 (expected: ~0.0)\n\n4Ô∏è‚É£ Testing decode_prediction()...\n   ‚úÖ decode_prediction(..., 0, 0) = '' (expected empty)\n   ‚úÖ decode_prediction(..., 5, 3) = '' (expected empty)\n   ‚úÖ decode_prediction(..., -1, 5) = '' (expected empty)\n   ‚úÖ decode_prediction(..., 2, 5) = 'his' (expected non-empty)\n\n================================================================================\n","output_type":"stream"}],"execution_count":26},{"id":"d93ea4be","cell_type":"code","source":"print(\"=\"*80)\nprint(\"üß™ TEST 5: Model Forward Pass (Sanity Check)\")\nprint(\"=\"*80)\n\n# Test that model can process a batch\nprint(\"\\n1Ô∏è‚É£ Testing model forward pass...\")\n\ntry:\n    # Take a small batch\n    batch_size = 4\n    sample_batch = processed_train.select(range(batch_size))\n    \n    # Convert to tensors\n    input_ids = torch.tensor(sample_batch[\"input_ids\"]).to(device)\n    attention_mask = torch.tensor(sample_batch[\"attention_mask\"]).to(device)\n    token_type_ids = torch.tensor(sample_batch[\"token_type_ids\"]).to(device)\n    start_positions = torch.tensor(sample_batch[\"start_positions\"]).to(device)\n    end_positions = torch.tensor(sample_batch[\"end_positions\"]).to(device)\n    \n    print(f\"   Input shape: {input_ids.shape}\")\n    print(f\"   Attention mask shape: {attention_mask.shape}\")\n    print(f\"   Token type IDs shape: {token_type_ids.shape}\")\n    \n    # Forward pass\n    model.to(device)\n    model.eval()\n    \n    with torch.no_grad():\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            start_positions=start_positions,\n            end_positions=end_positions\n        )\n    \n    print(f\"\\n   ‚úÖ Forward pass successful!\")\n    print(f\"   Loss: {outputs.loss.item():.4f}\")\n    print(f\"   Start logits shape: {outputs.start_logits.shape}\")\n    print(f\"   End logits shape: {outputs.end_logits.shape}\")\n    \n    # Check logits are valid\n    if torch.isnan(outputs.start_logits).any() or torch.isnan(outputs.end_logits).any():\n        print(f\"   ‚ùå WARNING: NaN values in logits!\")\n    else:\n        print(f\"   ‚úÖ Logits are valid (no NaN)\")\n    \n    # Check loss is reasonable\n    if outputs.loss.item() < 0 or outputs.loss.item() > 100:\n        print(f\"   ‚ö†Ô∏è  WARNING: Loss seems unusual: {outputs.loss.item()}\")\n    else:\n        print(f\"   ‚úÖ Loss is in reasonable range\")\n    \nexcept Exception as e:\n    print(f\"   ‚ùå CRITICAL ERROR during forward pass: {e}\")\n    import traceback\n    traceback.print_exc()\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:54:33.450762Z","iopub.execute_input":"2025-12-12T13:54:33.451425Z","iopub.status.idle":"2025-12-12T13:54:34.401959Z","shell.execute_reply.started":"2025-12-12T13:54:33.451403Z","shell.execute_reply":"2025-12-12T13:54:34.401188Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüß™ TEST 5: Model Forward Pass (Sanity Check)\n================================================================================\n\n1Ô∏è‚É£ Testing model forward pass...\n   Input shape: torch.Size([4, 384])\n   Attention mask shape: torch.Size([4, 384])\n   Token type IDs shape: torch.Size([4, 384])\n\n   ‚úÖ Forward pass successful!\n   Loss: 5.9142\n   Start logits shape: torch.Size([4, 384])\n   End logits shape: torch.Size([4, 384])\n   ‚úÖ Logits are valid (no NaN)\n   ‚úÖ Loss is in reasonable range\n\n================================================================================\n","output_type":"stream"}],"execution_count":27},{"id":"5cb1b468","cell_type":"code","source":"print(\"=\"*80)\nprint(\"üß™ TEST 6: Critical Boundary Cases\")\nprint(\"=\"*80)\n\n# Verify the fix for the <= vs < bug\nprint(\"\\n1Ô∏è‚É£ Testing chunk boundary logic (the critical bug fix)...\")\n\nboundary_correct = 0\nboundary_total = 0\n\nfor proc_idx in range(min(1000, len(processed_train))):\n    proc_example = processed_train[proc_idx]\n    orig_idx = proc_example[\"overflow_to_sample_mapping\"]\n    orig_example = uqa_train[orig_idx]\n    \n    input_ids = proc_example[\"input_ids\"]\n    start_pos = proc_example[\"start_positions\"]\n    end_pos = proc_example[\"end_positions\"]\n    \n    cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n    \n    # Skip no-answer cases\n    if start_pos == cls_idx:\n        continue\n    \n    # Find context boundaries\n    sep_indices = [k for k, x in enumerate(input_ids) if x == tokenizer.sep_token_id]\n    if not sep_indices:\n        continue\n    \n    context_start = sep_indices[0] + 1\n    \n    # Check if answer is near end of context chunk (within last 5 positions)\n    # This is where the bug would manifest\n    if len(sep_indices) > 1:\n        context_end = sep_indices[1]\n    else:\n        # Find first padding token\n        context_end = next((i for i, x in enumerate(input_ids) if x == tokenizer.pad_token_id), len(input_ids))\n    \n    if context_end - end_pos <= 5:\n        boundary_total += 1\n        \n        # Verify extraction is correct\n        predicted = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True).strip()\n        gold = orig_example[\"answer\"].strip()\n        \n        if predicted.lower() == gold.lower():\n            boundary_correct += 1\n\nprint(f\"\\n   Found {boundary_total} boundary cases (answer near chunk end)\")\nif boundary_total > 0:\n    boundary_accuracy = boundary_correct / boundary_total * 100\n    print(f\"   Boundary cases correct: {boundary_correct}/{boundary_total} ({boundary_accuracy:.1f}%)\")\n    \n    if boundary_accuracy < 95:\n        print(f\"   ‚ùå WARNING: Boundary logic may still have issues!\")\n    else:\n        print(f\"   ‚úÖ Boundary fix is working correctly!\")\nelse:\n    print(f\"   ‚ö†Ô∏è  No boundary cases found in first 1000 examples\")\n\n# Test the specific case from verification script\nprint(\"\\n2Ô∏è‚É£ Testing the exact bug scenario...\")\n# Answer [90, 99] inclusive, Chunk [0, 100) exclusive\ntest_start = 90\ntest_end = 99  # inclusive\nchunk_start = 0\nchunk_end = 100  # exclusive\n\n# Correct logic (what we implemented)\ncorrect_result = test_start >= chunk_start and test_end < chunk_end\n# Buggy logic (what we fixed)\nbuggy_result = test_start >= chunk_start and test_end <= chunk_end\n\nprint(f\"   Scenario: answer=[{test_start},{test_end}], chunk=[{chunk_start},{chunk_end})\")\nprint(f\"   Correct logic (< for end): {correct_result}\")\nprint(f\"   Buggy logic (<= for end): {buggy_result}\")\n\nif correct_result == True and buggy_result == True:\n    print(f\"   ‚úÖ Both agree when answer is inside chunk\")\nelif correct_result != buggy_result:\n    print(f\"   ‚ö†Ô∏è  Logics differ - this is where the bug would cause mislabeling\")\n\n# Now test the failing case\ntest_end = 100  # Now extends beyond\ncorrect_result = test_start >= chunk_start and test_end < chunk_end\nbuggy_result = test_start >= chunk_start and test_end <= chunk_end\n\nprint(f\"\\n   Scenario: answer=[{test_start},{test_end}], chunk=[{chunk_start},{chunk_end})\")\nprint(f\"   Correct logic (< for end): {correct_result} ‚úÖ\")\nprint(f\"   Buggy logic (<= for end): {buggy_result} ‚ùå\")\n\nif correct_result == False and buggy_result == True:\n    print(f\"   ‚úÖ Fix verified: correct logic rejects, buggy logic accepts (WRONG)\")\nelse:\n    print(f\"   ‚ùå Something is wrong with the logic\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T13:54:34.402813Z","iopub.execute_input":"2025-12-12T13:54:34.403034Z","iopub.status.idle":"2025-12-12T13:54:35.483471Z","shell.execute_reply.started":"2025-12-12T13:54:34.403017Z","shell.execute_reply":"2025-12-12T13:54:35.482790Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüß™ TEST 6: Critical Boundary Cases\n================================================================================\n\n1Ô∏è‚É£ Testing chunk boundary logic (the critical bug fix)...\n\n   Found 2 boundary cases (answer near chunk end)\n   Boundary cases correct: 2/2 (100.0%)\n   ‚úÖ Boundary fix is working correctly!\n\n2Ô∏è‚É£ Testing the exact bug scenario...\n   Scenario: answer=[90,99], chunk=[0,100)\n   Correct logic (< for end): True\n   Buggy logic (<= for end): True\n   ‚úÖ Both agree when answer is inside chunk\n\n   Scenario: answer=[90,100], chunk=[0,100)\n   Correct logic (< for end): False ‚úÖ\n   Buggy logic (<= for end): True ‚ùå\n   ‚úÖ Fix verified: correct logic rejects, buggy logic accepts (WRONG)\n\n================================================================================\n","output_type":"stream"}],"execution_count":28},{"id":"37f1f131","cell_type":"markdown","source":"---\n\n## üî¨ COMPREHENSIVE QA PIPELINE VERIFICATION\n\nBefore training, let's verify **every single component** of the QA pipeline end-to-end.","metadata":{}},{"id":"16","cell_type":"code","source":"# processed_train","metadata":{"execution":{"iopub.status.busy":"2025-12-12T13:54:35.484262Z","iopub.execute_input":"2025-12-12T13:54:35.484466Z","iopub.status.idle":"2025-12-12T13:54:35.488755Z","shell.execute_reply.started":"2025-12-12T13:54:35.484452Z","shell.execute_reply":"2025-12-12T13:54:35.487989Z"},"id":"D-emFQTIaZRL","trusted":true},"outputs":[],"execution_count":29},{"id":"19","cell_type":"code","source":"# processed_val","metadata":{"execution":{"iopub.status.busy":"2025-12-12T13:54:35.489575Z","iopub.execute_input":"2025-12-12T13:54:35.489863Z","iopub.status.idle":"2025-12-12T13:54:35.503327Z","shell.execute_reply.started":"2025-12-12T13:54:35.489843Z","shell.execute_reply":"2025-12-12T13:54:35.502613Z"},"id":"Yy3SiWwCabEi","trusted":true},"outputs":[],"execution_count":30},{"id":"20","cell_type":"code","source":"# Save newly processed data (OPTIONAL - for future reuse with same filtered dataset)\nprocessed_train.save_to_disk(\"/kaggle/working/cache/processed_train_uqa_filtered\")\nprocessed_val.save_to_disk(\"/kaggle/working/cache/processed_val_uqa_filtered\")\n\n# # ‚ùå DO NOT load old cache - it has index mismatches with filtered data!\n# # If you've already run the preprocessing cell above, skip this cell\n\nprocessed_train = load_from_disk(\"/kaggle/working/cache/processed_train_uqa_filtered\")\nprocessed_val = load_from_disk(\"/kaggle/working/cache/processed_val_uqa_filtered\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["05d936e2dc9d412b8637c174a3c0be64","7e0b41aa16f241a4ba3bb8a2f3525984","2bebe7e1f3f341dfaabf29963d2c5995","41b300b02ed2413ba80865aaa99ece2a","b4740a7137e742d687e2075b60d2be8a","80132c8e4fa743fca850936ecfebc7f7","303bb3d75f7d4e94aeb60c5491ea6e61","d7463faafecf4e46a87dc6863a646cea","bc199cddba714aeda650d97fef015a14","1a508a6457bb460ba17d5adb0a9e9f85","3aac2656907a416291a622717ccaf929","fa1af70d9c95443c9f09666359ba3769","ddb717fd4dbc40c6bd8422a02f925060","6d1342eeaf4f4f0489fe0746ceaaeb09","a10683e5c1164f349cbdc75b1567994c","71cfe2c8df474badb255d7d28da04348","077fbb403e5f4069841e558a3cc0c065","b068b9fac9f24eca9bd430bab30ea70c","8cbfc6f4ec434674ac59d3fbdbddcd3b","4d675a6788b641c6a09604ef17514dec","bd9b9f21be9744c49d99ac4bc76f11e1","89469ab4bd6f48d8b9aa369473c7230f"]},"execution":{"iopub.status.busy":"2025-12-12T13:54:35.504207Z","iopub.execute_input":"2025-12-12T13:54:35.504397Z","iopub.status.idle":"2025-12-12T13:54:36.210137Z","shell.execute_reply.started":"2025-12-12T13:54:35.504382Z","shell.execute_reply":"2025-12-12T13:54:36.209573Z"},"id":"77ecdd17","outputId":"602e648b-4a75-424b-da09-d58f3295a65e","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/2 shards):   0%|          | 0/233583 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3db702147234b3881f7c510c5949946"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/25286 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d6ac4f80ac84f00ac5962c5c616ce75"}},"metadata":{}}],"execution_count":31},{"id":"21","cell_type":"code","source":"def print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2025-12-12T13:54:36.210965Z","iopub.execute_input":"2025-12-12T13:54:36.211138Z","iopub.status.idle":"2025-12-12T13:54:36.215441Z","shell.execute_reply.started":"2025-12-12T13:54:36.211123Z","shell.execute_reply":"2025-12-12T13:54:36.214758Z"},"id":"c0e06e6b","trusted":true},"outputs":[],"execution_count":32},{"id":"22","cell_type":"code","source":"# build LoRA model\n\npeft_model = get_peft_model(model, lora_config)\npeft_model.gradient_checkpointing_enable()\nprint_trainable_parameters(peft_model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-12-12T13:54:36.216362Z","iopub.execute_input":"2025-12-12T13:54:36.216705Z","iopub.status.idle":"2025-12-12T13:54:36.281097Z","shell.execute_reply.started":"2025-12-12T13:54:36.216682Z","shell.execute_reply":"2025-12-12T13:54:36.280196Z"},"id":"ba9eeeed","outputId":"27071b6e-b703-4b47-9288-9a1c6f3eba55","trusted":true},"outputs":[{"name":"stdout","text":"trainable params: 345602 || all params: 132430084 || trainable%: 0.26096940329661045\n","output_type":"stream"}],"execution_count":33},{"id":"23","cell_type":"code","source":"# Show what the model sees during training\nprint(\"=\"*80)\nprint(\"üéì MODEL TRAINING DATA FLOW\")\nprint(\"=\"*80)\n\n# Take one batch from preprocessed data\nbatch_size = 4\nsample_batch = processed_train.select(range(batch_size))\n\nprint(f\"\\n1Ô∏è‚É£ BATCH STRUCTURE\")\nprint(\"-\"*80)\nprint(f\"Batch size: {batch_size} chunks\")\nprint(f\"Each chunk in the batch contains:\")\n\n# Show batch structure\nfor key in sample_batch.column_names:\n    sample_value = sample_batch[0][key]\n    if isinstance(sample_value, list):\n        print(f\"  - {key}: shape ({batch_size}, {len(sample_value)})\")\n    else:\n        print(f\"  - {key}: shape ({batch_size},)\")\n\nprint(f\"\\n2Ô∏è‚É£ WHAT THE MODEL RECEIVES (for 1 chunk in batch)\")\nprint(\"-\"*80)\nexample_idx = 0\nprint(f\"Input IDs: {len(sample_batch[example_idx]['input_ids'])} tokens\")\nprint(f\"  First 10 token IDs: {sample_batch[example_idx]['input_ids'][:10]}\")\nprint(f\"\\nAttention mask: {sample_batch[example_idx]['attention_mask'][:20]}...\")\nprint(f\"  (1=attend to token, 0=ignore padding)\")\nprint(f\"\\nToken type IDs: {sample_batch[example_idx]['token_type_ids'][:20]}...\")\nprint(f\"  (0=question tokens, 1=context tokens)\")\n\nprint(f\"\\n3Ô∏è‚É£ TRAINING TARGETS (what model learns to predict)\")\nprint(\"-\"*80)\nprint(f\"Target start position: {sample_batch[example_idx]['start_positions']}\")\nprint(f\"Target end position: {sample_batch[example_idx]['end_positions']}\")\nprint(f\"\\nüí° The model learns to output these exact positions!\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"execution":{"iopub.status.busy":"2025-12-12T13:54:36.282044Z","iopub.execute_input":"2025-12-12T13:54:36.282338Z","iopub.status.idle":"2025-12-12T13:54:36.315510Z","shell.execute_reply.started":"2025-12-12T13:54:36.282307Z","shell.execute_reply":"2025-12-12T13:54:36.314446Z"},"trusted":true},"outputs":[{"name":"stdout","text":"================================================================================\nüéì MODEL TRAINING DATA FLOW\n================================================================================\n\n1Ô∏è‚É£ BATCH STRUCTURE\n--------------------------------------------------------------------------------\nBatch size: 4 chunks\nEach chunk in the batch contains:\n  - input_ids: shape (4, 384)\n  - attention_mask: shape (4, 384)\n  - token_type_ids: shape (4, 384)\n  - start_positions: shape (4,)\n  - end_positions: shape (4,)\n  - overflow_to_sample_mapping: shape (4,)\n\n2Ô∏è‚É£ WHAT THE MODEL RECEIVES (for 1 chunk in batch)\n--------------------------------------------------------------------------------\nInput IDs: 384 tokens\n  First 10 token IDs: [57344, 1580, 1583, 1740, 1583, 32, 1729, 1608, 1575, 1574]\n\nAttention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]...\n  (1=attend to token, 0=ignore padding)\n\nToken type IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]...\n  (0=question tokens, 1=context tokens)\n\n3Ô∏è‚É£ TRAINING TARGETS (what model learns to predict)\n--------------------------------------------------------------------------------\nTarget start position: 0\nTarget end position: 0\n\nüí° The model learns to output these exact positions!\n\n================================================================================\n","output_type":"stream"}],"execution_count":34},{"id":"5a98237c","cell_type":"markdown","source":"---","metadata":{}},{"id":"24","cell_type":"markdown","source":"## Model Training:\n","metadata":{}},{"id":"26","cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"outputs/canine-s-uqa-filtered\",\n\n    per_device_train_batch_size=4,  # increased train_batch_size from tydiqa\n    per_device_eval_batch_size=16,\n\n    gradient_accumulation_steps=4,  # decreased grad accum from tydiqa\n    gradient_checkpointing=True,\n\n    num_train_epochs=1, # same as tydiqa\n    learning_rate=3e-5,  \n    weight_decay=0.01,\n    \n    eval_strategy=\"no\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=1000,  # increased to 1000\n    logging_steps=50,\n    \n    fp16=True,\n    bf16=False,\n    report_to=\"none\",\n    push_to_hub=True,\n    hub_model_id=\"VohraAK/canine-s-uqa-filtered\",\n    hub_strategy=\"checkpoint\",\n    )\n\n# CustomEvalCallback - EXACT TyDiQA approach\nclass CustomEvalCallback(TrainerCallback):\n    def __init__(self, eval_func, eval_dataset):\n        self.eval_func = eval_func\n        self.eval_dataset = eval_dataset\n\n    def on_save(self, args, state, control, model=None, **kwargs):\n        \"\"\"\n        Runs AFTER checkpoint is saved.\n        Loads checkpoint from disk and evaluates it.\n        \"\"\"\n        checkpoint_path = f\"{args.output_dir}/checkpoint-{state.global_step}\"\n        print(f\"\\nüîç Running custom evaluation at step {state.global_step}...\")\n\n        # Call evaluation function (loads from checkpoint)\n        metrics = self.eval_func(checkpoint_path)\n\n        # Add metrics to state's log_history\n        state.log_history.append({\n            \"step\": state.global_step,\n            \"eval_exact_match\": metrics[\"exact_match\"],\n            \"eval_f1\": metrics[\"f1\"],\n            \"eval_edit_distance\": metrics[\"edit_distance\"],\n        })\n\n        # Print metrics\n        print(f\"‚úÖ Step {state.global_step}: EM={metrics['exact_match']*100:.2f}, F1={metrics['f1']*100:.2f}, EditDist={metrics['edit_distance']*100:.2f}\")\n\n        # Re-save trainer_state.json with updated metrics\n        state_path = f\"{checkpoint_path}/trainer_state.json\"\n        try:\n            with open(state_path, 'r') as f:\n                state_dict = json.load(f)\n            state_dict['log_history'] = state.log_history\n            with open(state_path, 'w') as f:\n                json.dump(state_dict, f, indent=2)\n            print(f\"üíæ Updated trainer_state.json with custom metrics\")\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Warning: Could not update trainer_state.json: {e}\")\n\n        # Push to Hub\n        try:\n            print(f\"‚òÅÔ∏è  Pushing checkpoint-{state.global_step} to Hub...\")\n            api = HfApi()\n            api.upload_folder(\n                folder_path=checkpoint_path,\n                repo_id=args.hub_model_id,\n                path_in_repo=f\"checkpoint-{state.global_step}\",\n                commit_message=f\"Add checkpoint {state.global_step} (EM={metrics['exact_match']*100:.1f}%, F1={metrics['f1']*100:.1f}%)\",\n                repo_type=\"model\"\n            )\n            print(f\"‚úÖ Pushed checkpoint-{state.global_step} to Hub\")\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Warning: Could not push to Hub: {e}\")\n\n        return control\n\n","metadata":{"execution":{"iopub.status.busy":"2025-12-12T13:54:36.316502Z","iopub.execute_input":"2025-12-12T13:54:36.316807Z","iopub.status.idle":"2025-12-12T13:54:36.362869Z","shell.execute_reply.started":"2025-12-12T13:54:36.316787Z","shell.execute_reply":"2025-12-12T13:54:36.362227Z"},"id":"c4abaaab","trusted":true},"outputs":[],"execution_count":35},{"id":"27","cell_type":"code","source":"trainer_cb = CustomEvalCallback(evaluate_checkpoint, processed_val)\n\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=processed_train,\n    eval_dataset=processed_val,\n    callbacks=[trainer_cb],\n)","metadata":{"execution":{"iopub.status.busy":"2025-12-12T13:54:36.363703Z","iopub.execute_input":"2025-12-12T13:54:36.364020Z","iopub.status.idle":"2025-12-12T13:54:36.919098Z","shell.execute_reply.started":"2025-12-12T13:54:36.363994Z","shell.execute_reply":"2025-12-12T13:54:36.918341Z"},"id":"055f5dda","trusted":true},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":36},{"id":"28","cell_type":"code","source":"trainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.status.busy":"2025-12-12T13:54:36.919985Z","iopub.execute_input":"2025-12-12T13:54:36.920216Z","iopub.status.idle":"2025-12-12T17:52:21.825625Z","shell.execute_reply.started":"2025-12-12T13:54:36.920191Z","shell.execute_reply":"2025-12-12T17:52:21.824970Z"},"id":"TOUimesUX5Re","outputId":"cfa62dcd-8eb4-475a-910b-1c38a3894cc2","trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='14599' max='14599' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [14599/14599 3:57:43, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>5.893600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>5.850800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>5.797100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>5.764900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>5.719000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>5.683300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>5.651800</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>5.611800</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>5.557500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>5.509600</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>5.507700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>5.443100</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>5.413700</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>5.367900</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>5.313300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>5.306800</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>5.277100</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>5.234200</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>5.234700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>5.173200</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>5.167100</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>5.142300</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>5.101400</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>5.048800</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>5.031700</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>4.947500</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>4.962200</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>4.950500</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>4.896800</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>4.918800</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>4.912500</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>4.909900</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>4.832900</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>4.777800</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>4.839100</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>4.712400</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>4.736600</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>4.720400</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>4.701500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>4.670900</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>4.631600</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>4.689100</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>4.656200</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>4.676100</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>4.622400</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>4.582700</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>4.512900</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>4.551900</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>4.549200</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>4.542200</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>4.573000</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>4.468500</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>4.404200</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>4.457700</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>4.399700</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>4.324800</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>4.347000</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>4.414700</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>4.266400</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>4.213600</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>4.290500</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>4.277200</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>4.264400</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>4.261300</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>4.328400</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>4.271800</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>4.125900</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>4.221100</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>4.221000</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>4.176700</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>4.129700</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>4.225400</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>4.141500</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>4.132100</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>4.173800</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>4.081900</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>4.071400</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>4.068900</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>4.224400</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>4.121200</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>4.002000</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>4.124800</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>3.995100</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>4.061200</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>4.013000</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>3.982000</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>3.999800</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>3.986800</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>3.945100</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>3.997400</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>3.988600</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>4.033400</td>\n    </tr>\n    <tr>\n      <td>4650</td>\n      <td>3.968900</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>3.966900</td>\n    </tr>\n    <tr>\n      <td>4750</td>\n      <td>3.873200</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>3.884500</td>\n    </tr>\n    <tr>\n      <td>4850</td>\n      <td>3.838600</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>3.868900</td>\n    </tr>\n    <tr>\n      <td>4950</td>\n      <td>3.797400</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>3.878500</td>\n    </tr>\n    <tr>\n      <td>5050</td>\n      <td>3.903000</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>3.859000</td>\n    </tr>\n    <tr>\n      <td>5150</td>\n      <td>3.854600</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>3.919500</td>\n    </tr>\n    <tr>\n      <td>5250</td>\n      <td>3.915800</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>3.688300</td>\n    </tr>\n    <tr>\n      <td>5350</td>\n      <td>3.763000</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>3.888800</td>\n    </tr>\n    <tr>\n      <td>5450</td>\n      <td>3.878200</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>3.750500</td>\n    </tr>\n    <tr>\n      <td>5550</td>\n      <td>3.698600</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>3.773000</td>\n    </tr>\n    <tr>\n      <td>5650</td>\n      <td>3.655500</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>3.778300</td>\n    </tr>\n    <tr>\n      <td>5750</td>\n      <td>3.741100</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>3.701100</td>\n    </tr>\n    <tr>\n      <td>5850</td>\n      <td>3.794200</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>3.623300</td>\n    </tr>\n    <tr>\n      <td>5950</td>\n      <td>3.586000</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>3.664400</td>\n    </tr>\n    <tr>\n      <td>6050</td>\n      <td>3.622500</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>3.772800</td>\n    </tr>\n    <tr>\n      <td>6150</td>\n      <td>3.651200</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>3.740900</td>\n    </tr>\n    <tr>\n      <td>6250</td>\n      <td>3.673900</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>3.615100</td>\n    </tr>\n    <tr>\n      <td>6350</td>\n      <td>3.856900</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>3.762400</td>\n    </tr>\n    <tr>\n      <td>6450</td>\n      <td>3.475600</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>3.583100</td>\n    </tr>\n    <tr>\n      <td>6550</td>\n      <td>3.539900</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>3.625800</td>\n    </tr>\n    <tr>\n      <td>6650</td>\n      <td>3.722400</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>3.603300</td>\n    </tr>\n    <tr>\n      <td>6750</td>\n      <td>3.662200</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>3.630000</td>\n    </tr>\n    <tr>\n      <td>6850</td>\n      <td>3.671500</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>3.556800</td>\n    </tr>\n    <tr>\n      <td>6950</td>\n      <td>3.652500</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>3.637500</td>\n    </tr>\n    <tr>\n      <td>7050</td>\n      <td>3.574400</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>3.454900</td>\n    </tr>\n    <tr>\n      <td>7150</td>\n      <td>3.551800</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>3.676400</td>\n    </tr>\n    <tr>\n      <td>7250</td>\n      <td>3.574200</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>3.572300</td>\n    </tr>\n    <tr>\n      <td>7350</td>\n      <td>3.497500</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>3.658200</td>\n    </tr>\n    <tr>\n      <td>7450</td>\n      <td>3.570400</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>3.559100</td>\n    </tr>\n    <tr>\n      <td>7550</td>\n      <td>3.677300</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>3.607500</td>\n    </tr>\n    <tr>\n      <td>7650</td>\n      <td>3.494300</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>3.452400</td>\n    </tr>\n    <tr>\n      <td>7750</td>\n      <td>3.524100</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>3.550400</td>\n    </tr>\n    <tr>\n      <td>7850</td>\n      <td>3.654400</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>3.521200</td>\n    </tr>\n    <tr>\n      <td>7950</td>\n      <td>3.578900</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>3.558000</td>\n    </tr>\n    <tr>\n      <td>8050</td>\n      <td>3.515600</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>3.492700</td>\n    </tr>\n    <tr>\n      <td>8150</td>\n      <td>3.360100</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>3.622100</td>\n    </tr>\n    <tr>\n      <td>8250</td>\n      <td>3.423800</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>3.596000</td>\n    </tr>\n    <tr>\n      <td>8350</td>\n      <td>3.557100</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>3.665600</td>\n    </tr>\n    <tr>\n      <td>8450</td>\n      <td>3.416200</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>3.568200</td>\n    </tr>\n    <tr>\n      <td>8550</td>\n      <td>3.394400</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>3.422100</td>\n    </tr>\n    <tr>\n      <td>8650</td>\n      <td>3.511900</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>3.443300</td>\n    </tr>\n    <tr>\n      <td>8750</td>\n      <td>3.480800</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>3.416300</td>\n    </tr>\n    <tr>\n      <td>8850</td>\n      <td>3.409600</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>3.341600</td>\n    </tr>\n    <tr>\n      <td>8950</td>\n      <td>3.413700</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>3.342900</td>\n    </tr>\n    <tr>\n      <td>9050</td>\n      <td>3.528900</td>\n    </tr>\n    <tr>\n      <td>9100</td>\n      <td>3.367100</td>\n    </tr>\n    <tr>\n      <td>9150</td>\n      <td>3.390900</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>3.550700</td>\n    </tr>\n    <tr>\n      <td>9250</td>\n      <td>3.462300</td>\n    </tr>\n    <tr>\n      <td>9300</td>\n      <td>3.412100</td>\n    </tr>\n    <tr>\n      <td>9350</td>\n      <td>3.438200</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>3.434200</td>\n    </tr>\n    <tr>\n      <td>9450</td>\n      <td>3.492400</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>3.415000</td>\n    </tr>\n    <tr>\n      <td>9550</td>\n      <td>3.504400</td>\n    </tr>\n    <tr>\n      <td>9600</td>\n      <td>3.463400</td>\n    </tr>\n    <tr>\n      <td>9650</td>\n      <td>3.410100</td>\n    </tr>\n    <tr>\n      <td>9700</td>\n      <td>3.468100</td>\n    </tr>\n    <tr>\n      <td>9750</td>\n      <td>3.460300</td>\n    </tr>\n    <tr>\n      <td>9800</td>\n      <td>3.372600</td>\n    </tr>\n    <tr>\n      <td>9850</td>\n      <td>3.427900</td>\n    </tr>\n    <tr>\n      <td>9900</td>\n      <td>3.452300</td>\n    </tr>\n    <tr>\n      <td>9950</td>\n      <td>3.369400</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>3.466900</td>\n    </tr>\n    <tr>\n      <td>10050</td>\n      <td>3.408800</td>\n    </tr>\n    <tr>\n      <td>10100</td>\n      <td>3.402200</td>\n    </tr>\n    <tr>\n      <td>10150</td>\n      <td>3.241700</td>\n    </tr>\n    <tr>\n      <td>10200</td>\n      <td>3.319100</td>\n    </tr>\n    <tr>\n      <td>10250</td>\n      <td>3.424500</td>\n    </tr>\n    <tr>\n      <td>10300</td>\n      <td>3.334900</td>\n    </tr>\n    <tr>\n      <td>10350</td>\n      <td>3.364100</td>\n    </tr>\n    <tr>\n      <td>10400</td>\n      <td>3.393700</td>\n    </tr>\n    <tr>\n      <td>10450</td>\n      <td>3.376900</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>3.359000</td>\n    </tr>\n    <tr>\n      <td>10550</td>\n      <td>3.385700</td>\n    </tr>\n    <tr>\n      <td>10600</td>\n      <td>3.270500</td>\n    </tr>\n    <tr>\n      <td>10650</td>\n      <td>3.495300</td>\n    </tr>\n    <tr>\n      <td>10700</td>\n      <td>3.309700</td>\n    </tr>\n    <tr>\n      <td>10750</td>\n      <td>3.330000</td>\n    </tr>\n    <tr>\n      <td>10800</td>\n      <td>3.420000</td>\n    </tr>\n    <tr>\n      <td>10850</td>\n      <td>3.384800</td>\n    </tr>\n    <tr>\n      <td>10900</td>\n      <td>3.218700</td>\n    </tr>\n    <tr>\n      <td>10950</td>\n      <td>3.329300</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>3.385300</td>\n    </tr>\n    <tr>\n      <td>11050</td>\n      <td>3.260100</td>\n    </tr>\n    <tr>\n      <td>11100</td>\n      <td>3.296800</td>\n    </tr>\n    <tr>\n      <td>11150</td>\n      <td>3.148300</td>\n    </tr>\n    <tr>\n      <td>11200</td>\n      <td>3.461900</td>\n    </tr>\n    <tr>\n      <td>11250</td>\n      <td>3.368300</td>\n    </tr>\n    <tr>\n      <td>11300</td>\n      <td>3.355400</td>\n    </tr>\n    <tr>\n      <td>11350</td>\n      <td>3.452500</td>\n    </tr>\n    <tr>\n      <td>11400</td>\n      <td>3.523500</td>\n    </tr>\n    <tr>\n      <td>11450</td>\n      <td>3.440400</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>3.294600</td>\n    </tr>\n    <tr>\n      <td>11550</td>\n      <td>3.311400</td>\n    </tr>\n    <tr>\n      <td>11600</td>\n      <td>3.361600</td>\n    </tr>\n    <tr>\n      <td>11650</td>\n      <td>3.332900</td>\n    </tr>\n    <tr>\n      <td>11700</td>\n      <td>3.244700</td>\n    </tr>\n    <tr>\n      <td>11750</td>\n      <td>3.339900</td>\n    </tr>\n    <tr>\n      <td>11800</td>\n      <td>3.209400</td>\n    </tr>\n    <tr>\n      <td>11850</td>\n      <td>3.332400</td>\n    </tr>\n    <tr>\n      <td>11900</td>\n      <td>3.302900</td>\n    </tr>\n    <tr>\n      <td>11950</td>\n      <td>3.319900</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>3.443600</td>\n    </tr>\n    <tr>\n      <td>12050</td>\n      <td>3.280300</td>\n    </tr>\n    <tr>\n      <td>12100</td>\n      <td>3.246000</td>\n    </tr>\n    <tr>\n      <td>12150</td>\n      <td>3.314600</td>\n    </tr>\n    <tr>\n      <td>12200</td>\n      <td>3.237200</td>\n    </tr>\n    <tr>\n      <td>12250</td>\n      <td>3.207300</td>\n    </tr>\n    <tr>\n      <td>12300</td>\n      <td>3.248500</td>\n    </tr>\n    <tr>\n      <td>12350</td>\n      <td>3.474000</td>\n    </tr>\n    <tr>\n      <td>12400</td>\n      <td>3.364500</td>\n    </tr>\n    <tr>\n      <td>12450</td>\n      <td>3.349700</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>3.305700</td>\n    </tr>\n    <tr>\n      <td>12550</td>\n      <td>3.404500</td>\n    </tr>\n    <tr>\n      <td>12600</td>\n      <td>3.285600</td>\n    </tr>\n    <tr>\n      <td>12650</td>\n      <td>3.360400</td>\n    </tr>\n    <tr>\n      <td>12700</td>\n      <td>3.241200</td>\n    </tr>\n    <tr>\n      <td>12750</td>\n      <td>3.263400</td>\n    </tr>\n    <tr>\n      <td>12800</td>\n      <td>3.321600</td>\n    </tr>\n    <tr>\n      <td>12850</td>\n      <td>3.259600</td>\n    </tr>\n    <tr>\n      <td>12900</td>\n      <td>3.310200</td>\n    </tr>\n    <tr>\n      <td>12950</td>\n      <td>3.145200</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>3.311000</td>\n    </tr>\n    <tr>\n      <td>13050</td>\n      <td>3.268200</td>\n    </tr>\n    <tr>\n      <td>13100</td>\n      <td>3.317600</td>\n    </tr>\n    <tr>\n      <td>13150</td>\n      <td>3.382200</td>\n    </tr>\n    <tr>\n      <td>13200</td>\n      <td>3.474600</td>\n    </tr>\n    <tr>\n      <td>13250</td>\n      <td>3.229500</td>\n    </tr>\n    <tr>\n      <td>13300</td>\n      <td>3.239000</td>\n    </tr>\n    <tr>\n      <td>13350</td>\n      <td>3.271400</td>\n    </tr>\n    <tr>\n      <td>13400</td>\n      <td>3.224100</td>\n    </tr>\n    <tr>\n      <td>13450</td>\n      <td>3.250100</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>3.175900</td>\n    </tr>\n    <tr>\n      <td>13550</td>\n      <td>3.218900</td>\n    </tr>\n    <tr>\n      <td>13600</td>\n      <td>3.299400</td>\n    </tr>\n    <tr>\n      <td>13650</td>\n      <td>3.324300</td>\n    </tr>\n    <tr>\n      <td>13700</td>\n      <td>3.355200</td>\n    </tr>\n    <tr>\n      <td>13750</td>\n      <td>3.306900</td>\n    </tr>\n    <tr>\n      <td>13800</td>\n      <td>3.404300</td>\n    </tr>\n    <tr>\n      <td>13850</td>\n      <td>3.272400</td>\n    </tr>\n    <tr>\n      <td>13900</td>\n      <td>3.274600</td>\n    </tr>\n    <tr>\n      <td>13950</td>\n      <td>3.294700</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>3.301700</td>\n    </tr>\n    <tr>\n      <td>14050</td>\n      <td>3.228600</td>\n    </tr>\n    <tr>\n      <td>14100</td>\n      <td>3.315800</td>\n    </tr>\n    <tr>\n      <td>14150</td>\n      <td>3.263800</td>\n    </tr>\n    <tr>\n      <td>14200</td>\n      <td>3.290000</td>\n    </tr>\n    <tr>\n      <td>14250</td>\n      <td>3.286000</td>\n    </tr>\n    <tr>\n      <td>14300</td>\n      <td>3.142700</td>\n    </tr>\n    <tr>\n      <td>14350</td>\n      <td>3.419500</td>\n    </tr>\n    <tr>\n      <td>14400</td>\n      <td>3.376400</td>\n    </tr>\n    <tr>\n      <td>14450</td>\n      <td>3.269300</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>3.417100</td>\n    </tr>\n    <tr>\n      <td>14550</td>\n      <td>3.284500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nüîç Running custom evaluation at step 1000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"üß™ Evaluating checkpoint: outputs/canine-s-uqa-filtered/checkpoint-1000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/25286 [00:00<?, ?samples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89532b1714fa417eae0138ce6dc7b85b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nüîç Debug: Sample predictions:\n  Pred: '' | Gold: 'ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å'\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: '€Å€åŸà⁄Øÿ≥ ŸÖŸÅÿ±Ÿàÿ∂€Å'\n  Pred: '' | Gold: ''\nExamples evaluated: 8000\nExact Match: 28.52\nF1: 29.12\nEdit Distance (normalized): 30.36\n‚úÖ Step 1000: EM=28.52, F1=29.12, EditDist=30.36\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-1000 to Hub...\n‚úÖ Pushed checkpoint-1000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 2000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"üß™ Evaluating checkpoint: outputs/canine-s-uqa-filtered/checkpoint-2000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/25286 [00:00<?, ?samples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41c4cf372a8746eeb235755cf4cba9a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nüîç Debug: Sample predictions:\n  Pred: '' | Gold: 'ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å'\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: '€Å€åŸà⁄Øÿ≥ ŸÖŸÅÿ±Ÿàÿ∂€Å'\n  Pred: '' | Gold: ''\nExamples evaluated: 8000\nExact Match: 31.06\nF1: 31.41\nEdit Distance (normalized): 32.08\n‚úÖ Step 2000: EM=31.06, F1=31.41, EditDist=32.08\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-2000 to Hub...\n‚úÖ Pushed checkpoint-2000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 3000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"üß™ Evaluating checkpoint: outputs/canine-s-uqa-filtered/checkpoint-3000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/25286 [00:00<?, ?samples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3846b1fcb1f745859074e64669d7b4f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nüîç Debug: Sample predictions:\n  Pred: '' | Gold: 'ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å'\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: '€Å€åŸà⁄Øÿ≥ ŸÖŸÅÿ±Ÿàÿ∂€Å'\n  Pred: '' | Gold: ''\nExamples evaluated: 8000\nExact Match: 31.84\nF1: 32.04\nEdit Distance (normalized): 32.56\n‚úÖ Step 3000: EM=31.84, F1=32.04, EditDist=32.56\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-3000 to Hub...\n‚úÖ Pushed checkpoint-3000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 4000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"üß™ Evaluating checkpoint: outputs/canine-s-uqa-filtered/checkpoint-4000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/25286 [00:00<?, ?samples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"634feeec77cf4987927f529c79610ae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nüîç Debug: Sample predictions:\n  Pred: '' | Gold: 'ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å'\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: '€Å€åŸà⁄Øÿ≥ ŸÖŸÅÿ±Ÿàÿ∂€Å'\n  Pred: '' | Gold: ''\nExamples evaluated: 8000\nExact Match: 32.05\nF1: 32.22\nEdit Distance (normalized): 32.64\n‚úÖ Step 4000: EM=32.05, F1=32.22, EditDist=32.64\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-4000 to Hub...\n‚úÖ Pushed checkpoint-4000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 5000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"üß™ Evaluating checkpoint: outputs/canine-s-uqa-filtered/checkpoint-5000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/25286 [00:00<?, ?samples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"909dcdb610c743b19e7e4e75ee2b4764"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nüîç Debug: Sample predictions:\n  Pred: '' | Gold: 'ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å'\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: '€Å€åŸà⁄Øÿ≥ ŸÖŸÅÿ±Ÿàÿ∂€Å'\n  Pred: '' | Gold: ''\nExamples evaluated: 8000\nExact Match: 32.27\nF1: 32.42\nEdit Distance (normalized): 32.83\n‚úÖ Step 5000: EM=32.27, F1=32.42, EditDist=32.83\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-5000 to Hub...\n‚úÖ Pushed checkpoint-5000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 6000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"üß™ Evaluating checkpoint: outputs/canine-s-uqa-filtered/checkpoint-6000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/25286 [00:00<?, ?samples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0677f7cb7d04f2ab14e7a6337be2a46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nüîç Debug: Sample predictions:\n  Pred: '' | Gold: 'ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å'\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: '€Å€åŸà⁄Øÿ≥ ŸÖŸÅÿ±Ÿàÿ∂€Å'\n  Pred: '' | Gold: ''\nExamples evaluated: 8000\nExact Match: 32.49\nF1: 32.62\nEdit Distance (normalized): 33.00\n‚úÖ Step 6000: EM=32.49, F1=32.62, EditDist=33.00\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-6000 to Hub...\n‚úÖ Pushed checkpoint-6000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 7000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"üß™ Evaluating checkpoint: outputs/canine-s-uqa-filtered/checkpoint-7000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/25286 [00:00<?, ?samples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4a4f5665672460bb8ee56280d04e663"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nüîç Debug: Sample predictions:\n  Pred: '' | Gold: 'ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å'\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: '€Å€åŸà⁄Øÿ≥ ŸÖŸÅÿ±Ÿàÿ∂€Å'\n  Pred: '' | Gold: ''\nExamples evaluated: 8000\nExact Match: 32.64\nF1: 32.76\nEdit Distance (normalized): 33.09\n‚úÖ Step 7000: EM=32.64, F1=32.76, EditDist=33.09\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-7000 to Hub...\n‚úÖ Pushed checkpoint-7000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 8000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"üß™ Evaluating checkpoint: outputs/canine-s-uqa-filtered/checkpoint-8000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/25286 [00:00<?, ?samples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5267585f1a214f72b3c9192f508388c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nüîç Debug: Sample predictions:\n  Pred: '' | Gold: 'ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å'\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: '€Å€åŸà⁄Øÿ≥ ŸÖŸÅÿ±Ÿàÿ∂€Å'\n  Pred: '' | Gold: ''\nExamples evaluated: 8000\nExact Match: 32.67\nF1: 32.78\nEdit Distance (normalized): 33.08\n‚úÖ Step 8000: EM=32.67, F1=32.78, EditDist=33.08\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-8000 to Hub...\n‚úÖ Pushed checkpoint-8000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 9000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"üß™ Evaluating checkpoint: outputs/canine-s-uqa-filtered/checkpoint-9000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/25286 [00:00<?, ?samples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d9aa39321b448bfad7b0e07b941359c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nüîç Debug: Sample predictions:\n  Pred: '' | Gold: 'ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å'\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: '€Å€åŸà⁄Øÿ≥ ŸÖŸÅÿ±Ÿàÿ∂€Å'\n  Pred: '' | Gold: ''\nExamples evaluated: 8000\nExact Match: 32.73\nF1: 32.83\nEdit Distance (normalized): 33.12\n‚úÖ Step 9000: EM=32.73, F1=32.83, EditDist=33.12\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-9000 to Hub...\n‚úÖ Pushed checkpoint-9000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 10000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"üß™ Evaluating checkpoint: outputs/canine-s-uqa-filtered/checkpoint-10000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/25286 [00:00<?, ?samples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67c5f80cea5443d5b242feb16ca10d4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nüîç Debug: Sample predictions:\n  Pred: '' | Gold: 'ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å'\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: '€Å€åŸà⁄Øÿ≥ ŸÖŸÅÿ±Ÿàÿ∂€Å'\n  Pred: '' | Gold: ''\nExamples evaluated: 8000\nExact Match: 32.70\nF1: 32.78\nEdit Distance (normalized): 33.03\n‚úÖ Step 10000: EM=32.70, F1=32.78, EditDist=33.03\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-10000 to Hub...\n‚úÖ Pushed checkpoint-10000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 11000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"üß™ Evaluating checkpoint: outputs/canine-s-uqa-filtered/checkpoint-11000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/25286 [00:00<?, ?samples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fce0249cbaa4643a1e70c5c2eac8663"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nüîç Debug: Sample predictions:\n  Pred: '' | Gold: 'ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å'\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: '€Å€åŸà⁄Øÿ≥ ŸÖŸÅÿ±Ÿàÿ∂€Å'\n  Pred: '' | Gold: ''\nExamples evaluated: 8000\nExact Match: 32.75\nF1: 32.82\nEdit Distance (normalized): 33.04\n‚úÖ Step 11000: EM=32.75, F1=32.82, EditDist=33.04\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-11000 to Hub...\n‚úÖ Pushed checkpoint-11000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 12000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"üß™ Evaluating checkpoint: outputs/canine-s-uqa-filtered/checkpoint-12000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/25286 [00:00<?, ?samples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5ce7adce20e49d1a024fafa26a0a56a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nüîç Debug: Sample predictions:\n  Pred: '' | Gold: 'ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å'\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: '€Å€åŸà⁄Øÿ≥ ŸÖŸÅÿ±Ÿàÿ∂€Å'\n  Pred: '' | Gold: ''\nExamples evaluated: 8000\nExact Match: 32.84\nF1: 32.92\nEdit Distance (normalized): 33.12\n‚úÖ Step 12000: EM=32.84, F1=32.92, EditDist=33.12\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-12000 to Hub...\n‚úÖ Pushed checkpoint-12000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 13000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"üß™ Evaluating checkpoint: outputs/canine-s-uqa-filtered/checkpoint-13000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/25286 [00:00<?, ?samples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc12ec2fe6e6438e9037f60d88a5e754"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nüîç Debug: Sample predictions:\n  Pred: '' | Gold: 'ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å'\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: '€Å€åŸà⁄Øÿ≥ ŸÖŸÅÿ±Ÿàÿ∂€Å'\n  Pred: '' | Gold: ''\nExamples evaluated: 8000\nExact Match: 32.85\nF1: 32.93\nEdit Distance (normalized): 33.13\n‚úÖ Step 13000: EM=32.85, F1=32.93, EditDist=33.13\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-13000 to Hub...\n‚úÖ Pushed checkpoint-13000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 14000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"üß™ Evaluating checkpoint: outputs/canine-s-uqa-filtered/checkpoint-14000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/25286 [00:00<?, ?samples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b6b0b05a80444efa8da89d97cf45ef0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nüîç Debug: Sample predictions:\n  Pred: '' | Gold: 'ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å'\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: '€Å€åŸà⁄Øÿ≥ ŸÖŸÅÿ±Ÿàÿ∂€Å'\n  Pred: '' | Gold: ''\nExamples evaluated: 8000\nExact Match: 32.86\nF1: 32.95\nEdit Distance (normalized): 33.15\n‚úÖ Step 14000: EM=32.86, F1=32.95, EditDist=33.15\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-14000 to Hub...\n‚úÖ Pushed checkpoint-14000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 14599...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"üß™ Evaluating checkpoint: outputs/canine-s-uqa-filtered/checkpoint-14599\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/25286 [00:00<?, ?samples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6f06bbee1c9472ab3d51fbd653ca701"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nüîç Debug: Sample predictions:\n  Pred: '' | Gold: 'ŸÖÿ∫ÿ±ÿ®€å ⁄©€åŸæ ÿµŸàÿ®€Å'\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: ''\n  Pred: '' | Gold: '€Å€åŸà⁄Øÿ≥ ŸÖŸÅÿ±Ÿàÿ∂€Å'\n  Pred: '' | Gold: ''\nExamples evaluated: 8000\nExact Match: 32.86\nF1: 32.95\nEdit Distance (normalized): 33.14\n‚úÖ Step 14599: EM=32.86, F1=32.95, EditDist=33.14\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-14599 to Hub...\n‚úÖ Pushed checkpoint-14599 to Hub\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=14599, training_loss=3.851544821586533, metrics={'train_runtime': 14264.3846, 'train_samples_per_second': 16.375, 'train_steps_per_second': 1.023, 'total_flos': 5.77203549135575e+16, 'train_loss': 3.851544821586533, 'epoch': 1.0})"},"metadata":{}}],"execution_count":37},{"id":"29","cell_type":"markdown","source":"---","metadata":{}},{"id":"30","cell_type":"markdown","source":"### Diagnosing Preprocessing Functions!!!\n\nThese functions are just analysing the preprocessing logic above, they're just using the base model, NOT our trained model...","metadata":{"id":"cc44692c-6652-4cda-9ba4-8a03acdab88d"}},{"id":"31","cell_type":"code","source":"# # Diagnostic cell (fixed): Investigate preprocessing and truncation for many samples\n# import random\n# import pandas as pd\n# from transformers import AutoTokenizer\n\n# # Set display options to see full Urdu text\n# pd.set_option('display.max_colwidth', None)\n\n# try:\n#     tokenizer = AutoTokenizer.from_pretrained(\"google/canine-s\")\n# except Exception:\n#     tokenizer = None\n\n# num_samples = 20000  # Number of samples to check\n# results = []\n\n# for split_name, orig_data, proc_data in [\n#     (\"train\", uqa_train, processed_train),\n#     (\"val\", uqa_val, processed_val)\n# ]:\n#     # Sample random indices\n#     if len(proc_data) < num_samples:\n#         current_indices = range(len(proc_data))\n#     else:\n#         current_indices = random.sample(range(len(proc_data)), num_samples)\n\n#     for idx in current_indices:\n#         proc = proc_data[idx]\n#         # Use overflow_to_sample_mapping to get the correct original index\n#         orig_idx = proc[\"overflow_to_sample_mapping\"]\n#         orig = orig_data[orig_idx]\n\n#         input_ids = proc[\"input_ids\"]\n#         start_pos = proc[\"start_positions\"]\n#         end_pos = proc[\"end_positions\"]\n\n#         gold_answer = orig.get(\"gold_answer\", orig.get(\"answer\", \"\"))\n#         question = orig.get(\"question\", \"\")\n\n#         # Decode input_ids to text (for debugging context)\n#         if tokenizer:\n#             decoded_text = tokenizer.decode(input_ids, skip_special_tokens=False)\n#         else:\n#             decoded_text = str(input_ids)\n\n#         # Extract predicted answer span\n#         if 0 <= start_pos < len(input_ids) and 0 <= end_pos < len(input_ids):\n#             if tokenizer:\n#                 pred_span = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True)\n#             else:\n#                 pred_span = str(input_ids[start_pos:end_pos+1])\n#         else:\n#             pred_span = \"[CLS]\" # Represents no answer found in this chunk or invalid\n\n#         # Check if pred_span matches gold answer\n#         # We strip() to ignore minor whitespace differences\n#         pred_matches_gold = pred_span.strip() == gold_answer.strip()\n\n#         # Check if gold is even reachable in this chunk\n#         gold_in_decoded = gold_answer in decoded_text\n\n#         results.append({\n#             \"Split\": split_name,\n#             \"Question\": question,\n#             \"Gold Answer\": gold_answer,\n#             \"Extracted Answer\": pred_span,\n#             \"Match\": pred_matches_gold,\n#             \"Gold Reachable\": gold_in_decoded,\n#             \"orig_idx\": orig_idx\n#         })\n\n# # Create DataFrame\n# results_df = pd.DataFrame(results)\n\n# # --- SIDE BY SIDE COMPARISON ---\n\n# # 1. Filter for Solvable Mismatches (Gold was there, but we predicted wrong)\n# problem_cases = results_df[\n#     (results_df[\"Gold Reachable\"] == True) &\n#     (results_df[\"Match\"] == False)\n# ][[\"Question\", \"Gold Answer\", \"Extracted Answer\", \"Split\"]]\n\n# print(f\"üîç Checked {len(results_df)} samples.\")\n# print(f\"‚ùå Found {len(problem_cases)} cases where Gold was present but Extraction failed.\")\n\n# print(\"\\nüìä Side-by-Side Comparison (Top 20 Failures):\")\n# display(problem_cases.head(50))\n\n# print(\"\\n‚úÖ Side-by-Side Comparison (First 10 Rows - Mixed):\")\n# display(results_df[[\"Question\", \"Gold Answer\", \"Extracted Answer\", \"Match\"]].head(50))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"49f3717d","outputId":"38f435a4-1b55-4c2b-b6a5-86540fc23755","trusted":true,"execution":{"iopub.status.busy":"2025-12-12T17:52:21.826772Z","iopub.execute_input":"2025-12-12T17:52:21.827297Z","iopub.status.idle":"2025-12-12T17:52:21.832979Z","shell.execute_reply.started":"2025-12-12T17:52:21.827277Z","shell.execute_reply":"2025-12-12T17:52:21.832178Z"}},"outputs":[],"execution_count":38},{"id":"32","cell_type":"code","source":"# # Accuracy: fraction of rows where extracted answer matches gold answer\n# accuracy = (results_df[\"Match\"]).mean()\n\n# # Precision: among rows where extracted answer is non-empty, fraction that matches gold\n# # We filter out cases where the model predicted nothing (empty string) or just whitespace\n# non_empty_pred = results_df[\"Extracted Answer\"].str.strip() != \"\"\n\n# # Avoid division by zero if no predictions were made\n# if non_empty_pred.sum() > 0:\n#     precision = (results_df[\"Match\"] & non_empty_pred).sum() / non_empty_pred.sum()\n# else:\n#     precision = 0.0\n\n# print(f\"Accuracy: {accuracy:.3f}\")\n# print(f\"Precision: {precision:.3f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e67abc12","outputId":"c597ec41-a56e-4e5d-9eb6-e71bd0eafd38","trusted":true,"execution":{"iopub.status.busy":"2025-12-12T17:52:21.833967Z","iopub.execute_input":"2025-12-12T17:52:21.835152Z","iopub.status.idle":"2025-12-12T17:52:21.853265Z","shell.execute_reply.started":"2025-12-12T17:52:21.835133Z","shell.execute_reply":"2025-12-12T17:52:21.852455Z"}},"outputs":[],"execution_count":39}]}