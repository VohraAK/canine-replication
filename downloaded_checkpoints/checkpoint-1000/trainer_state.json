{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.1195564455868727,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0029889111396718174,
      "grad_norm": 4.618643283843994,
      "learning_rate": 2.9913927077106994e-05,
      "loss": 5.7589,
      "step": 25
    },
    {
      "epoch": 0.005977822279343635,
      "grad_norm": 4.617073059082031,
      "learning_rate": 2.982426778242678e-05,
      "loss": 5.7011,
      "step": 50
    },
    {
      "epoch": 0.008966733419015452,
      "grad_norm": 4.3329386711120605,
      "learning_rate": 2.9734608487746562e-05,
      "loss": 5.6459,
      "step": 75
    },
    {
      "epoch": 0.01195564455868727,
      "grad_norm": 4.009007453918457,
      "learning_rate": 2.964494919306635e-05,
      "loss": 5.5872,
      "step": 100
    },
    {
      "epoch": 0.014944555698359088,
      "grad_norm": 3.810464859008789,
      "learning_rate": 2.9555289898386134e-05,
      "loss": 5.5216,
      "step": 125
    },
    {
      "epoch": 0.017933466838030904,
      "grad_norm": 4.306461334228516,
      "learning_rate": 2.946563060370592e-05,
      "loss": 5.4687,
      "step": 150
    },
    {
      "epoch": 0.020922377977702723,
      "grad_norm": 4.688884258270264,
      "learning_rate": 2.9375971309025702e-05,
      "loss": 5.4111,
      "step": 175
    },
    {
      "epoch": 0.02391128911737454,
      "grad_norm": 4.547159194946289,
      "learning_rate": 2.9286312014345486e-05,
      "loss": 5.3537,
      "step": 200
    },
    {
      "epoch": 0.02690020025704636,
      "grad_norm": 4.1249470710754395,
      "learning_rate": 2.9196652719665274e-05,
      "loss": 5.3054,
      "step": 225
    },
    {
      "epoch": 0.029889111396718175,
      "grad_norm": 4.418153285980225,
      "learning_rate": 2.9106993424985058e-05,
      "loss": 5.2477,
      "step": 250
    },
    {
      "epoch": 0.032878022536389995,
      "grad_norm": 3.69183087348938,
      "learning_rate": 2.9017334130304842e-05,
      "loss": 5.1665,
      "step": 275
    },
    {
      "epoch": 0.03586693367606181,
      "grad_norm": 4.324131965637207,
      "learning_rate": 2.8927674835624626e-05,
      "loss": 5.1251,
      "step": 300
    },
    {
      "epoch": 0.03885584481573363,
      "grad_norm": 4.295875072479248,
      "learning_rate": 2.8838015540944414e-05,
      "loss": 5.0749,
      "step": 325
    },
    {
      "epoch": 0.04184475595540545,
      "grad_norm": 4.111362934112549,
      "learning_rate": 2.8748356246264198e-05,
      "loss": 5.0181,
      "step": 350
    },
    {
      "epoch": 0.044833667095077266,
      "grad_norm": 4.110341548919678,
      "learning_rate": 2.865869695158398e-05,
      "loss": 4.9662,
      "step": 375
    },
    {
      "epoch": 0.04782257823474908,
      "grad_norm": 4.222002029418945,
      "learning_rate": 2.8569037656903766e-05,
      "loss": 4.9208,
      "step": 400
    },
    {
      "epoch": 0.0508114893744209,
      "grad_norm": 4.266299724578857,
      "learning_rate": 2.847937836222355e-05,
      "loss": 4.8576,
      "step": 425
    },
    {
      "epoch": 0.05380040051409272,
      "grad_norm": 4.102354049682617,
      "learning_rate": 2.8389719067543338e-05,
      "loss": 4.8143,
      "step": 450
    },
    {
      "epoch": 0.05678931165376453,
      "grad_norm": 4.059512138366699,
      "learning_rate": 2.8300059772863122e-05,
      "loss": 4.7942,
      "step": 475
    },
    {
      "epoch": 0.05977822279343635,
      "grad_norm": 3.8011794090270996,
      "learning_rate": 2.8210400478182906e-05,
      "loss": 4.7242,
      "step": 500
    },
    {
      "epoch": 0.06276713393310816,
      "grad_norm": 4.370105743408203,
      "learning_rate": 2.812074118350269e-05,
      "loss": 4.6547,
      "step": 525
    },
    {
      "epoch": 0.06575604507277999,
      "grad_norm": 3.9886491298675537,
      "learning_rate": 2.8031081888822474e-05,
      "loss": 4.6253,
      "step": 550
    },
    {
      "epoch": 0.0687449562124518,
      "grad_norm": 3.971111536026001,
      "learning_rate": 2.7941422594142262e-05,
      "loss": 4.6008,
      "step": 575
    },
    {
      "epoch": 0.07173386735212361,
      "grad_norm": 4.014000415802002,
      "learning_rate": 2.7851763299462043e-05,
      "loss": 4.5092,
      "step": 600
    },
    {
      "epoch": 0.07472277849179544,
      "grad_norm": 3.988618850708008,
      "learning_rate": 2.776210400478183e-05,
      "loss": 4.4605,
      "step": 625
    },
    {
      "epoch": 0.07771168963146725,
      "grad_norm": 3.8805654048919678,
      "learning_rate": 2.7672444710101614e-05,
      "loss": 4.4344,
      "step": 650
    },
    {
      "epoch": 0.08070060077113908,
      "grad_norm": 3.750361919403076,
      "learning_rate": 2.7582785415421402e-05,
      "loss": 4.3845,
      "step": 675
    },
    {
      "epoch": 0.0836895119108109,
      "grad_norm": 3.698408842086792,
      "learning_rate": 2.7493126120741183e-05,
      "loss": 4.3363,
      "step": 700
    },
    {
      "epoch": 0.0866784230504827,
      "grad_norm": 3.6642539501190186,
      "learning_rate": 2.7403466826060967e-05,
      "loss": 4.2885,
      "step": 725
    },
    {
      "epoch": 0.08966733419015453,
      "grad_norm": 3.4553029537200928,
      "learning_rate": 2.7313807531380754e-05,
      "loss": 4.239,
      "step": 750
    },
    {
      "epoch": 0.09265624532982634,
      "grad_norm": 3.7352263927459717,
      "learning_rate": 2.722414823670054e-05,
      "loss": 4.1707,
      "step": 775
    },
    {
      "epoch": 0.09564515646949816,
      "grad_norm": 3.5851099491119385,
      "learning_rate": 2.7134488942020326e-05,
      "loss": 4.1704,
      "step": 800
    },
    {
      "epoch": 0.09863406760916998,
      "grad_norm": 3.6672122478485107,
      "learning_rate": 2.7044829647340107e-05,
      "loss": 4.125,
      "step": 825
    },
    {
      "epoch": 0.1016229787488418,
      "grad_norm": 3.724611282348633,
      "learning_rate": 2.6955170352659894e-05,
      "loss": 4.0838,
      "step": 850
    },
    {
      "epoch": 0.10461188988851361,
      "grad_norm": 3.461249351501465,
      "learning_rate": 2.686551105797968e-05,
      "loss": 4.0181,
      "step": 875
    },
    {
      "epoch": 0.10760080102818544,
      "grad_norm": 3.22706937789917,
      "learning_rate": 2.6775851763299463e-05,
      "loss": 3.9945,
      "step": 900
    },
    {
      "epoch": 0.11058971216785725,
      "grad_norm": 3.283191680908203,
      "learning_rate": 2.6686192468619247e-05,
      "loss": 3.9454,
      "step": 925
    },
    {
      "epoch": 0.11357862330752906,
      "grad_norm": 3.16184139251709,
      "learning_rate": 2.659653317393903e-05,
      "loss": 3.8738,
      "step": 950
    },
    {
      "epoch": 0.11656753444720089,
      "grad_norm": 3.5615744590759277,
      "learning_rate": 2.650687387925882e-05,
      "loss": 3.8927,
      "step": 975
    },
    {
      "epoch": 0.1195564455868727,
      "grad_norm": 3.0512897968292236,
      "learning_rate": 2.6417214584578603e-05,
      "loss": 3.859,
      "step": 1000
    }
  ],
  "logging_steps": 25,
  "max_steps": 8365,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3953736695808000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
