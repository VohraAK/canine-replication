{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c186240c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:26:56.618242Z",
     "iopub.status.busy": "2025-11-21T11:26:56.617987Z",
     "iopub.status.idle": "2025-11-21T11:28:25.584179Z",
     "shell.execute_reply": "2025-11-21T11:28:25.583238Z",
     "shell.execute_reply.started": "2025-11-21T11:26:56.618214Z"
    },
    "id": "c186240c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install peft evaluate transformers Levenshtein ipywidgets\n",
    "%pip install protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8da8ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:28:25.586356Z",
     "iopub.status.busy": "2025-11-21T11:28:25.586117Z",
     "iopub.status.idle": "2025-11-21T11:28:25.590896Z",
     "shell.execute_reply": "2025-11-21T11:28:25.590100Z",
     "shell.execute_reply.started": "2025-11-21T11:28:25.586329Z"
    },
    "id": "cd8da8ab",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_DISABLE_CHAT_TEMPLATES\"] = \"1\"\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADDITIONAL_CHAT_TEMPLATES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87eba82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T11:28:25.592114Z",
     "iopub.status.busy": "2025-11-21T11:28:25.591640Z"
    },
    "id": "d87eba82",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "# from UQA.canine_utils import preprocess_uqa, lora_config, print_trainable_parameters, normalize_answer, exact_match_score, f1_score, edit_distance_score, gold_answer, decode_prediction\n",
    "from transformers import CanineTokenizer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
    "import json\n",
    "from huggingface_hub import HfApi, notebook_login, whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e98cebe-4c08-4850-b3c1-1529564fdb1b",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "notebook_login()\n",
    "# whoami()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d474e2e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:05:07.014709Z",
     "iopub.status.busy": "2025-11-20T10:05:07.014446Z",
     "iopub.status.idle": "2025-11-20T10:05:08.052550Z",
     "shell.execute_reply": "2025-11-20T10:05:08.051963Z",
     "shell.execute_reply.started": "2025-11-20T10:05:07.014691Z"
    },
    "id": "d474e2e8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "uqa_dataset = load_dataset(\"uqa/UQA\")\n",
    "uqa_train = uqa_dataset[\"train\"].shuffle(seed=42).select(range(40000))\n",
    "uqa_val = uqa_dataset[\"validation\"].shuffle(seed=42).select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd5a40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-20T10:05:08.053463Z",
     "iopub.status.busy": "2025-11-20T10:05:08.053223Z",
     "iopub.status.idle": "2025-11-20T10:05:09.441212Z",
     "shell.execute_reply": "2025-11-20T10:05:09.440563Z",
     "shell.execute_reply.started": "2025-11-20T10:05:08.053446Z"
    },
    "id": "f2dd5a40",
    "outputId": "4ea979ff-002f-491f-bbb3-debb423da2b6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import CanineTokenizer, CanineForQuestionAnswering\n",
    "import torch\n",
    "model_name = 'google/canine-s'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "tokenizer = CanineTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=False)\n",
    "model = CanineForQuestionAnswering.from_pretrained(model_name, trust_remote_code=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d8765",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:05:09.443136Z",
     "iopub.status.busy": "2025-11-20T10:05:09.442945Z",
     "iopub.status.idle": "2025-11-20T10:05:09.456875Z",
     "shell.execute_reply": "2025-11-20T10:05:09.456152Z",
     "shell.execute_reply.started": "2025-11-20T10:05:09.443121Z"
    },
    "id": "438d8765",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# preprocessors\n",
    "MAX_SEQ_LENGTH = 384\n",
    "DOC_STRIDE = 64\n",
    "\n",
    "def _build_byte_to_char_index(text):\n",
    "    cumulative = [0]\n",
    "    for char in text:\n",
    "        cumulative.append(cumulative[-1] + len(char.encode(\"utf-8\")))\n",
    "    return cumulative\n",
    "\n",
    "def _byte_to_char(cumulative_bytes, byte_index):\n",
    "    from bisect import bisect_right\n",
    "    position = bisect_right(cumulative_bytes, byte_index) - 1\n",
    "    return max(position, 0)\n",
    "\n",
    "# Safe preprocessing: enforce tokenizer/model limits\n",
    "def preprocess_uqa(examples, tokenizer, max_length=MAX_SEQ_LENGTH, doc_stride=DOC_STRIDE, model_obj=None):\n",
    "    # compute global allowed max (use tokenizer/model if available)\n",
    "    tokenizer_max = getattr(tokenizer, \"model_max_length\", max_length)\n",
    "    model_max = getattr(model_obj.config, \"max_position_embeddings\", None) if model_obj is not None else None\n",
    "    # choose the smallest of the configured limits\n",
    "    max_allowed = max_length\n",
    "    if tokenizer_max is not None and tokenizer_max > 0:\n",
    "        max_allowed = min(max_allowed, tokenizer_max)\n",
    "    if model_max is not None and model_max > 0:\n",
    "        max_allowed = min(max_allowed, model_max)\n",
    "\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    contexts = examples[\"context\"]\n",
    "    answers = examples[\"answer\"]\n",
    "    answer_starts = examples[\"answer_start\"]\n",
    "    special_tokens = tokenizer.num_special_tokens_to_add(pair=True)\n",
    "\n",
    "    encoded = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": [],\n",
    "               \"start_positions\": [], \"end_positions\": [], \"overflow_to_sample_mapping\": []}\n",
    "\n",
    "    for example_idx, (question, context, answer, answer_start) in enumerate(zip(questions, contexts, answers, answer_starts)):\n",
    "        question_tokens = tokenizer.encode(question, add_special_tokens=False)\n",
    "        context_tokens = tokenizer.encode(context, add_special_tokens=False)\n",
    "\n",
    "        # compute how many context tokens we can include (reserve special + question)\n",
    "        max_context_tokens = max_allowed - len(question_tokens) - special_tokens\n",
    "        if max_context_tokens <= 0 or not context_tokens:\n",
    "            # skip or emit a short feature that points to CLS\n",
    "            continue\n",
    "\n",
    "        # rest of function unchanged but using max_context_tokens (same as before)\n",
    "        stride_tokens = max_context_tokens - doc_stride\n",
    "        if stride_tokens <= 0:\n",
    "            stride_tokens = max_context_tokens\n",
    "        span_start = 0\n",
    "        context_length = len(context_tokens)\n",
    "        while span_start < context_length:\n",
    "            span_end = min(span_start + max_context_tokens, context_length)\n",
    "            context_chunk = context_tokens[span_start:span_end]\n",
    "            input_ids = tokenizer.build_inputs_with_special_tokens(question_tokens, context_chunk)\n",
    "            token_type_ids = tokenizer.create_token_type_ids_from_sequences(question_tokens, context_chunk)\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "            context_offset = len(input_ids) - len(context_chunk) - 1\n",
    "\n",
    "            if answer and answer_start != -1:\n",
    "                byte_map = _build_byte_to_char_index(context)\n",
    "                start_char = _byte_to_char(byte_map, answer_start)\n",
    "                end_char = _byte_to_char(byte_map, max(answer_start + len(answer) - 1, answer_start))\n",
    "                answer_span = (start_char, end_char)\n",
    "                start_char, end_char = answer_span\n",
    "                answer_in_chunk = start_char >= span_start and end_char < span_end\n",
    "                if answer_in_chunk:\n",
    "                    start_pos = context_offset + (start_char - span_start)\n",
    "                    end_pos = context_offset + (end_char - span_start)\n",
    "                else:\n",
    "                    start_pos = cls_index\n",
    "                    end_pos = cls_index\n",
    "            else:\n",
    "                start_pos = cls_index\n",
    "                end_pos = cls_index\n",
    "\n",
    "            # ensure final length <= max_allowed by truncating if necessary\n",
    "            if len(input_ids) > max_allowed:\n",
    "                input_ids = input_ids[:max_allowed]\n",
    "                attention_mask = attention_mask[:max_allowed]\n",
    "                token_type_ids = token_type_ids[:max_allowed]\n",
    "                # if start/end fall outside, point to CLS\n",
    "                if start_pos >= max_allowed or end_pos >= max_allowed:\n",
    "                    start_pos = cls_index\n",
    "                    end_pos = cls_index\n",
    "\n",
    "            padding = max_allowed - len(input_ids)\n",
    "            if padding > 0:\n",
    "                pad_id = tokenizer.pad_token_id\n",
    "                input_ids += [pad_id] * padding\n",
    "                attention_mask += [0] * padding\n",
    "                token_type_ids += [0] * padding\n",
    "\n",
    "            encoded[\"input_ids\"].append(input_ids)\n",
    "            encoded[\"attention_mask\"].append(attention_mask)\n",
    "            encoded[\"token_type_ids\"].append(token_type_ids)\n",
    "            encoded[\"start_positions\"].append(start_pos)\n",
    "            encoded[\"end_positions\"].append(end_pos)\n",
    "            encoded[\"overflow_to_sample_mapping\"].append(example_idx)\n",
    "\n",
    "            if span_end == context_length:\n",
    "                break\n",
    "            span_start += stride_tokens\n",
    "\n",
    "    return encoded\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e95eec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:05:09.457795Z",
     "iopub.status.busy": "2025-11-20T10:05:09.457537Z",
     "iopub.status.idle": "2025-11-20T10:05:09.472873Z",
     "shell.execute_reply": "2025-11-20T10:05:09.472105Z",
     "shell.execute_reply.started": "2025-11-20T10:05:09.457770Z"
    },
    "id": "a3e95eec",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.QUESTION_ANS,\n",
    "    r=32,   # changed from 8\n",
    "    lora_alpha=64,  # changed from 32\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\", \"key\"],   # added key, output.dense\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"qa_outputs\"],\n",
    ")\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11807b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T10:05:09.473989Z",
     "iopub.status.busy": "2025-11-20T10:05:09.473735Z"
    },
    "id": "d11807b9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# preprocess the train and val splits\n",
    "processed_train = uqa_train.map(lambda examples: preprocess_uqa(examples, tokenizer), batched=True, remove_columns=uqa_train.column_names)\n",
    "processed_val = uqa_val.map(lambda examples: preprocess_uqa(examples, tokenizer), batched=True, remove_columns=uqa_val.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D-emFQTIaZRL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-emFQTIaZRL",
    "outputId": "eece57cd-d7ed-4931-dda0-b7a34d9c6e95",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "processed_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yy3SiWwCabEi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yy3SiWwCabEi",
    "outputId": "52db5812-b46b-4a23-8781-bec0571d73c2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "processed_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ecdd17",
   "metadata": {
    "id": "77ecdd17",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "processed_train.save_to_disk(\"/kaggle/working/cache/processed_train_uqa\")\n",
    "processed_val.save_to_disk(\"/kaggle/working/cache/processed_val_uqa\")   # cached it\n",
    "\n",
    "\n",
    "processed_train = load_from_disk(\"/kaggle/working/cache/processed_train_uqa\")\n",
    "processed_val = load_from_disk(\"/kaggle/working/cache/processed_val_uqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e06e6b",
   "metadata": {
    "id": "c0e06e6b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9eeeed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba9eeeed",
    "outputId": "e987b70d-b2a2-4618-d538-578bff1909bb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# build LoRA model\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.gradient_checkpointing_enable()\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c5c7a7",
   "metadata": {
    "id": "61c5c7a7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# evals\n",
    "\n",
    "\n",
    "def normalize_answer(text):\n",
    "    text = (text or \"\").lower()\n",
    "    def remove_articles(s):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n",
    "    def remove_punctuation(s):\n",
    "        return \"\".join(ch for ch in s if ch not in string.punctuation)\n",
    "    def white_space_fix(s):\n",
    "        return \" \".join(s.split())\n",
    "    return white_space_fix(remove_articles(remove_punctuation(text)))\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    gold_tokens = normalize_answer(ground_truth).split()\n",
    "    if not gold_tokens:\n",
    "        return 1.0 if not pred_tokens else 0.0\n",
    "    if not pred_tokens:\n",
    "        return 0.0\n",
    "    common = Counter(pred_tokens) & Counter(gold_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gold_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def edit_distance_score(prediction, ground_truth):\n",
    "    pred_norm = normalize_answer(prediction)\n",
    "    gold_norm = normalize_answer(ground_truth)\n",
    "    if not gold_norm and not pred_norm:\n",
    "        return 1.0\n",
    "    if not gold_norm or not pred_norm:\n",
    "        return 0.0\n",
    "    distance = Levenshtein.distance(pred_norm, gold_norm)\n",
    "    max_len = max(len(pred_norm), len(gold_norm))\n",
    "    return 1.0 - (distance / max_len) if max_len > 0 else 1.0\n",
    "\n",
    "def gold_answer(example):\n",
    "    # Extracts the gold answer substring from the context using character offsets\n",
    "    answer = example.get(\"answer\")\n",
    "    context = example.get(\"context\")\n",
    "    answer_start = example.get(\"answer_start\", -1)\n",
    "    if answer and answer_start is not None and answer_start != -1:\n",
    "        return context[answer_start: answer_start + len(answer)]\n",
    "    return \"[CLS]\"\n",
    "\n",
    "\n",
    "def decode_prediction(input_ids, start_idx, end_idx, tokenizer=None):\n",
    "    if start_idx > end_idx:\n",
    "        start_idx, end_idx = end_idx, start_idx\n",
    "    if tokenizer is None:\n",
    "        raise ValueError(\"Tokenizer must be provided for decoding.\")\n",
    "    cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "    # If both point to CLS token, return [CLS] sentinel\n",
    "    if start_idx == cls_index and end_idx == cls_index:\n",
    "        return \"[CLS]\"\n",
    "    start_idx = max(start_idx, 0)\n",
    "    end_idx = min(end_idx, len(input_ids) - 1)\n",
    "    if start_idx > end_idx:\n",
    "        return \"[CLS]\"\n",
    "    text = tokenizer.decode(input_ids[start_idx:end_idx + 1], skip_special_tokens=True)\n",
    "    text = text.strip()\n",
    "    return text if text else \"[CLS]\"\n",
    "\n",
    "\n",
    "def evaluate_checkpoint(checkpoint_path=None, model_instance=None, eval_dataset=None):\n",
    "    \"\"\"Evaluate either a checkpoint path (loads model) or a provided model instance.\n",
    "\n",
    "    - checkpoint_path: path to checkpoint folder\n",
    "    - model_instance: an in-memory model (preferably a PeftModel or CanineForQuestionAnswering)\n",
    "    - eval_dataset: optional dataset to evaluate; if None the default processed_val will be used\n",
    "    \"\"\"\n",
    "    if eval_dataset is None:\n",
    "        eval_dataset = processed_val\n",
    "\n",
    "    # If a model_instance is given, use it directly (avoid re-loading a fresh base model)\n",
    "    if model_instance is not None:\n",
    "        eval_model = model_instance\n",
    "    else:\n",
    "        base_model = CanineForQuestionAnswering.from_pretrained(model_name, trust_remote_code=False)\n",
    "        eval_model = get_peft_model(base_model, lora_config)\n",
    "        # Try loading adapter weights; fall back to PeftModel.from_pretrained if needed\n",
    "        try:\n",
    "            eval_model.load_adapter(checkpoint_path)\n",
    "        except Exception:\n",
    "            from peft import PeftModel\n",
    "            eval_model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "\n",
    "    eval_model.to(device)\n",
    "\n",
    "    eval_args = TrainingArguments(\n",
    "        # Small evaluation config; uses cpu/mps if no gpu during eval\n",
    "        output_dir=\"outputs/canine-s-uqa\",\n",
    "        per_device_eval_batch_size=16,\n",
    "        dataloader_drop_last=False,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # Run evaluation via a lightweight Trainer so prediction loop is standard\n",
    "    eval_trainer = Trainer(\n",
    "        model=eval_model,\n",
    "        args=eval_args,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    predictions = eval_trainer.predict(eval_dataset)\n",
    "    start_logits, end_logits = predictions.predictions\n",
    "    best_predictions = {}\n",
    "    for feature_index, feature in enumerate(eval_dataset):\n",
    "        sample_idx = int(feature[\"overflow_to_sample_mapping\"])\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        start_idx = int(np.argmax(start_logits[feature_index]))\n",
    "        end_idx = int(np.argmax(end_logits[feature_index]))\n",
    "        score = float(start_logits[feature_index][start_idx] + end_logits[feature_index][end_idx])\n",
    "        prediction_text = decode_prediction(input_ids, start_idx, end_idx, tokenizer=tokenizer)\n",
    "        stored = best_predictions.get(sample_idx)\n",
    "        if stored is None or score > stored[0]:\n",
    "            best_predictions[sample_idx] = (score, prediction_text)\n",
    "\n",
    "    em_scores = []\n",
    "    f1_scores = []\n",
    "    edit_dist_scores = []\n",
    "    for sample_idx, (_, prediction_text) in best_predictions.items():\n",
    "        reference = gold_answer(uqa_val[int(sample_idx)])\n",
    "        em_scores.append(exact_match_score(prediction_text, reference))\n",
    "        f1_scores.append(f1_score(prediction_text, reference))\n",
    "        edit_dist_scores.append(edit_distance_score(prediction_text, reference))\n",
    "\n",
    "    em = float(np.mean(em_scores)) if em_scores else 0.0\n",
    "    f1 = float(np.mean(f1_scores)) if f1_scores else 0.0\n",
    "    edit_dist = float(np.mean(edit_dist_scores)) if edit_dist_scores else 0.0\n",
    "    print(f\"Examples evaluated: {len(em_scores)}\")\n",
    "    print(f\"Exact Match: {em * 100:.2f}\")\n",
    "    print(f\"F1: {f1 * 100:.2f}\")\n",
    "    print(f\"Edit Distance (normalized): {edit_dist * 100:.2f}\")\n",
    "    return {\"exact_match\": em, \"f1\": f1, \"edit_distance\": edit_dist}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4abaaab",
   "metadata": {
    "id": "c4abaaab",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/canine-s-uqa\",\n",
    "    \n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    \n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    num_train_epochs=1,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"no\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    logging_steps=25,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"VohraAK/canine-s-uqa\",\n",
    "    hub_strategy=\"checkpoint\",\n",
    "    )\n",
    "\n",
    "class CustomEvalCallback(TrainerCallback):\n",
    "    def __init__(self, eval_func, eval_dataset, use_in_memory_model=True, verbose=True):\n",
    "        self.eval_func = eval_func\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.use_in_memory_model = use_in_memory_model\n",
    "        self.verbose = verbose\n",
    "        # trainer reference (set after trainer exists)\n",
    "        self.trainer = None\n",
    "\n",
    "    def on_save(self, args, state, control, model=None, **kwargs):\n",
    "        checkpoint_path = f\"{args.output_dir}/checkpoint-{state.global_step}\"\n",
    "        if self.verbose:\n",
    "            print(f\"\\nüîç Running custom evaluation at step {state.global_step}...\")\n",
    "\n",
    "        # Prefer evaluating the in-memory trainer model (fast + avoids re-loading)\n",
    "        if self.use_in_memory_model and self.trainer is not None:\n",
    "            if self.verbose:\n",
    "                print(\"Using in-memory model for evaluation (no reloading).\")\n",
    "            try:\n",
    "                metrics = self.eval_func(checkpoint_path=None, model_instance=self.trainer.model, eval_dataset=self.eval_dataset)\n",
    "            except Exception as e:\n",
    "                print(\"‚ö†Ô∏è in-memory evaluation failed, falling back to checkpoint load:\", e)\n",
    "                metrics = self.eval_func(checkpoint_path)\n",
    "        else:\n",
    "            metrics = self.eval_func(checkpoint_path)\n",
    "\n",
    "        # record metrics in state.log_history\n",
    "        state.log_history.append({\n",
    "            \"step\": state.global_step,\n",
    "            \"eval_exact_match\": metrics.get(\"exact_match\"),\n",
    "            \"eval_f1\": metrics.get(\"f1\"),\n",
    "            \"eval_edit_distance\": metrics.get(\"edit_distance\"),\n",
    "        })\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"‚úÖ Step {state.global_step}: EM={metrics.get('exact_match',0)*100:.2f}, F1={metrics.get('f1',0)*100:.2f}, EditDist={metrics.get('edit_distance',0)*100:.2f}\")\n",
    "\n",
    "        # Update trainer_state.json to include custom metrics\n",
    "        state_path = f\"{checkpoint_path}/trainer_state.json\"\n",
    "        try:\n",
    "            with open(state_path, 'r') as f:\n",
    "                state_dict = json.load(f)\n",
    "            state_dict['log_history'] = state.log_history\n",
    "            with open(state_path, 'w') as f:\n",
    "                json.dump(state_dict, f, indent=2)\n",
    "            if self.verbose:\n",
    "                print(f\"üíæ Updated trainer_state.json with custom metrics\")\n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                print(f\"‚ö†Ô∏è  Warning: Could not update trainer_state.json: {e}\")\n",
    "\n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(f\"‚òÅÔ∏è  Pushing checkpoint-{state.global_step} to Hub...\")\n",
    "            api = HfApi()\n",
    "            api.upload_folder(\n",
    "                folder_path=checkpoint_path,\n",
    "                repo_id=args.hub_model_id,\n",
    "                path_in_repo=f\"checkpoint-{state.global_step}\",\n",
    "                commit_message=f\"Add checkpoint {state.global_step} (EM={metrics.get('exact_match',0)*100:.1f}%, F1={metrics.get('f1',0)*100:.1f}%)\",\n",
    "                repo_type=\"model\"\n",
    "            )\n",
    "            if self.verbose:\n",
    "                print(f\"‚úÖ Pushed checkpoint-{state.global_step} to Hub\")\n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                print(f\"‚ö†Ô∏è  Warning: Could not push to Hub: {e}\")\n",
    "\n",
    "        return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f5dda",
   "metadata": {
    "id": "055f5dda",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer_cb = CustomEvalCallback(evaluate_checkpoint, processed_val, use_in_memory_model=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_train,\n",
    "    eval_dataset=processed_val,\n",
    "    callbacks=[trainer_cb],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TOUimesUX5Re",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TOUimesUX5Re",
    "outputId": "c0ada6de-de08-41ce-e8fc-5c59232de758",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
