{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c186240c","cell_type":"code","source":"# %pip install peft evaluate transformers Levenshtein ipywidgets\n# %pip install protobuf==3.20.3\n# !rm -rf /kaggle/working/cache","metadata":{"execution":{"iopub.status.busy":"2025-11-25T10:01:41.749272Z","iopub.execute_input":"2025-11-25T10:01:41.749873Z","iopub.status.idle":"2025-11-25T10:01:41.753208Z","shell.execute_reply.started":"2025-11-25T10:01:41.749845Z","shell.execute_reply":"2025-11-25T10:01:41.752383Z"},"id":"c186240c","trusted":true},"outputs":[],"execution_count":11},{"id":"cd8da8ab","cell_type":"code","source":"import os\nos.environ[\"TRANSFORMERS_DISABLE_CHAT_TEMPLATES\"] = \"1\"\nos.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\nos.environ[\"TRANSFORMERS_NO_ADDITIONAL_CHAT_TEMPLATES\"] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2025-11-25T10:01:41.754373Z","iopub.execute_input":"2025-11-25T10:01:41.754744Z","iopub.status.idle":"2025-11-25T10:01:41.766413Z","shell.execute_reply.started":"2025-11-25T10:01:41.754723Z","shell.execute_reply":"2025-11-25T10:01:41.765542Z"},"id":"cd8da8ab","trusted":true},"outputs":[],"execution_count":12},{"id":"d87eba82","cell_type":"code","source":"from datasets import load_dataset, load_from_disk\n# from UQA.canine_utils import preprocess_uqa, lora_config, print_trainable_parameters, normalize_answer, exact_match_score, f1_score, edit_distance_score, gold_answer, decode_prediction\nfrom transformers import CanineTokenizer\nfrom peft import LoraConfig, TaskType, get_peft_model\nimport re\nimport string\nfrom collections import Counter\nimport numpy as np\nimport Levenshtein\n\nfrom transformers import TrainingArguments, Trainer, TrainerCallback\nimport json\nfrom huggingface_hub import HfApi, notebook_login, whoami","metadata":{"execution":{"iopub.status.busy":"2025-11-25T10:01:41.767717Z","iopub.execute_input":"2025-11-25T10:01:41.768259Z","iopub.status.idle":"2025-11-25T10:01:41.779869Z","shell.execute_reply.started":"2025-11-25T10:01:41.768235Z","shell.execute_reply":"2025-11-25T10:01:41.779102Z"},"id":"d87eba82","trusted":true},"outputs":[],"execution_count":13},{"id":"0e98cebe-4c08-4850-b3c1-1529564fdb1b","cell_type":"code","source":"\n# notebook_login()\n# whoami()","metadata":{"execution":{"iopub.status.busy":"2025-11-25T10:01:41.780490Z","iopub.execute_input":"2025-11-25T10:01:41.780714Z","iopub.status.idle":"2025-11-25T10:01:41.791632Z","shell.execute_reply.started":"2025-11-25T10:01:41.780696Z","shell.execute_reply":"2025-11-25T10:01:41.790867Z"},"trusted":true},"outputs":[],"execution_count":14},{"id":"f2dd5a40","cell_type":"code","source":"from transformers import CanineTokenizer, CanineForQuestionAnswering\nimport torch\nmodel_name = 'google/canine-s'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n\ntokenizer = CanineTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=False)\nmodel = CanineForQuestionAnswering.from_pretrained(model_name, trust_remote_code=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-11-25T10:01:41.793514Z","iopub.execute_input":"2025-11-25T10:01:41.793835Z","iopub.status.idle":"2025-11-25T10:01:43.874179Z","shell.execute_reply.started":"2025-11-25T10:01:41.793809Z","shell.execute_reply":"2025-11-25T10:01:43.873521Z"},"id":"f2dd5a40","outputId":"4ea979ff-002f-491f-bbb3-debb423da2b6","trusted":true},"outputs":[{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":15},{"id":"d474e2e8","cell_type":"code","source":"uqa_dataset = load_dataset(\"uqa/UQA\")\nuqa_train = uqa_dataset[\"train\"].shuffle(seed=42).select(range(40000))\nuqa_val = uqa_dataset[\"validation\"].shuffle(seed=42).select(range(10000))","metadata":{"execution":{"iopub.status.busy":"2025-11-25T10:01:43.875005Z","iopub.execute_input":"2025-11-25T10:01:43.875226Z","iopub.status.idle":"2025-11-25T10:01:45.718520Z","shell.execute_reply.started":"2025-11-25T10:01:43.875206Z","shell.execute_reply":"2025-11-25T10:01:45.717914Z"},"id":"d474e2e8","trusted":true},"outputs":[],"execution_count":16},{"id":"89c472d5","cell_type":"markdown","source":"---","metadata":{}},{"id":"6e80a8d3","cell_type":"markdown","source":"## Updated preprocessors!\n\nPreviously, we tried to apply the same approach we used in TYDIQA on UQA, the problem was the preprocessors were aligning the answer spans in units of **byte-level spans** instead of **character-level spans**. The calculations were adding byte-level offsets to the answer lengths, and since Urdu characters may be quantified in multiple bytes, the model was being fed the wrong spans -> GIGO!","metadata":{}},{"id":"438d8765","cell_type":"code","source":"MAX_SEQ_LENGTH = 384\nDOC_STRIDE = 64 \n\ndef preprocess_uqa(examples, tokenizer, max_length=MAX_SEQ_LENGTH, doc_stride=DOC_STRIDE, model_obj=None, indices=None):\n    # Handle tokenizer/model limits safely\n    tokenizer_max = getattr(tokenizer, \"model_max_length\", max_length)\n    model_max = getattr(model_obj.config, \"max_position_embeddings\", None) if model_obj is not None else None\n    max_allowed = max_length\n    if tokenizer_max is not None and tokenizer_max > 0:\n        max_allowed = min(max_allowed, tokenizer_max)\n    if model_max is not None and model_max > 0:\n        max_allowed = min(max_allowed, model_max)\n\n    questions = [q.strip() for q in examples[\"question\"]]\n    contexts = examples[\"context\"]\n    answers = examples[\"answer\"]\n    answer_starts = examples[\"answer_start\"]\n    \n    encoded = {\n        \"input_ids\": [], \n        \"attention_mask\": [], \n        \"token_type_ids\": [],\n        \"start_positions\": [], \n        \"end_positions\": [], \n        \"overflow_to_sample_mapping\": []\n    }\n\n    for i, (question, context, answer, answer_start) in enumerate(zip(questions, contexts, answers, answer_starts)):\n        example_idx = indices[i] if indices is not None else i\n        \n        # CANINE encodes to characters directly. \n        # add_special_tokens=False gives us raw character IDs.\n        question_ids = tokenizer.encode(question, add_special_tokens=False)\n        context_ids = tokenizer.encode(context, add_special_tokens=False)\n\n        # 1. Setup Targets (DIRECT MAPPING)\n        # UQA answer_start is a CHARACTER index. CANINE tokens are CHARACTERS.\n        # Therefore, answer_start maps 1:1 to the context_ids index.\n        if answer and answer_start != -1:\n            gold_char_start = answer_start\n            gold_char_end = answer_start + len(answer) # Points to char AFTER the answer\n        else:\n            gold_char_start = -1\n            gold_char_end = -1\n\n        # 2. Calculate Window Size\n        # [CLS] + Question + [SEP] + Context + [SEP]\n        special_tokens_count = tokenizer.num_special_tokens_to_add(pair=True)\n        max_context_length = max_allowed - len(question_ids) - special_tokens_count\n        \n        if max_context_length <= 0:\n            # Edge case: Question is too long, skip or truncate question (skipping here for safety)\n            continue\n\n        # 3. Sliding Window Loop\n        stride_step = max_context_length - doc_stride\n        if stride_step <= 0: stride_step = max_context_length # Fallback if doc_stride is too big\n        \n        for chunk_start_idx in range(0, len(context_ids), stride_step):\n            chunk_end_idx = min(chunk_start_idx + max_context_length, len(context_ids))\n            context_chunk = context_ids[chunk_start_idx:chunk_end_idx]\n            \n            # Build inputs using tokenizer utility to handle special tokens correctly\n            input_ids = tokenizer.build_inputs_with_special_tokens(question_ids, context_chunk)\n            token_type_ids = tokenizer.create_token_type_ids_from_sequences(question_ids, context_chunk)\n            attention_mask = [1] * len(input_ids)\n            \n            # Calculate Offset: Where does the context actually start in input_ids?\n            # Typically: [CLS] (1) + Q_Len + [SEP] (1) = Start of Context\n            # We calculate this dynamically to be safe:\n            sep_indices = [k for k, x in enumerate(input_ids) if x == tokenizer.sep_token_id]\n            if not sep_indices: \n                continue # Should not happen\n            context_offset_in_input = sep_indices[0] + 1 \n            \n            # 4. Label Assignment\n            # Check if the answer lies ENTIRELY within this specific chunk\n            is_answer_in_chunk = (\n                gold_char_start >= chunk_start_idx and \n                gold_char_end <= chunk_end_idx and\n                gold_char_start != -1\n            )\n            \n            if is_answer_in_chunk:\n                # Map global context index to local window index\n                start_pos = context_offset_in_input + (gold_char_start - chunk_start_idx)\n                # -1 because end_positions is usually inclusive in HF Trainers\n                end_pos = context_offset_in_input + (gold_char_end - chunk_start_idx) - 1\n            else:\n                # Label as [CLS] (index 0) if answer is not here\n                start_pos = 0\n                end_pos = 0\n\n            # 5. Padding\n            pad_len = max_allowed - len(input_ids)\n            if pad_len > 0:\n                input_ids += [tokenizer.pad_token_id] * pad_len\n                attention_mask += [0] * pad_len\n                token_type_ids += [0] * pad_len\n            \n            # 6. Final Safety Truncation (just in case)\n            if len(input_ids) > max_allowed:\n                input_ids = input_ids[:max_allowed]\n                attention_mask = attention_mask[:max_allowed]\n                token_type_ids = token_type_ids[:max_allowed]\n                # If labels were pushed out by truncation, reset to 0\n                if start_pos >= max_allowed or end_pos >= max_allowed:\n                    start_pos = 0\n                    end_pos = 0\n\n            encoded[\"input_ids\"].append(input_ids)\n            encoded[\"attention_mask\"].append(attention_mask)\n            encoded[\"token_type_ids\"].append(token_type_ids)\n            encoded[\"start_positions\"].append(start_pos)\n            encoded[\"end_positions\"].append(end_pos)\n            encoded[\"overflow_to_sample_mapping\"].append(example_idx)\n            \n            # Break loop if this chunk reached the end of the context\n            if chunk_end_idx >= len(context_ids):\n                break\n\n    return encoded\n","metadata":{"execution":{"iopub.status.busy":"2025-11-25T10:01:45.719799Z","iopub.execute_input":"2025-11-25T10:01:45.720068Z","iopub.status.idle":"2025-11-25T10:01:45.732155Z","shell.execute_reply.started":"2025-11-25T10:01:45.720042Z","shell.execute_reply":"2025-11-25T10:01:45.731305Z"},"id":"438d8765","trusted":true},"outputs":[],"execution_count":17},{"id":"a3e95eec","cell_type":"code","source":"# LoRA config\nlora_config = LoraConfig(\n    task_type=TaskType.QUESTION_ANS,\n    r=32,   # changed from 8\n    lora_alpha=64,  # changed from 32\n    lora_dropout=0.1,\n    target_modules=[\"query\", \"value\", \"key\"],   # added key, output.dense\n    bias=\"none\",\n    modules_to_save=[\"qa_outputs\"],\n)\n\ndef print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-25T10:01:45.732928Z","iopub.execute_input":"2025-11-25T10:01:45.733140Z","iopub.status.idle":"2025-11-25T10:01:45.751371Z","shell.execute_reply.started":"2025-11-25T10:01:45.733123Z","shell.execute_reply":"2025-11-25T10:01:45.750654Z"},"id":"a3e95eec","trusted":true},"outputs":[],"execution_count":18},{"id":"d11807b9","cell_type":"code","source":"# preprocess the train and val splits\n# processed_train = uqa_train.map(lambda examples, indices: preprocess_uqa(examples, tokenizer, indices=indices), batched=True, remove_columns=uqa_train.column_names, with_indices=True)\n# processed_val = uqa_val.map(lambda examples, indices: preprocess_uqa(examples, tokenizer, indices=indices), batched=True, remove_columns=uqa_val.column_names, with_indices=True)","metadata":{"execution":{"iopub.status.busy":"2025-11-25T10:01:45.752239Z","iopub.execute_input":"2025-11-25T10:01:45.752489Z","iopub.status.idle":"2025-11-25T10:01:45.767280Z","shell.execute_reply.started":"2025-11-25T10:01:45.752469Z","shell.execute_reply":"2025-11-25T10:01:45.766540Z"},"id":"d11807b9","trusted":true},"outputs":[],"execution_count":19},{"id":"D-emFQTIaZRL","cell_type":"code","source":"# processed_train","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-11-25T10:01:45.768099Z","iopub.execute_input":"2025-11-25T10:01:45.768419Z","iopub.status.idle":"2025-11-25T10:01:45.780915Z","shell.execute_reply.started":"2025-11-25T10:01:45.768399Z","shell.execute_reply":"2025-11-25T10:01:45.780144Z"},"id":"D-emFQTIaZRL","outputId":"eece57cd-d7ed-4931-dda0-b7a34d9c6e95","trusted":true},"outputs":[],"execution_count":20},{"id":"Yy3SiWwCabEi","cell_type":"code","source":"# processed_val","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-11-25T10:01:45.781762Z","iopub.execute_input":"2025-11-25T10:01:45.782363Z","iopub.status.idle":"2025-11-25T10:01:45.795011Z","shell.execute_reply.started":"2025-11-25T10:01:45.782335Z","shell.execute_reply":"2025-11-25T10:01:45.794207Z"},"id":"Yy3SiWwCabEi","outputId":"52db5812-b46b-4a23-8781-bec0571d73c2","trusted":true},"outputs":[],"execution_count":21},{"id":"77ecdd17","cell_type":"code","source":"# processed_train.save_to_disk(\"/kaggle/working/cache/processed_train_uqa\")\n# processed_val.save_to_disk(\"/kaggle/working/cache/processed_val_uqa\")   # cached it\n\n\nprocessed_train = load_from_disk(\"/kaggle/working/cache/processed_train_uqa\")\nprocessed_val = load_from_disk(\"/kaggle/working/cache/processed_val_uqa\")","metadata":{"execution":{"iopub.status.busy":"2025-11-25T10:01:45.797209Z","iopub.execute_input":"2025-11-25T10:01:45.797487Z","iopub.status.idle":"2025-11-25T10:01:45.821164Z","shell.execute_reply.started":"2025-11-25T10:01:45.797468Z","shell.execute_reply":"2025-11-25T10:01:45.820573Z"},"id":"77ecdd17","trusted":true},"outputs":[],"execution_count":22},{"id":"c0e06e6b","cell_type":"code","source":"def print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2025-11-25T10:01:45.821974Z","iopub.execute_input":"2025-11-25T10:01:45.822256Z","iopub.status.idle":"2025-11-25T10:01:45.826672Z","shell.execute_reply.started":"2025-11-25T10:01:45.822231Z","shell.execute_reply":"2025-11-25T10:01:45.825908Z"},"id":"c0e06e6b","trusted":true},"outputs":[],"execution_count":23},{"id":"ba9eeeed","cell_type":"code","source":"# build LoRA model\n\npeft_model = get_peft_model(model, lora_config)\npeft_model.gradient_checkpointing_enable()\nprint_trainable_parameters(peft_model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-11-25T10:01:45.827382Z","iopub.execute_input":"2025-11-25T10:01:45.827582Z","iopub.status.idle":"2025-11-25T10:01:45.914836Z","shell.execute_reply.started":"2025-11-25T10:01:45.827567Z","shell.execute_reply":"2025-11-25T10:01:45.914036Z"},"id":"ba9eeeed","outputId":"e987b70d-b2a2-4618-d538-578bff1909bb","trusted":true},"outputs":[{"name":"stdout","text":"trainable params: 2065922 || all params: 134150404 || trainable%: 1.5400043074040985\n","output_type":"stream"}],"execution_count":24},{"id":"61c5c7a7","cell_type":"code","source":"# evals\n\n\ndef normalize_answer(text):\n    text = (text or \"\").lower()\n    def remove_articles(s):\n        return re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n    def remove_punctuation(s):\n        return \"\".join(ch for ch in s if ch not in string.punctuation)\n    def white_space_fix(s):\n        return \" \".join(s.split())\n    return white_space_fix(remove_articles(remove_punctuation(text)))\n\ndef exact_match_score(prediction, ground_truth):\n    return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n\ndef f1_score(prediction, ground_truth):\n    pred_tokens = normalize_answer(prediction).split()\n    gold_tokens = normalize_answer(ground_truth).split()\n    if not gold_tokens:\n        return 1.0 if not pred_tokens else 0.0\n    if not pred_tokens:\n        return 0.0\n    common = Counter(pred_tokens) & Counter(gold_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0.0\n    precision = num_same / len(pred_tokens)\n    recall = num_same / len(gold_tokens)\n    return 2 * precision * recall / (precision + recall)\n\ndef edit_distance_score(prediction, ground_truth):\n    pred_norm = normalize_answer(prediction)\n    gold_norm = normalize_answer(ground_truth)\n    if not gold_norm and not pred_norm:\n        return 1.0\n    if not gold_norm or not pred_norm:\n        return 0.0\n    distance = Levenshtein.distance(pred_norm, gold_norm)\n    max_len = max(len(pred_norm), len(gold_norm))\n    return 1.0 - (distance / max_len) if max_len > 0 else 1.0\n\ndef gold_answer(example):\n    # Extracts the gold answer substring from the context using character offsets\n    answer = example.get(\"answer\")\n    context = example.get(\"context\")\n    answer_start = example.get(\"answer_start\", -1)\n    if answer and answer_start is not None and answer_start != -1:\n        return context[answer_start: answer_start + len(answer)]\n    return \"[CLS]\"\n\n\ndef decode_prediction(input_ids, start_idx, end_idx, tokenizer=None):\n    if start_idx > end_idx:\n        start_idx, end_idx = end_idx, start_idx\n    if tokenizer is None:\n        raise ValueError(\"Tokenizer must be provided for decoding.\")\n    cls_index = input_ids.index(tokenizer.cls_token_id)\n    # If both point to CLS token, return [CLS] sentinel\n    if start_idx == cls_index and end_idx == cls_index:\n        return \"[CLS]\"\n    start_idx = max(start_idx, 0)\n    end_idx = min(end_idx, len(input_ids) - 1)\n    if start_idx > end_idx:\n        return \"[CLS]\"\n    text = tokenizer.decode(input_ids[start_idx:end_idx + 1], skip_special_tokens=True)\n    text = text.strip()\n    return text if text else \"[CLS]\"\n\n\ndef evaluate_checkpoint(checkpoint_path=None, model_instance=None, eval_dataset=None):\n    \"\"\"Evaluate either a checkpoint path (loads model) or a provided model instance.\n\n    - checkpoint_path: path to checkpoint folder\n    - model_instance: an in-memory model (preferably a PeftModel or CanineForQuestionAnswering)\n    - eval_dataset: optional dataset to evaluate; if None the default processed_val will be used\n    \"\"\"\n    if eval_dataset is None:\n        eval_dataset = processed_val\n\n    # If a model_instance is given, use it directly (avoid re-loading a fresh base model)\n    if model_instance is not None:\n        eval_model = model_instance\n    else:\n        base_model = CanineForQuestionAnswering.from_pretrained(model_name, trust_remote_code=False)\n        eval_model = get_peft_model(base_model, lora_config)\n        # Try loading adapter weights; fall back to PeftModel.from_pretrained if needed\n        try:\n            eval_model.load_adapter(checkpoint_path)\n        except Exception:\n            from peft import PeftModel\n            eval_model = PeftModel.from_pretrained(base_model, checkpoint_path)\n\n    eval_model.to(device)\n\n    eval_args = TrainingArguments(\n        # Small evaluation config; uses cpu/mps if no gpu during eval\n        output_dir=\"outputs/canine-s-uqa\",\n        per_device_eval_batch_size=16,\n        dataloader_drop_last=False,\n        fp16=True,\n        bf16=False,\n        report_to=\"none\",\n    )\n\n    # Run evaluation via a lightweight Trainer so prediction loop is standard\n    eval_trainer = Trainer(\n        model=eval_model,\n        args=eval_args,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n    )\n\n    predictions = eval_trainer.predict(eval_dataset)\n    start_logits, end_logits = predictions.predictions\n    best_predictions = {}\n    for feature_index, feature in enumerate(eval_dataset):\n        sample_idx = int(feature[\"overflow_to_sample_mapping\"])\n        input_ids = feature[\"input_ids\"]\n        start_idx = int(np.argmax(start_logits[feature_index]))\n        end_idx = int(np.argmax(end_logits[feature_index]))\n        score = float(start_logits[feature_index][start_idx] + end_logits[feature_index][end_idx])\n        prediction_text = decode_prediction(input_ids, start_idx, end_idx, tokenizer=tokenizer)\n        stored = best_predictions.get(sample_idx)\n        if stored is None or score > stored[0]:\n            best_predictions[sample_idx] = (score, prediction_text)\n\n    em_scores = []\n    f1_scores = []\n    edit_dist_scores = []\n    for sample_idx, (_, prediction_text) in best_predictions.items():\n        reference = gold_answer(uqa_val[int(sample_idx)])\n        em_scores.append(exact_match_score(prediction_text, reference))\n        f1_scores.append(f1_score(prediction_text, reference))\n        edit_dist_scores.append(edit_distance_score(prediction_text, reference))\n\n    em = float(np.mean(em_scores)) if em_scores else 0.0\n    f1 = float(np.mean(f1_scores)) if f1_scores else 0.0\n    edit_dist = float(np.mean(edit_dist_scores)) if edit_dist_scores else 0.0\n    print(f\"Examples evaluated: {len(em_scores)}\")\n    print(f\"Exact Match: {em * 100:.2f}\")\n    print(f\"F1: {f1 * 100:.2f}\")\n    print(f\"Edit Distance (normalized): {edit_dist * 100:.2f}\")\n    return {\"exact_match\": em, \"f1\": f1, \"edit_distance\": edit_dist}\n","metadata":{"execution":{"iopub.status.busy":"2025-11-25T10:01:45.915747Z","iopub.execute_input":"2025-11-25T10:01:45.916115Z","iopub.status.idle":"2025-11-25T10:01:45.932409Z","shell.execute_reply.started":"2025-11-25T10:01:45.916091Z","shell.execute_reply":"2025-11-25T10:01:45.931596Z"},"id":"61c5c7a7","trusted":true},"outputs":[],"execution_count":25},{"id":"c4abaaab","cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"outputs/canine-s-uqa\",\n    \n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=16,\n    \n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    \n    num_train_epochs=1,\n    learning_rate=3e-4,\n    weight_decay=0.01,\n    eval_strategy=\"no\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=500,\n    logging_steps=25,\n    fp16=True,\n    bf16=False,\n    report_to=\"none\",\n    push_to_hub=True,\n    hub_model_id=\"VohraAK/canine-s-uqa\",\n    hub_strategy=\"checkpoint\",\n    )\n\nclass CustomEvalCallback(TrainerCallback):\n    def __init__(self, eval_func, eval_dataset, use_in_memory_model=True, verbose=True):\n        self.eval_func = eval_func\n        self.eval_dataset = eval_dataset\n        self.use_in_memory_model = use_in_memory_model\n        self.verbose = verbose\n        # trainer reference (set after trainer exists)\n        self.trainer = None\n\n    def on_save(self, args, state, control, model=None, **kwargs):\n        checkpoint_path = f\"{args.output_dir}/checkpoint-{state.global_step}\"\n        if self.verbose:\n            print(f\"\\nüîç Running custom evaluation at step {state.global_step}...\")\n\n        # Prefer evaluating the in-memory trainer model (fast + avoids re-loading)\n        if self.use_in_memory_model and self.trainer is not None:\n            if self.verbose:\n                print(\"Using in-memory model for evaluation (no reloading).\")\n            try:\n                metrics = self.eval_func(checkpoint_path=None, model_instance=self.trainer.model, eval_dataset=self.eval_dataset)\n            except Exception as e:\n                print(\"‚ö†Ô∏è in-memory evaluation failed, falling back to checkpoint load:\", e)\n                metrics = self.eval_func(checkpoint_path)\n        else:\n            metrics = self.eval_func(checkpoint_path)\n\n        # record metrics in state.log_history\n        state.log_history.append({\n            \"step\": state.global_step,\n            \"eval_exact_match\": metrics.get(\"exact_match\"),\n            \"eval_f1\": metrics.get(\"f1\"),\n            \"eval_edit_distance\": metrics.get(\"edit_distance\"),\n        })\n\n        if self.verbose:\n            print(f\"‚úÖ Step {state.global_step}: EM={metrics.get('exact_match',0)*100:.2f}, F1={metrics.get('f1',0)*100:.2f}, EditDist={metrics.get('edit_distance',0)*100:.2f}\")\n\n        # Update trainer_state.json to include custom metrics\n        state_path = f\"{checkpoint_path}/trainer_state.json\"\n        try:\n            with open(state_path, 'r') as f:\n                state_dict = json.load(f)\n            state_dict['log_history'] = state.log_history\n            with open(state_path, 'w') as f:\n                json.dump(state_dict, f, indent=2)\n            if self.verbose:\n                print(f\"üíæ Updated trainer_state.json with custom metrics\")\n        except Exception as e:\n            if self.verbose:\n                print(f\"‚ö†Ô∏è  Warning: Could not update trainer_state.json: {e}\")\n\n        try:\n            if self.verbose:\n                print(f\"‚òÅÔ∏è  Pushing checkpoint-{state.global_step} to Hub...\")\n            api = HfApi()\n            api.upload_folder(\n                folder_path=checkpoint_path,\n                repo_id=args.hub_model_id,\n                path_in_repo=f\"checkpoint-{state.global_step}\",\n                commit_message=f\"Add checkpoint {state.global_step} (EM={metrics.get('exact_match',0)*100:.1f}%, F1={metrics.get('f1',0)*100:.1f}%)\",\n                repo_type=\"model\"\n            )\n            if self.verbose:\n                print(f\"‚úÖ Pushed checkpoint-{state.global_step} to Hub\")\n        except Exception as e:\n            if self.verbose:\n                print(f\"‚ö†Ô∏è  Warning: Could not push to Hub: {e}\")\n\n        return control","metadata":{"execution":{"iopub.status.busy":"2025-11-25T10:01:45.933653Z","iopub.execute_input":"2025-11-25T10:01:45.933927Z","iopub.status.idle":"2025-11-25T10:01:45.978263Z","shell.execute_reply.started":"2025-11-25T10:01:45.933904Z","shell.execute_reply":"2025-11-25T10:01:45.977428Z"},"id":"c4abaaab","trusted":true},"outputs":[],"execution_count":26},{"id":"055f5dda","cell_type":"code","source":"trainer_cb = CustomEvalCallback(evaluate_checkpoint, processed_val, use_in_memory_model=True)\n\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=processed_train,\n    eval_dataset=processed_val,\n    callbacks=[trainer_cb],\n)\n","metadata":{"execution":{"iopub.status.busy":"2025-11-25T10:01:45.979158Z","iopub.execute_input":"2025-11-25T10:01:45.979504Z","iopub.status.idle":"2025-11-25T10:01:46.891831Z","shell.execute_reply.started":"2025-11-25T10:01:45.979480Z","shell.execute_reply":"2025-11-25T10:01:46.891061Z"},"id":"055f5dda","trusted":true},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":27},{"id":"TOUimesUX5Re","cell_type":"code","source":"trainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.status.busy":"2025-11-25T10:01:46.893217Z","iopub.execute_input":"2025-11-25T10:01:46.893444Z","iopub.status.idle":"2025-11-25T10:15:46.370866Z","shell.execute_reply.started":"2025-11-25T10:01:46.893426Z","shell.execute_reply":"2025-11-25T10:15:46.369672Z"},"id":"TOUimesUX5Re","outputId":"c0ada6de-de08-41ce-e8fc-5c59232de758","trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1501' max='7313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1501/7313 13:35 < 52:42, 1.84 it/s, Epoch 0.21/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>5.858300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>5.633200</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>5.429500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>5.230600</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>5.118000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>4.972200</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>4.794400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>4.540800</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>4.499200</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>4.434200</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>4.431800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>4.303300</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>4.219300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>4.061500</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>3.959800</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>3.902200</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>3.836400</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>3.782400</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>3.710000</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>3.888000</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>3.570100</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>3.596400</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>3.580800</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>3.563100</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>3.638900</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>3.593000</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>3.574200</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>3.363500</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>3.308800</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>3.456800</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>3.184000</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>3.186100</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>3.325100</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>3.111700</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>3.225000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>3.025600</td>\n    </tr>\n    <tr>\n      <td>925</td>\n      <td>3.176300</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>3.238100</td>\n    </tr>\n    <tr>\n      <td>975</td>\n      <td>3.092500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>3.108600</td>\n    </tr>\n    <tr>\n      <td>1025</td>\n      <td>3.078900</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>3.037400</td>\n    </tr>\n    <tr>\n      <td>1075</td>\n      <td>2.982300</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>3.040300</td>\n    </tr>\n    <tr>\n      <td>1125</td>\n      <td>3.007800</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>2.954100</td>\n    </tr>\n    <tr>\n      <td>1175</td>\n      <td>2.983000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>2.819300</td>\n    </tr>\n    <tr>\n      <td>1225</td>\n      <td>2.831100</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>3.089000</td>\n    </tr>\n    <tr>\n      <td>1275</td>\n      <td>2.895600</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>2.947100</td>\n    </tr>\n    <tr>\n      <td>1325</td>\n      <td>2.803200</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>2.789900</td>\n    </tr>\n    <tr>\n      <td>1375</td>\n      <td>2.770500</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>3.015800</td>\n    </tr>\n    <tr>\n      <td>1425</td>\n      <td>2.849100</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>2.948400</td>\n    </tr>\n    <tr>\n      <td>1475</td>\n      <td>2.840300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nüîç Running custom evaluation at step 500...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_302/733281207.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Examples evaluated: 10000\nExact Match: 30.16\nF1: 30.54\nEdit Distance (normalized): 30.99\n‚úÖ Step 500: EM=30.16, F1=30.54, EditDist=30.99\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-500 to Hub...\n‚úÖ Pushed checkpoint-500 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 1000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_302/733281207.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Examples evaluated: 10000\nExact Match: 31.19\nF1: 31.42\nEdit Distance (normalized): 31.72\n‚úÖ Step 1000: EM=31.19, F1=31.42, EditDist=31.72\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-1000 to Hub...\n‚úÖ Pushed checkpoint-1000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 1500...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_302/733281207.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='184' max='1966' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 184/1966 00:21 < 03:25, 8.69 it/s]\n    </div>\n    "},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_302/4032920361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2195\u001b[0m                 \u001b[0;31m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2196\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2197\u001b[0;31m                 return inner_training_loop(\n\u001b[0m\u001b[1;32m   2198\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m                     \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2621\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msteps_skipped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_in_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m                         self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2624\u001b[0m                             \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m                             \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3104\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_rng_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_save\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_save\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             result = getattr(callback, event)(\n\u001b[0m\u001b[1;32m    557\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_302/1659446634.py\u001b[0m in \u001b[0;36mon_save\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# record metrics in state.log_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_302/733281207.py\u001b[0m in \u001b[0;36mevaluate_checkpoint\u001b[0;34m(checkpoint_path, model_instance, eval_dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0mstart_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mbest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4276\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4277\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4278\u001b[0m             \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4279\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4414\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_across_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4415\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4416\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_across_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4417\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_logits_for_metrics\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4418\u001b[0m                     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_logits_for_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mpad_across_processes\u001b[0;34m(self, tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m         \"\"\"\n\u001b[0;32m-> 2938\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpad_across_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munwrap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_fp32_wrapper\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_torch_compile\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mDistributedOperationException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0moperation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{function.__module__}.{function.__name__}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mpad_across_processes\u001b[0;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m     return recursively_apply(\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0m_pad_across_processes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_on_other_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mrecursively_apply\u001b[0;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         return honor_type(\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mhonor_type\u001b[0;34m(obj, generator)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             (\n\u001b[0;32m--> 110\u001b[0;31m                 recursively_apply(\n\u001b[0m\u001b[1;32m    111\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_on_other_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_on_other_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mrecursively_apply\u001b[0;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m         )\n\u001b[1;32m    125\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtest_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0merror_on_other_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         raise TypeError(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m_pad_across_processes\u001b[0;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;31m# Gather all sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m         \u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0;31m# Then pad to the maximum size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":28},{"id":"cc44692c-6652-4cda-9ba4-8a03acdab88d","cell_type":"markdown","source":"### Diagnosing Preprocessing Functions!!!","metadata":{}},{"id":"49f3717d","cell_type":"code","source":"# Diagnostic cell (fixed): Investigate preprocessing and truncation for many samples\nimport random\nimport pandas as pd\nfrom transformers import AutoTokenizer\n\n# Set display options to see full Urdu text\npd.set_option('display.max_colwidth', None)\n\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(\"google/canine-s\")\nexcept Exception:\n    tokenizer = None\n\nnum_samples = 20000  # Number of samples to check\nresults = []\n\nfor split_name, orig_data, proc_data in [\n    (\"train\", uqa_train, processed_train),\n    (\"val\", uqa_val, processed_val)\n]:\n    # Sample random indices\n    if len(proc_data) < num_samples:\n        current_indices = range(len(proc_data))\n    else:\n        current_indices = random.sample(range(len(proc_data)), num_samples)\n        \n    for idx in current_indices:\n        proc = proc_data[idx]\n        # Use overflow_to_sample_mapping to get the correct original index\n        orig_idx = proc[\"overflow_to_sample_mapping\"]\n        orig = orig_data[orig_idx]\n        \n        input_ids = proc[\"input_ids\"]\n        start_pos = proc[\"start_positions\"]\n        end_pos = proc[\"end_positions\"]\n        \n        gold_answer = orig.get(\"gold_answer\", orig.get(\"answer\", \"\"))\n        question = orig.get(\"question\", \"\")\n        \n        # Decode input_ids to text (for debugging context)\n        if tokenizer:\n            decoded_text = tokenizer.decode(input_ids, skip_special_tokens=False)\n        else:\n            decoded_text = str(input_ids)\n            \n        # Extract predicted answer span\n        if 0 <= start_pos < len(input_ids) and 0 <= end_pos < len(input_ids):\n            if tokenizer:\n                pred_span = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True)\n            else:\n                pred_span = str(input_ids[start_pos:end_pos+1])\n        else:\n            pred_span = \"[CLS]\" # Represents no answer found in this chunk or invalid\n            \n        # Check if pred_span matches gold answer\n        # We strip() to ignore minor whitespace differences\n        pred_matches_gold = pred_span.strip() == gold_answer.strip()\n        \n        # Check if gold is even reachable in this chunk\n        gold_in_decoded = gold_answer in decoded_text\n\n        results.append({\n            \"Split\": split_name,\n            \"Question\": question,\n            \"Gold Answer\": gold_answer,\n            \"Extracted Answer\": pred_span,\n            \"Match\": pred_matches_gold,\n            \"Gold Reachable\": gold_in_decoded,\n            \"orig_idx\": orig_idx\n        })\n\n# Create DataFrame\nresults_df = pd.DataFrame(results)\n\n# --- SIDE BY SIDE COMPARISON ---\n\n# 1. Filter for Solvable Mismatches (Gold was there, but we predicted wrong)\nproblem_cases = results_df[\n    (results_df[\"Gold Reachable\"] == True) & \n    (results_df[\"Match\"] == False)\n][[\"Question\", \"Gold Answer\", \"Extracted Answer\", \"Split\"]]\n\nprint(f\"üîç Checked {len(results_df)} samples.\")\nprint(f\"‚ùå Found {len(problem_cases)} cases where Gold was present but Extraction failed.\")\n\nprint(\"\\nüìä Side-by-Side Comparison (Top 20 Failures):\")\ndisplay(problem_cases.head(50))\n\nprint(\"\\n‚úÖ Side-by-Side Comparison (First 10 Rows - Mixed):\")\ndisplay(results_df[[\"Question\", \"Gold Answer\", \"Extracted Answer\", \"Match\"]].head(50))","metadata":{"execution":{"iopub.status.busy":"2025-11-25T10:15:46.371657Z","iopub.status.idle":"2025-11-25T10:15:46.371963Z","shell.execute_reply.started":"2025-11-25T10:15:46.371805Z","shell.execute_reply":"2025-11-25T10:15:46.371817Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"e67abc12","cell_type":"code","source":"# Accuracy: fraction of rows where extracted answer matches gold answer\naccuracy = (results_df[\"Match\"]).mean()\n\n# Precision: among rows where extracted answer is non-empty, fraction that matches gold\n# We filter out cases where the model predicted nothing (empty string) or just whitespace\nnon_empty_pred = results_df[\"Extracted Answer\"].str.strip() != \"\"\n\n# Avoid division by zero if no predictions were made\nif non_empty_pred.sum() > 0:\n    precision = (results_df[\"Match\"] & non_empty_pred).sum() / non_empty_pred.sum()\nelse:\n    precision = 0.0\n\nprint(f\"Accuracy: {accuracy:.3f}\")\nprint(f\"Precision: {precision:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2025-11-25T10:15:46.373305Z","iopub.status.idle":"2025-11-25T10:15:46.373561Z","shell.execute_reply.started":"2025-11-25T10:15:46.373453Z","shell.execute_reply":"2025-11-25T10:15:46.373463Z"},"trusted":true},"outputs":[],"execution_count":null}]}