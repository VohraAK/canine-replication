{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c186240c","cell_type":"code","source":"# %pip install peft evaluate transformers Levenshtein ipywidgets\n# %pip install protobuf==3.20.3","metadata":{"execution":{"iopub.status.busy":"2025-11-21T12:12:49.018442Z","iopub.execute_input":"2025-11-21T12:12:49.019245Z","iopub.status.idle":"2025-11-21T12:12:49.022419Z","shell.execute_reply.started":"2025-11-21T12:12:49.019218Z","shell.execute_reply":"2025-11-21T12:12:49.021656Z"},"id":"c186240c","trusted":true},"outputs":[],"execution_count":6},{"id":"cd8da8ab","cell_type":"code","source":"import os\nos.environ[\"TRANSFORMERS_DISABLE_CHAT_TEMPLATES\"] = \"1\"\nos.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\nos.environ[\"TRANSFORMERS_NO_ADDITIONAL_CHAT_TEMPLATES\"] = \"1\"","metadata":{"execution":{"iopub.status.busy":"2025-11-21T12:12:49.023547Z","iopub.execute_input":"2025-11-21T12:12:49.023767Z","iopub.status.idle":"2025-11-21T12:12:49.038162Z","shell.execute_reply.started":"2025-11-21T12:12:49.023751Z","shell.execute_reply":"2025-11-21T12:12:49.037459Z"},"id":"cd8da8ab","trusted":true},"outputs":[],"execution_count":7},{"id":"d87eba82","cell_type":"code","source":"from datasets import load_dataset, load_from_disk\n# from UQA.canine_utils import preprocess_uqa, lora_config, print_trainable_parameters, normalize_answer, exact_match_score, f1_score, edit_distance_score, gold_answer, decode_prediction\nfrom transformers import CanineTokenizer\nfrom peft import LoraConfig, TaskType, get_peft_model\nimport re\nimport string\nfrom collections import Counter\nimport numpy as np\nimport Levenshtein\n\nfrom transformers import TrainingArguments, Trainer, TrainerCallback\nimport json\nfrom huggingface_hub import HfApi, notebook_login, whoami","metadata":{"execution":{"iopub.status.busy":"2025-11-21T12:12:49.039331Z","iopub.execute_input":"2025-11-21T12:12:49.039553Z","iopub.status.idle":"2025-11-21T12:12:49.054642Z","shell.execute_reply.started":"2025-11-21T12:12:49.039529Z","shell.execute_reply":"2025-11-21T12:12:49.053951Z"},"id":"d87eba82","trusted":true},"outputs":[],"execution_count":8},{"id":"0e98cebe-4c08-4850-b3c1-1529564fdb1b","cell_type":"code","source":"# notebook_login()\n# whoami()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T12:12:49.055502Z","iopub.execute_input":"2025-11-21T12:12:49.055781Z","iopub.status.idle":"2025-11-21T12:12:49.070209Z","shell.execute_reply.started":"2025-11-21T12:12:49.055757Z","shell.execute_reply":"2025-11-21T12:12:49.069353Z"}},"outputs":[],"execution_count":9},{"id":"d474e2e8","cell_type":"code","source":"uqa_dataset = load_dataset(\"uqa/UQA\")\nuqa_train = uqa_dataset[\"train\"].shuffle(seed=42).select(range(40000))\nuqa_val = uqa_dataset[\"validation\"].shuffle(seed=42).select(range(10000))","metadata":{"execution":{"iopub.status.busy":"2025-11-21T12:12:49.071351Z","iopub.execute_input":"2025-11-21T12:12:49.071628Z","iopub.status.idle":"2025-11-21T12:12:52.951513Z","shell.execute_reply.started":"2025-11-21T12:12:49.071608Z","shell.execute_reply":"2025-11-21T12:12:52.950904Z"},"id":"d474e2e8","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/898 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06824dcacf804680a51a4b1c76e91659"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001-bac007e8ca7192(‚Ä¶):   0%|          | 0.00/30.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a4fb66713d040cfa71863ff6413e3c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001-cf8a6960d(‚Ä¶):   0%|          | 0.00/2.92M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f85c9ed64954d03bab64034a04b1bf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/124745 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e707a1acb32b4525a7b0be76cce5b3e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/16824 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a608d0b087843ebaaea5ad4c7b145f8"}},"metadata":{}}],"execution_count":10},{"id":"f2dd5a40","cell_type":"code","source":"from transformers import CanineTokenizer, CanineForQuestionAnswering\nimport torch\nmodel_name = 'google/canine-s'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n\ntokenizer = CanineTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=False)\nmodel = CanineForQuestionAnswering.from_pretrained(model_name, trust_remote_code=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-11-21T12:12:52.952385Z","iopub.execute_input":"2025-11-21T12:12:52.952651Z","iopub.status.idle":"2025-11-21T12:12:59.227373Z","shell.execute_reply.started":"2025-11-21T12:12:52.952629Z","shell.execute_reply":"2025-11-21T12:12:59.226723Z"},"id":"f2dd5a40","outputId":"4ea979ff-002f-491f-bbb3-debb423da2b6","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/854 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b015508c4f0547ee93bbc2d8bf9b3536"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/657 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af63930ba4c0473f8f25290a0dd4791a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/670 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5776b2d75b9c41ebb819fe03f787581e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/528M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e81afaa186449b78d80697c5802be5f"}},"metadata":{}},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":11},{"id":"438d8765","cell_type":"code","source":"# preprocessors\nMAX_SEQ_LENGTH = 384\nDOC_STRIDE = 64\n\ndef _build_byte_to_char_index(text):\n    cumulative = [0]\n    for char in text:\n        cumulative.append(cumulative[-1] + len(char.encode(\"utf-8\")))\n    return cumulative\n\ndef _byte_to_char(cumulative_bytes, byte_index):\n    from bisect import bisect_right\n    position = bisect_right(cumulative_bytes, byte_index) - 1\n    return max(position, 0)\n\n# Safe preprocessing: enforce tokenizer/model limits\ndef preprocess_uqa(examples, tokenizer, max_length=MAX_SEQ_LENGTH, doc_stride=DOC_STRIDE, model_obj=None):\n    # compute global allowed max (use tokenizer/model if available)\n    tokenizer_max = getattr(tokenizer, \"model_max_length\", max_length)\n    model_max = getattr(model_obj.config, \"max_position_embeddings\", None) if model_obj is not None else None\n    # choose the smallest of the configured limits\n    max_allowed = max_length\n    if tokenizer_max is not None and tokenizer_max > 0:\n        max_allowed = min(max_allowed, tokenizer_max)\n    if model_max is not None and model_max > 0:\n        max_allowed = min(max_allowed, model_max)\n\n    questions = [q.strip() for q in examples[\"question\"]]\n    contexts = examples[\"context\"]\n    answers = examples[\"answer\"]\n    answer_starts = examples[\"answer_start\"]\n    special_tokens = tokenizer.num_special_tokens_to_add(pair=True)\n\n    encoded = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": [],\n               \"start_positions\": [], \"end_positions\": [], \"overflow_to_sample_mapping\": []}\n\n    for example_idx, (question, context, answer, answer_start) in enumerate(zip(questions, contexts, answers, answer_starts)):\n        question_tokens = tokenizer.encode(question, add_special_tokens=False)\n        context_tokens = tokenizer.encode(context, add_special_tokens=False)\n\n        # compute how many context tokens we can include (reserve special + question)\n        max_context_tokens = max_allowed - len(question_tokens) - special_tokens\n        if max_context_tokens <= 0 or not context_tokens:\n            # skip or emit a short feature that points to CLS\n            continue\n\n        # rest of function unchanged but using max_context_tokens (same as before)\n        stride_tokens = max_context_tokens - doc_stride\n        if stride_tokens <= 0:\n            stride_tokens = max_context_tokens\n        span_start = 0\n        context_length = len(context_tokens)\n        while span_start < context_length:\n            span_end = min(span_start + max_context_tokens, context_length)\n            context_chunk = context_tokens[span_start:span_end]\n            input_ids = tokenizer.build_inputs_with_special_tokens(question_tokens, context_chunk)\n            token_type_ids = tokenizer.create_token_type_ids_from_sequences(question_tokens, context_chunk)\n            attention_mask = [1] * len(input_ids)\n            cls_index = input_ids.index(tokenizer.cls_token_id)\n            context_offset = len(input_ids) - len(context_chunk) - 1\n\n            if answer and answer_start != -1:\n                byte_map = _build_byte_to_char_index(context)\n                start_char = _byte_to_char(byte_map, answer_start)\n                end_char = _byte_to_char(byte_map, max(answer_start + len(answer) - 1, answer_start))\n                answer_span = (start_char, end_char)\n                start_char, end_char = answer_span\n                answer_in_chunk = start_char >= span_start and end_char < span_end\n                if answer_in_chunk:\n                    start_pos = context_offset + (start_char - span_start)\n                    end_pos = context_offset + (end_char - span_start)\n                else:\n                    start_pos = cls_index\n                    end_pos = cls_index\n            else:\n                start_pos = cls_index\n                end_pos = cls_index\n\n            # ensure final length <= max_allowed by truncating if necessary\n            if len(input_ids) > max_allowed:\n                input_ids = input_ids[:max_allowed]\n                attention_mask = attention_mask[:max_allowed]\n                token_type_ids = token_type_ids[:max_allowed]\n                # if start/end fall outside, point to CLS\n                if start_pos >= max_allowed or end_pos >= max_allowed:\n                    start_pos = cls_index\n                    end_pos = cls_index\n\n            padding = max_allowed - len(input_ids)\n            if padding > 0:\n                pad_id = tokenizer.pad_token_id\n                input_ids += [pad_id] * padding\n                attention_mask += [0] * padding\n                token_type_ids += [0] * padding\n\n            encoded[\"input_ids\"].append(input_ids)\n            encoded[\"attention_mask\"].append(attention_mask)\n            encoded[\"token_type_ids\"].append(token_type_ids)\n            encoded[\"start_positions\"].append(start_pos)\n            encoded[\"end_positions\"].append(end_pos)\n            encoded[\"overflow_to_sample_mapping\"].append(example_idx)\n\n            if span_end == context_length:\n                break\n            span_start += stride_tokens\n\n    return encoded\n\n\n","metadata":{"execution":{"iopub.status.busy":"2025-11-21T12:12:59.229207Z","iopub.execute_input":"2025-11-21T12:12:59.229805Z","iopub.status.idle":"2025-11-21T12:12:59.242276Z","shell.execute_reply.started":"2025-11-21T12:12:59.229783Z","shell.execute_reply":"2025-11-21T12:12:59.241465Z"},"id":"438d8765","trusted":true},"outputs":[],"execution_count":12},{"id":"a3e95eec","cell_type":"code","source":"# LoRA config\nlora_config = LoraConfig(\n    task_type=TaskType.QUESTION_ANS,\n    r=32,   # changed from 8\n    lora_alpha=64,  # changed from 32\n    lora_dropout=0.1,\n    target_modules=[\"query\", \"value\", \"key\"],   # added key, output.dense\n    bias=\"none\",\n    modules_to_save=[\"qa_outputs\"],\n)\n\ndef print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-21T12:12:59.243033Z","iopub.execute_input":"2025-11-21T12:12:59.243293Z","iopub.status.idle":"2025-11-21T12:12:59.262467Z","shell.execute_reply.started":"2025-11-21T12:12:59.243277Z","shell.execute_reply":"2025-11-21T12:12:59.261612Z"},"id":"a3e95eec","trusted":true},"outputs":[],"execution_count":13},{"id":"d11807b9","cell_type":"code","source":"# preprocess the train and val splits\nprocessed_train = uqa_train.map(lambda examples: preprocess_uqa(examples, tokenizer), batched=True, remove_columns=uqa_train.column_names)\nprocessed_val = uqa_val.map(lambda examples: preprocess_uqa(examples, tokenizer), batched=True, remove_columns=uqa_val.column_names)","metadata":{"execution":{"iopub.status.busy":"2025-11-21T12:12:59.263310Z","iopub.execute_input":"2025-11-21T12:12:59.263662Z","iopub.status.idle":"2025-11-21T12:14:17.494364Z","shell.execute_reply.started":"2025-11-21T12:12:59.263642Z","shell.execute_reply":"2025-11-21T12:14:17.493717Z"},"id":"d11807b9","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea5c76ff81084819b20681da7cd1fcf4"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (3179 > 2048). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a7137b895424df6965f757601f8aae4"}},"metadata":{}}],"execution_count":14},{"id":"D-emFQTIaZRL","cell_type":"code","source":"processed_train","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-emFQTIaZRL","outputId":"eece57cd-d7ed-4931-dda0-b7a34d9c6e95","trusted":true,"execution":{"iopub.status.busy":"2025-11-21T12:14:17.495498Z","iopub.execute_input":"2025-11-21T12:14:17.495750Z","iopub.status.idle":"2025-11-21T12:14:17.500727Z","shell.execute_reply.started":"2025-11-21T12:14:17.495733Z","shell.execute_reply":"2025-11-21T12:14:17.500032Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'token_type_ids', 'start_positions', 'end_positions', 'overflow_to_sample_mapping'],\n    num_rows: 116995\n})"},"metadata":{}}],"execution_count":15},{"id":"Yy3SiWwCabEi","cell_type":"code","source":"processed_val","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yy3SiWwCabEi","outputId":"52db5812-b46b-4a23-8781-bec0571d73c2","trusted":true,"execution":{"iopub.status.busy":"2025-11-21T12:14:17.501447Z","iopub.execute_input":"2025-11-21T12:14:17.501628Z","iopub.status.idle":"2025-11-21T12:14:17.732451Z","shell.execute_reply.started":"2025-11-21T12:14:17.501614Z","shell.execute_reply":"2025-11-21T12:14:17.731659Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'token_type_ids', 'start_positions', 'end_positions', 'overflow_to_sample_mapping'],\n    num_rows: 31446\n})"},"metadata":{}}],"execution_count":16},{"id":"77ecdd17","cell_type":"code","source":"processed_train.save_to_disk(\"/kaggle/working/cache/processed_train_uqa\")\nprocessed_val.save_to_disk(\"/kaggle/working/cache/processed_val_uqa\")   # cached it\n\n\nprocessed_train = load_from_disk(\"/kaggle/working/cache/processed_train_uqa\")\nprocessed_val = load_from_disk(\"/kaggle/working/cache/processed_val_uqa\")","metadata":{"id":"77ecdd17","trusted":true,"execution":{"iopub.status.busy":"2025-11-21T12:14:17.734486Z","iopub.execute_input":"2025-11-21T12:14:17.734781Z","iopub.status.idle":"2025-11-21T12:14:18.223858Z","shell.execute_reply.started":"2025-11-21T12:14:17.734761Z","shell.execute_reply":"2025-11-21T12:14:18.223210Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/116995 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6bb599718cf4ddf98bf7c52cfd0fd29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/31446 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39ff4347cdc04f7ebf4264b4b921c881"}},"metadata":{}}],"execution_count":17},{"id":"c0e06e6b","cell_type":"code","source":"def print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n\n","metadata":{"id":"c0e06e6b","trusted":true,"execution":{"iopub.status.busy":"2025-11-21T12:14:18.224556Z","iopub.execute_input":"2025-11-21T12:14:18.224786Z","iopub.status.idle":"2025-11-21T12:14:18.228991Z","shell.execute_reply.started":"2025-11-21T12:14:18.224770Z","shell.execute_reply":"2025-11-21T12:14:18.228215Z"}},"outputs":[],"execution_count":18},{"id":"ba9eeeed","cell_type":"code","source":"# build LoRA model\n\npeft_model = get_peft_model(model, lora_config)\npeft_model.gradient_checkpointing_enable()\nprint_trainable_parameters(peft_model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ba9eeeed","outputId":"e987b70d-b2a2-4618-d538-578bff1909bb","trusted":true,"execution":{"iopub.status.busy":"2025-11-21T12:14:18.229865Z","iopub.execute_input":"2025-11-21T12:14:18.230133Z","iopub.status.idle":"2025-11-21T12:14:18.319663Z","shell.execute_reply.started":"2025-11-21T12:14:18.230107Z","shell.execute_reply":"2025-11-21T12:14:18.318954Z"}},"outputs":[{"name":"stdout","text":"trainable params: 2065922 || all params: 134150404 || trainable%: 1.5400043074040985\n","output_type":"stream"}],"execution_count":19},{"id":"61c5c7a7","cell_type":"code","source":"# evals\n\n\ndef normalize_answer(text):\n    text = (text or \"\").lower()\n    def remove_articles(s):\n        return re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n    def remove_punctuation(s):\n        return \"\".join(ch for ch in s if ch not in string.punctuation)\n    def white_space_fix(s):\n        return \" \".join(s.split())\n    return white_space_fix(remove_articles(remove_punctuation(text)))\n\ndef exact_match_score(prediction, ground_truth):\n    return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n\ndef f1_score(prediction, ground_truth):\n    pred_tokens = normalize_answer(prediction).split()\n    gold_tokens = normalize_answer(ground_truth).split()\n    if not gold_tokens:\n        return 1.0 if not pred_tokens else 0.0\n    if not pred_tokens:\n        return 0.0\n    common = Counter(pred_tokens) & Counter(gold_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0.0\n    precision = num_same / len(pred_tokens)\n    recall = num_same / len(gold_tokens)\n    return 2 * precision * recall / (precision + recall)\n\ndef edit_distance_score(prediction, ground_truth):\n    pred_norm = normalize_answer(prediction)\n    gold_norm = normalize_answer(ground_truth)\n    if not gold_norm and not pred_norm:\n        return 1.0\n    if not gold_norm or not pred_norm:\n        return 0.0\n    distance = Levenshtein.distance(pred_norm, gold_norm)\n    max_len = max(len(pred_norm), len(gold_norm))\n    return 1.0 - (distance / max_len) if max_len > 0 else 1.0\n\ndef gold_answer(example):\n    # Extracts the gold answer substring from the context using character offsets\n    answer = example.get(\"answer\")\n    context = example.get(\"context\")\n    answer_start = example.get(\"answer_start\", -1)\n    if answer and answer_start is not None and answer_start != -1:\n        return context[answer_start: answer_start + len(answer)]\n    return \"[CLS]\"\n\n\ndef decode_prediction(input_ids, start_idx, end_idx, tokenizer=None):\n    if start_idx > end_idx:\n        start_idx, end_idx = end_idx, start_idx\n    if tokenizer is None:\n        raise ValueError(\"Tokenizer must be provided for decoding.\")\n    cls_index = input_ids.index(tokenizer.cls_token_id)\n    # If both point to CLS token, return [CLS] sentinel\n    if start_idx == cls_index and end_idx == cls_index:\n        return \"[CLS]\"\n    start_idx = max(start_idx, 0)\n    end_idx = min(end_idx, len(input_ids) - 1)\n    if start_idx > end_idx:\n        return \"[CLS]\"\n    text = tokenizer.decode(input_ids[start_idx:end_idx + 1], skip_special_tokens=True)\n    text = text.strip()\n    return text if text else \"[CLS]\"\n\n\ndef evaluate_checkpoint(checkpoint_path=None, model_instance=None, eval_dataset=None):\n    \"\"\"Evaluate either a checkpoint path (loads model) or a provided model instance.\n\n    - checkpoint_path: path to checkpoint folder\n    - model_instance: an in-memory model (preferably a PeftModel or CanineForQuestionAnswering)\n    - eval_dataset: optional dataset to evaluate; if None the default processed_val will be used\n    \"\"\"\n    if eval_dataset is None:\n        eval_dataset = processed_val\n\n    # If a model_instance is given, use it directly (avoid re-loading a fresh base model)\n    if model_instance is not None:\n        eval_model = model_instance\n    else:\n        base_model = CanineForQuestionAnswering.from_pretrained(model_name, trust_remote_code=False)\n        eval_model = get_peft_model(base_model, lora_config)\n        # Try loading adapter weights; fall back to PeftModel.from_pretrained if needed\n        try:\n            eval_model.load_adapter(checkpoint_path)\n        except Exception:\n            from peft import PeftModel\n            eval_model = PeftModel.from_pretrained(base_model, checkpoint_path)\n\n    eval_model.to(device)\n\n    eval_args = TrainingArguments(\n        # Small evaluation config; uses cpu/mps if no gpu during eval\n        output_dir=\"outputs/canine-s-uqa\",\n        per_device_eval_batch_size=16,\n        dataloader_drop_last=False,\n        fp16=True,\n        bf16=False,\n        report_to=\"none\",\n    )\n\n    # Run evaluation via a lightweight Trainer so prediction loop is standard\n    eval_trainer = Trainer(\n        model=eval_model,\n        args=eval_args,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n    )\n\n    predictions = eval_trainer.predict(eval_dataset)\n    start_logits, end_logits = predictions.predictions\n    best_predictions = {}\n    for feature_index, feature in enumerate(eval_dataset):\n        sample_idx = int(feature[\"overflow_to_sample_mapping\"])\n        input_ids = feature[\"input_ids\"]\n        start_idx = int(np.argmax(start_logits[feature_index]))\n        end_idx = int(np.argmax(end_logits[feature_index]))\n        score = float(start_logits[feature_index][start_idx] + end_logits[feature_index][end_idx])\n        prediction_text = decode_prediction(input_ids, start_idx, end_idx, tokenizer=tokenizer)\n        stored = best_predictions.get(sample_idx)\n        if stored is None or score > stored[0]:\n            best_predictions[sample_idx] = (score, prediction_text)\n\n    em_scores = []\n    f1_scores = []\n    edit_dist_scores = []\n    for sample_idx, (_, prediction_text) in best_predictions.items():\n        reference = gold_answer(uqa_val[int(sample_idx)])\n        em_scores.append(exact_match_score(prediction_text, reference))\n        f1_scores.append(f1_score(prediction_text, reference))\n        edit_dist_scores.append(edit_distance_score(prediction_text, reference))\n\n    em = float(np.mean(em_scores)) if em_scores else 0.0\n    f1 = float(np.mean(f1_scores)) if f1_scores else 0.0\n    edit_dist = float(np.mean(edit_dist_scores)) if edit_dist_scores else 0.0\n    print(f\"Examples evaluated: {len(em_scores)}\")\n    print(f\"Exact Match: {em * 100:.2f}\")\n    print(f\"F1: {f1 * 100:.2f}\")\n    print(f\"Edit Distance (normalized): {edit_dist * 100:.2f}\")\n    return {\"exact_match\": em, \"f1\": f1, \"edit_distance\": edit_dist}\n","metadata":{"id":"61c5c7a7","trusted":true,"execution":{"iopub.status.busy":"2025-11-21T12:14:18.320382Z","iopub.execute_input":"2025-11-21T12:14:18.320567Z","iopub.status.idle":"2025-11-21T12:14:18.337953Z","shell.execute_reply.started":"2025-11-21T12:14:18.320551Z","shell.execute_reply":"2025-11-21T12:14:18.337063Z"}},"outputs":[],"execution_count":20},{"id":"c4abaaab","cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"outputs/canine-s-uqa\",\n    \n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=16,\n    \n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    \n    num_train_epochs=1,\n    learning_rate=3e-4,\n    weight_decay=0.01,\n    eval_strategy=\"no\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=500,\n    logging_steps=25,\n    fp16=True,\n    bf16=False,\n    report_to=\"none\",\n    push_to_hub=True,\n    hub_model_id=\"VohraAK/canine-s-uqa\",\n    hub_strategy=\"checkpoint\",\n    )\n\nclass CustomEvalCallback(TrainerCallback):\n    def __init__(self, eval_func, eval_dataset, use_in_memory_model=True, verbose=True):\n        self.eval_func = eval_func\n        self.eval_dataset = eval_dataset\n        self.use_in_memory_model = use_in_memory_model\n        self.verbose = verbose\n        # trainer reference (set after trainer exists)\n        self.trainer = None\n\n    def on_save(self, args, state, control, model=None, **kwargs):\n        checkpoint_path = f\"{args.output_dir}/checkpoint-{state.global_step}\"\n        if self.verbose:\n            print(f\"\\nüîç Running custom evaluation at step {state.global_step}...\")\n\n        # Prefer evaluating the in-memory trainer model (fast + avoids re-loading)\n        if self.use_in_memory_model and self.trainer is not None:\n            if self.verbose:\n                print(\"Using in-memory model for evaluation (no reloading).\")\n            try:\n                metrics = self.eval_func(checkpoint_path=None, model_instance=self.trainer.model, eval_dataset=self.eval_dataset)\n            except Exception as e:\n                print(\"‚ö†Ô∏è in-memory evaluation failed, falling back to checkpoint load:\", e)\n                metrics = self.eval_func(checkpoint_path)\n        else:\n            metrics = self.eval_func(checkpoint_path)\n\n        # record metrics in state.log_history\n        state.log_history.append({\n            \"step\": state.global_step,\n            \"eval_exact_match\": metrics.get(\"exact_match\"),\n            \"eval_f1\": metrics.get(\"f1\"),\n            \"eval_edit_distance\": metrics.get(\"edit_distance\"),\n        })\n\n        if self.verbose:\n            print(f\"‚úÖ Step {state.global_step}: EM={metrics.get('exact_match',0)*100:.2f}, F1={metrics.get('f1',0)*100:.2f}, EditDist={metrics.get('edit_distance',0)*100:.2f}\")\n\n        # Update trainer_state.json to include custom metrics\n        state_path = f\"{checkpoint_path}/trainer_state.json\"\n        try:\n            with open(state_path, 'r') as f:\n                state_dict = json.load(f)\n            state_dict['log_history'] = state.log_history\n            with open(state_path, 'w') as f:\n                json.dump(state_dict, f, indent=2)\n            if self.verbose:\n                print(f\"üíæ Updated trainer_state.json with custom metrics\")\n        except Exception as e:\n            if self.verbose:\n                print(f\"‚ö†Ô∏è  Warning: Could not update trainer_state.json: {e}\")\n\n        try:\n            if self.verbose:\n                print(f\"‚òÅÔ∏è  Pushing checkpoint-{state.global_step} to Hub...\")\n            api = HfApi()\n            api.upload_folder(\n                folder_path=checkpoint_path,\n                repo_id=args.hub_model_id,\n                path_in_repo=f\"checkpoint-{state.global_step}\",\n                commit_message=f\"Add checkpoint {state.global_step} (EM={metrics.get('exact_match',0)*100:.1f}%, F1={metrics.get('f1',0)*100:.1f}%)\",\n                repo_type=\"model\"\n            )\n            if self.verbose:\n                print(f\"‚úÖ Pushed checkpoint-{state.global_step} to Hub\")\n        except Exception as e:\n            if self.verbose:\n                print(f\"‚ö†Ô∏è  Warning: Could not push to Hub: {e}\")\n\n        return control","metadata":{"id":"c4abaaab","trusted":true,"execution":{"iopub.status.busy":"2025-11-21T12:14:18.338899Z","iopub.execute_input":"2025-11-21T12:14:18.339233Z","iopub.status.idle":"2025-11-21T12:14:18.384173Z","shell.execute_reply.started":"2025-11-21T12:14:18.339215Z","shell.execute_reply":"2025-11-21T12:14:18.383511Z"}},"outputs":[],"execution_count":21},{"id":"055f5dda","cell_type":"code","source":"trainer_cb = CustomEvalCallback(evaluate_checkpoint, processed_val, use_in_memory_model=True)\n\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=processed_train,\n    eval_dataset=processed_val,\n    callbacks=[trainer_cb],\n)\n","metadata":{"id":"055f5dda","trusted":true,"execution":{"iopub.status.busy":"2025-11-21T12:14:18.384937Z","iopub.execute_input":"2025-11-21T12:14:18.385136Z","iopub.status.idle":"2025-11-21T12:14:18.851640Z","shell.execute_reply.started":"2025-11-21T12:14:18.385120Z","shell.execute_reply":"2025-11-21T12:14:18.851039Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":22},{"id":"TOUimesUX5Re","cell_type":"code","source":"trainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"TOUimesUX5Re","outputId":"c0ada6de-de08-41ce-e8fc-5c59232de758","trusted":true,"execution":{"iopub.status.busy":"2025-11-21T12:14:18.852389Z","iopub.execute_input":"2025-11-21T12:14:18.852674Z","execution_failed":"2025-11-21T12:47:08.367Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2501' max='7313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2501/7313 24:39 < 47:28, 1.69 it/s, Epoch 0.34/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>5.919800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>5.689400</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>5.488100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>5.369100</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>5.171900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>5.064900</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>4.856200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>4.693500</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>4.596600</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>4.623400</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>4.581200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>4.355800</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>4.275100</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>4.207000</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>4.233300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>4.064200</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>3.938500</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>3.933500</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>3.697500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>4.110900</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>3.706500</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>3.684300</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>3.548200</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>3.467700</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>3.711700</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>3.521100</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>3.586200</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>3.446600</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>3.444000</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>3.440900</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>3.247600</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>3.210100</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>3.567900</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>3.155200</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>3.187100</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>3.013900</td>\n    </tr>\n    <tr>\n      <td>925</td>\n      <td>3.355800</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>3.353600</td>\n    </tr>\n    <tr>\n      <td>975</td>\n      <td>3.016300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>3.342900</td>\n    </tr>\n    <tr>\n      <td>1025</td>\n      <td>3.107100</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>3.177400</td>\n    </tr>\n    <tr>\n      <td>1075</td>\n      <td>3.254400</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>3.304500</td>\n    </tr>\n    <tr>\n      <td>1125</td>\n      <td>3.217100</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>3.017900</td>\n    </tr>\n    <tr>\n      <td>1175</td>\n      <td>3.250700</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>2.986900</td>\n    </tr>\n    <tr>\n      <td>1225</td>\n      <td>3.160400</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>3.169300</td>\n    </tr>\n    <tr>\n      <td>1275</td>\n      <td>2.921300</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>3.216300</td>\n    </tr>\n    <tr>\n      <td>1325</td>\n      <td>3.066200</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>2.855000</td>\n    </tr>\n    <tr>\n      <td>1375</td>\n      <td>3.054200</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>3.158500</td>\n    </tr>\n    <tr>\n      <td>1425</td>\n      <td>2.990900</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>3.160900</td>\n    </tr>\n    <tr>\n      <td>1475</td>\n      <td>2.819800</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>3.055800</td>\n    </tr>\n    <tr>\n      <td>1525</td>\n      <td>2.996400</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>3.342700</td>\n    </tr>\n    <tr>\n      <td>1575</td>\n      <td>2.939600</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>2.857700</td>\n    </tr>\n    <tr>\n      <td>1625</td>\n      <td>2.932800</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>3.094600</td>\n    </tr>\n    <tr>\n      <td>1675</td>\n      <td>2.780500</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>2.980300</td>\n    </tr>\n    <tr>\n      <td>1725</td>\n      <td>2.819900</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>2.859200</td>\n    </tr>\n    <tr>\n      <td>1775</td>\n      <td>2.871800</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>2.662600</td>\n    </tr>\n    <tr>\n      <td>1825</td>\n      <td>3.172100</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>2.777800</td>\n    </tr>\n    <tr>\n      <td>1875</td>\n      <td>2.613500</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>2.798100</td>\n    </tr>\n    <tr>\n      <td>1925</td>\n      <td>2.860200</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>2.620500</td>\n    </tr>\n    <tr>\n      <td>1975</td>\n      <td>2.505300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>2.819900</td>\n    </tr>\n    <tr>\n      <td>2025</td>\n      <td>2.532700</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>3.124500</td>\n    </tr>\n    <tr>\n      <td>2075</td>\n      <td>2.794200</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>2.656200</td>\n    </tr>\n    <tr>\n      <td>2125</td>\n      <td>2.791100</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>3.037700</td>\n    </tr>\n    <tr>\n      <td>2175</td>\n      <td>2.844100</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>2.866800</td>\n    </tr>\n    <tr>\n      <td>2225</td>\n      <td>2.768700</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>2.680000</td>\n    </tr>\n    <tr>\n      <td>2275</td>\n      <td>2.897000</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>2.856900</td>\n    </tr>\n    <tr>\n      <td>2325</td>\n      <td>2.964000</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>2.534500</td>\n    </tr>\n    <tr>\n      <td>2375</td>\n      <td>2.452700</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>2.500000</td>\n    </tr>\n    <tr>\n      <td>2425</td>\n      <td>2.726400</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>2.651700</td>\n    </tr>\n    <tr>\n      <td>2475</td>\n      <td>2.730400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nüîç Running custom evaluation at step 500...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_117/733281207.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Examples evaluated: 1000\nExact Match: 32.90\nF1: 32.90\nEdit Distance (normalized): 33.03\n‚úÖ Step 500: EM=32.90, F1=32.90, EditDist=33.03\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-500 to Hub...\n‚úÖ Pushed checkpoint-500 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 1000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_117/733281207.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Examples evaluated: 1000\nExact Match: 33.10\nF1: 33.10\nEdit Distance (normalized): 33.23\n‚úÖ Step 1000: EM=33.10, F1=33.10, EditDist=33.23\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-1000 to Hub...\n‚úÖ Pushed checkpoint-1000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 1500...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_117/733281207.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Examples evaluated: 1000\nExact Match: 33.10\nF1: 33.10\nEdit Distance (normalized): 33.23\n‚úÖ Step 1500: EM=33.10, F1=33.10, EditDist=33.23\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-1500 to Hub...\n‚úÖ Pushed checkpoint-1500 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 2000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_117/733281207.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Examples evaluated: 1000\nExact Match: 33.10\nF1: 33.10\nEdit Distance (normalized): 33.23\n‚úÖ Step 2000: EM=33.10, F1=33.10, EditDist=33.23\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-2000 to Hub...\n‚úÖ Pushed checkpoint-2000 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 2500...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_117/733281207.py:107: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='229' max='1966' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 229/1966 00:26 < 03:18, 8.73 it/s]\n    </div>\n    "},"metadata":{}}],"execution_count":null}]}