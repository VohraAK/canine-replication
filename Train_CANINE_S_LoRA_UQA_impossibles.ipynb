{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:52:15.035568Z",
     "iopub.status.busy": "2025-12-12T13:52:15.034974Z",
     "iopub.status.idle": "2025-12-12T13:52:15.039057Z",
     "shell.execute_reply": "2025-12-12T13:52:15.038290Z",
     "shell.execute_reply.started": "2025-12-12T13:52:15.035543Z"
    },
    "id": "c186240c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %pip install peft evaluate transformers Levenshtein ipywidgets\n",
    "# %pip install protobuf==3.20.3\n",
    "# !rm -rf /kaggle/working/cache\n",
    "# !rm -rf /kaggle/working/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:52:15.040641Z",
     "iopub.status.busy": "2025-12-12T13:52:15.040331Z",
     "iopub.status.idle": "2025-12-12T13:52:15.053754Z",
     "shell.execute_reply": "2025-12-12T13:52:15.053021Z",
     "shell.execute_reply.started": "2025-12-12T13:52:15.040606Z"
    },
    "id": "cd8da8ab",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# X\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_DISABLE_CHAT_TEMPLATES\"] = \"1\"\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_ADDITIONAL_CHAT_TEMPLATES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:52:15.098194Z",
     "iopub.status.busy": "2025-12-12T13:52:15.097584Z",
     "iopub.status.idle": "2025-12-12T13:52:15.103051Z",
     "shell.execute_reply": "2025-12-12T13:52:15.102099Z",
     "shell.execute_reply.started": "2025-12-12T13:52:15.098171Z"
    },
    "id": "d87eba82",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import CanineTokenizer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
    "import json\n",
    "from huggingface_hub import HfApi, notebook_login, whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:52:15.104851Z",
     "iopub.status.busy": "2025-12-12T13:52:15.104405Z",
     "iopub.status.idle": "2025-12-12T13:52:15.123099Z",
     "shell.execute_reply": "2025-12-12T13:52:15.122319Z",
     "shell.execute_reply.started": "2025-12-12T13:52:15.104822Z"
    },
    "id": "0e98cebe-4c08-4850-b3c1-1529564fdb1b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# notebook_login()\n",
    "# whoami()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-12T13:52:15.124095Z",
     "iopub.status.busy": "2025-12-12T13:52:15.123798Z",
     "iopub.status.idle": "2025-12-12T13:52:20.760221Z",
     "shell.execute_reply": "2025-12-12T13:52:20.759617Z",
     "shell.execute_reply.started": "2025-12-12T13:52:15.124072Z"
    },
    "id": "f2dd5a40",
    "outputId": "140c30ea-575d-45cd-ea54-7818cdfe6bf5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CanineTokenizer, CanineForQuestionAnswering\n",
    "import torch\n",
    "model_name = 'google/canine-s'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "tokenizer = CanineTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=False)\n",
    "model = CanineForQuestionAnswering.from_pretrained(model_name, trust_remote_code=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f75a7e9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:52:20.761392Z",
     "iopub.status.busy": "2025-12-12T13:52:20.761114Z",
     "iopub.status.idle": "2025-12-12T13:52:20.765408Z",
     "shell.execute_reply": "2025-12-12T13:52:20.764637Z",
     "shell.execute_reply.started": "2025-12-12T13:52:20.761365Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# funtion to  filter out impossible questions\n",
    "def filter_function(example):\n",
    "    return not example['is_impossible']\n",
    "\n",
    "def only_impossible(example):\n",
    "    return not filter_function(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:52:20.767619Z",
     "iopub.status.busy": "2025-12-12T13:52:20.767382Z",
     "iopub.status.idle": "2025-12-12T13:52:25.949008Z",
     "shell.execute_reply": "2025-12-12T13:52:25.948202Z",
     "shell.execute_reply.started": "2025-12-12T13:52:20.767606Z"
    },
    "id": "d474e2e8",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset after filtering:\n",
      "   Original train size: 124,745\n",
      "   Filtered train size: 41,727\n",
      "   Using for training: 40,000\n",
      "   Validation size: 5,000\n"
     ]
    }
   ],
   "source": [
    "uqa_dataset = load_dataset(\"uqa/UQA\")\n",
    "\n",
    "# filtering\n",
    "uqa_dataset_filtered = uqa_dataset.filter(only_impossible)\n",
    "\n",
    "# uqa_train = uqa_dataset_filtered[\"train\"].shuffle(seed=42)\n",
    "# uqa_val = uqa_dataset_filtered[\"validation\"].shuffle(seed=42)\n",
    "\n",
    "# trying on impossible questions only (decresed to 40000:5000 split due to fewer examples)\n",
    "uqa_train = uqa_dataset_filtered[\"train\"].shuffle(seed=42).select(range(40000))\n",
    "uqa_val = uqa_dataset_filtered[\"validation\"].shuffle(seed=42).select(range(5000))\n",
    "\n",
    "print(f\"üìä Dataset after filtering:\")\n",
    "print(f\"   Original train size: {len(uqa_dataset['train']):,}\")\n",
    "print(f\"   Filtered train size: {len(uqa_dataset_filtered['train']):,}\")\n",
    "print(f\"   Using for training: {len(uqa_train):,}\")\n",
    "print(f\"   Validation size: {len(uqa_val):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "297e1ff7-52f6-4981-b360-45141788f2f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:52:25.949947Z",
     "iopub.status.busy": "2025-12-12T13:52:25.949743Z",
     "iopub.status.idle": "2025-12-12T13:52:25.956627Z",
     "shell.execute_reply": "2025-12-12T13:52:25.955926Z",
     "shell.execute_reply.started": "2025-12-12T13:52:25.949931Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context length (characters): 779\n",
      "Context length (tokens): 779\n",
      "1:1 mapping: True\n"
     ]
    }
   ],
   "source": [
    "# Check character-token alignment\n",
    "ex = uqa_train[444]\n",
    "context = ex[\"context\"]\n",
    "context_tokens = tokenizer.encode(ex[\"context\"], add_special_tokens=False)\n",
    "\n",
    "print(f\"Context length (characters): {len(context)}\")\n",
    "print(f\"Context length (tokens): {len(context_tokens)}\")\n",
    "print(f\"1:1 mapping: {len(context) == len(context_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:52:25.957603Z",
     "iopub.status.busy": "2025-12-12T13:52:25.957412Z",
     "iopub.status.idle": "2025-12-12T13:52:26.363579Z",
     "shell.execute_reply": "2025-12-12T13:52:26.362791Z",
     "shell.execute_reply.started": "2025-12-12T13:52:25.957588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Explore raw UQA dataset structure\n",
    "# print(\"=\"*80)\n",
    "# print(\"UQA DATASET STRUCTURE\")\n",
    "# print(\"=\"*80)\n",
    "# print(f\"Training set size: {len(uqa_train):,} examples\")\n",
    "# print(f\"Validation set size: {len(uqa_val):,} examples\")\n",
    "# print(f\"\\nDataset columns: {uqa_train.column_names}\")\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# # Show a few examples\n",
    "# print(\"\\nüìù EXAMPLE 1 - Question with Answer\")\n",
    "# print(\"=\"*80)\n",
    "# ex1 = uqa_train[0]\n",
    "# print(f\"Question: {ex1['question']}\")\n",
    "# print(f\"\\nContext (first 300 chars): {ex1['context'][:300]}...\")\n",
    "# print(f\"\\nAnswer: '{ex1['answer']}'\")\n",
    "# print(f\"Answer starts at character position: {ex1['answer_start']}\")\n",
    "\n",
    "# # Verify the answer extraction\n",
    "# if ex1['answer_start'] != -1:\n",
    "#     extracted = ex1['context'][ex1['answer_start']:ex1['answer_start']+len(ex1['answer'])]\n",
    "#     print(f\"‚úì Extracted from context: '{extracted}'\")\n",
    "#     print(f\"‚úì Match: {extracted == ex1['answer']}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"\\nüìù EXAMPLE 2 - Another Question\")\n",
    "# print(\"=\"*80)\n",
    "# ex2 = uqa_train[100]\n",
    "# print(f\"Question: {ex2['question']}\")\n",
    "# print(f\"\\nContext length: {len(ex2['context'])} characters\")\n",
    "# print(f\"Answer: '{ex2['answer']}'\")\n",
    "# print(f\"Answer starts at position: {ex2['answer_start']}\")\n",
    "\n",
    "# # Show answer in context\n",
    "# if ex2['answer_start'] != -1:\n",
    "#     start = max(0, ex2['answer_start'] - 50)\n",
    "#     end = min(len(ex2['context']), ex2['answer_start'] + len(ex2['answer']) + 50)\n",
    "#     context_snippet = ex2['context'][start:end]\n",
    "#     answer_pos = ex2['answer_start'] - start\n",
    "#     print(f\"\\nContext around answer:\")\n",
    "#     print(f\"...{context_snippet}...\")\n",
    "#     print(f\"    {' '*answer_pos}{'~'*len(ex2['answer'])} (answer here)\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"\\nüìä DATASET STATISTICS\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Compute some basic statistics\n",
    "# import numpy as np\n",
    "# question_lengths = [len(ex['question']) for ex in uqa_train.select(range(1000))]\n",
    "# context_lengths = [len(ex['context']) for ex in uqa_train.select(range(1000))]\n",
    "# answer_lengths = [len(ex['answer']) if ex['answer'] else 0 for ex in uqa_train.select(range(1000))]\n",
    "# has_answer = [ex['answer_start'] != -1 for ex in uqa_train.select(range(1000))]\n",
    "\n",
    "# print(f\"Question length (chars): mean={np.mean(question_lengths):.1f}, max={np.max(question_lengths)}\")\n",
    "# print(f\"Context length (chars): mean={np.mean(context_lengths):.1f}, max={np.max(context_lengths)}\")\n",
    "# print(f\"Answer length (chars): mean={np.mean(answer_lengths):.1f}, max={np.max(answer_lengths)}\")\n",
    "# print(f\"Questions with answers: {sum(has_answer)/len(has_answer)*100:.1f}%\")\n",
    "# print(f\"Questions without answers: {(1-sum(has_answer)/len(has_answer))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "89c472d5"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "6e80a8d3"
   },
   "source": [
    "## Updated preprocessors!\n",
    "\n",
    "Previously, we tried to apply the same approach we used in TYDIQA on UQA, the problem was the preprocessors were aligning the answer spans in units of **byte-level spans** instead of **character-level spans**. The calculations were adding byte-level offsets to the answer lengths, and since Urdu characters may be quantified in multiple bytes, the model was being fed the wrong spans -> GIGO!\n",
    "\n",
    "We are now testing an updated preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:52:26.364675Z",
     "iopub.status.busy": "2025-12-12T13:52:26.364403Z",
     "iopub.status.idle": "2025-12-12T13:52:26.378985Z",
     "shell.execute_reply": "2025-12-12T13:52:26.378217Z",
     "shell.execute_reply.started": "2025-12-12T13:52:26.364654Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FIXED preprocessing function for UQA with CANINE-S.\n",
    "TyDiQA-style preprocessor adapted for UQA character offsets.\n",
    "\n",
    "Key fixes applied:\n",
    "1. Uses character-level offsets (UQA native format, no byte conversion needed)\n",
    "2. Fixed boundary check: uses `<` instead of `<=` for chunk_end\n",
    "3. Calculates gold_char_end as inclusive (answer_start + len(answer) - 1)\n",
    "4. Dynamic cls_index for no-answer cases\n",
    "5. Simplified context_offset calculation\n",
    "\n",
    "This preprocessor passed all 200 real-world UQA examples in testing.\n",
    "\"\"\"\n",
    "\n",
    "MAX_SEQ_LENGTH = 384\n",
    "DOC_STRIDE = 64  # Using TyDiQA's value for proven results\n",
    "\n",
    "def preprocess_uqa(examples, tokenizer, max_length=MAX_SEQ_LENGTH, doc_stride=DOC_STRIDE, model_obj=None, indices=None):\n",
    "    \"\"\"\n",
    "    TyDiQA-style preprocessor adapted for UQA (character offsets).\n",
    "    \n",
    "    Args:\n",
    "        examples: Batch with question, context, answer, answer_start fields\n",
    "        tokenizer: CanineTokenizer instance\n",
    "        max_length: Maximum sequence length (default 384)\n",
    "        doc_stride: Sliding window overlap (default 64)\n",
    "        model_obj: Optional model object (for compatibility)\n",
    "        indices: Optional example indices for overflow mapping\n",
    "    \n",
    "    Returns:\n",
    "        Dict with input_ids, attention_mask, token_type_ids, start_positions, \n",
    "        end_positions, overflow_to_sample_mapping\n",
    "    \"\"\"\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    contexts = examples[\"context\"]\n",
    "    answers = examples[\"answer\"]\n",
    "    answer_starts = examples[\"answer_start\"]\n",
    "    \n",
    "    special_tokens = tokenizer.num_special_tokens_to_add(pair=True)\n",
    "    \n",
    "    encoded = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"start_positions\": [],\n",
    "        \"end_positions\": [],\n",
    "        \"overflow_to_sample_mapping\": [],\n",
    "    }\n",
    "    \n",
    "    for example_idx, (question, context, answer, answer_start) in enumerate(zip(questions, contexts, answers, answer_starts)):\n",
    "        question_tokens = tokenizer.encode(question, add_special_tokens=False)\n",
    "        context_tokens = tokenizer.encode(context, add_special_tokens=False)\n",
    "        \n",
    "        max_context_tokens = max_length - len(question_tokens) - special_tokens\n",
    "        if max_context_tokens <= 0 or not context_tokens:\n",
    "            continue\n",
    "        \n",
    "        # UQA uses character offsets (not bytes like TyDiQA)\n",
    "        if answer and answer_start != -1:\n",
    "            start_char = answer_start\n",
    "            end_char = answer_start + len(answer) - 1  # Inclusive\n",
    "            answer_span = (start_char, end_char)\n",
    "        else:\n",
    "            answer_span = None\n",
    "        \n",
    "        stride_tokens = max_context_tokens - doc_stride\n",
    "        if stride_tokens <= 0:\n",
    "            stride_tokens = max_context_tokens\n",
    "        \n",
    "        span_start = 0\n",
    "        context_length = len(context_tokens)\n",
    "        while span_start < context_length:\n",
    "            span_end = min(span_start + max_context_tokens, context_length)\n",
    "            context_chunk = context_tokens[span_start:span_end]\n",
    "            \n",
    "            input_ids = tokenizer.build_inputs_with_special_tokens(question_tokens, context_chunk)\n",
    "            token_type_ids = tokenizer.create_token_type_ids_from_sequences(question_tokens, context_chunk)\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            \n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "            context_offset = len(input_ids) - len(context_chunk) - 1\n",
    "            \n",
    "            if answer_span is None:\n",
    "                start_pos = cls_index\n",
    "                end_pos = cls_index\n",
    "            else:\n",
    "                start_char, end_char = answer_span\n",
    "                # CRITICAL FIX: Use < instead of <= for exclusive chunk_end\n",
    "                answer_in_chunk = start_char >= span_start and end_char < span_end\n",
    "                if answer_in_chunk:\n",
    "                    start_pos = context_offset + (start_char - span_start)\n",
    "                    end_pos = context_offset + (end_char - span_start)\n",
    "                else:\n",
    "                    start_pos = cls_index\n",
    "                    end_pos = cls_index\n",
    "            \n",
    "            padding = max_length - len(input_ids)\n",
    "            if padding > 0:\n",
    "                pad_id = tokenizer.pad_token_id\n",
    "                input_ids += [pad_id] * padding\n",
    "                attention_mask += [0] * padding\n",
    "                token_type_ids += [0] * padding\n",
    "            else:\n",
    "                input_ids = input_ids[:max_length]\n",
    "                attention_mask = attention_mask[:max_length]\n",
    "                token_type_ids = token_type_ids[:max_length]\n",
    "                if start_pos >= max_length or end_pos >= max_length:\n",
    "                    start_pos = cls_index\n",
    "                    end_pos = cls_index\n",
    "            \n",
    "            encoded[\"input_ids\"].append(input_ids)\n",
    "            encoded[\"attention_mask\"].append(attention_mask)\n",
    "            encoded[\"token_type_ids\"].append(token_type_ids)\n",
    "            encoded[\"start_positions\"].append(start_pos)\n",
    "            encoded[\"end_positions\"].append(end_pos)\n",
    "            encoded[\"overflow_to_sample_mapping\"].append(example_idx if indices is None else indices[example_idx])\n",
    "            \n",
    "            if span_end == context_length:\n",
    "                break\n",
    "            span_start += stride_tokens\n",
    "    \n",
    "    return encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:52:26.380135Z",
     "iopub.status.busy": "2025-12-12T13:52:26.379876Z",
     "iopub.status.idle": "2025-12-12T13:52:26.396805Z",
     "shell.execute_reply": "2025-12-12T13:52:26.396102Z",
     "shell.execute_reply.started": "2025-12-12T13:52:26.380114Z"
    },
    "id": "a3e95eec",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.QUESTION_ANS,\n",
    "    r=8,   # shadowing tydiqa for now\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"], # shadowing tydiqa for now\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"qa_outputs\"],\n",
    ")\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce2da1b",
   "metadata": {},
   "source": [
    "### Preprocessing examples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:52:26.397923Z",
     "iopub.status.busy": "2025-12-12T13:52:26.397600Z",
     "iopub.status.idle": "2025-12-12T13:52:26.418040Z",
     "shell.execute_reply": "2025-12-12T13:52:26.417356Z",
     "shell.execute_reply.started": "2025-12-12T13:52:26.397905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# print(\"=\"*80)\n",
    "# print(\"üî¨ PREPROCESSING WALKTHROUGH - Single Example\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Take one example\n",
    "# example = uqa_train[0]\n",
    "# print(f\"\\n1Ô∏è‚É£ ORIGINAL DATA\")\n",
    "# print(\"-\"*80)\n",
    "# print(f\"Question: {example['question']}\")\n",
    "# print(f\"Answer: '{example['answer']}'\")\n",
    "# print(f\"Answer position: {example['answer_start']}\")\n",
    "# print(f\"Context length: {len(example['context'])} characters\")\n",
    "\n",
    "# # Preprocess it\n",
    "# batch = {\n",
    "#     'question': [example['question']],\n",
    "#     'context': [example['context']],\n",
    "#     'answer': [example['answer']],\n",
    "#     'answer_start': [example['answer_start']]\n",
    "# }\n",
    "# processed = preprocess_uqa(batch, tokenizer, indices=[0])\n",
    "\n",
    "# print(f\"\\n2Ô∏è‚É£ AFTER PREPROCESSING\")\n",
    "# print(\"-\"*80)\n",
    "# print(f\"Number of chunks created: {len(processed['input_ids'])}\")\n",
    "# print(f\"(Sliding window creates multiple chunks per example)\")\n",
    "\n",
    "# # Show first chunk in detail\n",
    "# chunk_idx = 0\n",
    "# print(f\"\\n3Ô∏è‚É£ CHUNK {chunk_idx} DETAILS\")\n",
    "# print(\"-\"*80)\n",
    "# print(f\"Input IDs length: {len(processed['input_ids'][chunk_idx])} tokens\")\n",
    "# print(f\"Start position: {processed['start_positions'][chunk_idx]}\")\n",
    "# print(f\"End position: {processed['end_positions'][chunk_idx]}\")\n",
    "# print(f\"Maps to original example: {processed['overflow_to_sample_mapping'][chunk_idx]}\")\n",
    "\n",
    "# # Decode the inputs to show what the model sees\n",
    "# input_ids = processed['input_ids'][chunk_idx]\n",
    "# decoded_input = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "# print(f\"\\n4Ô∏è‚É£ DECODED INPUT (first 400 chars, with special tokens)\")\n",
    "# print(\"-\"*80)\n",
    "# print(decoded_input[:400] + \"...\")\n",
    "\n",
    "# # Decode the labeled answer span\n",
    "# start_pos = processed['start_positions'][chunk_idx]\n",
    "# end_pos = processed['end_positions'][chunk_idx]\n",
    "# cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n",
    "\n",
    "# if start_pos == cls_idx and end_pos == cls_idx:\n",
    "#     labeled_answer = \"[NO ANSWER IN THIS CHUNK]\"\n",
    "# else:\n",
    "#     labeled_answer = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True)\n",
    "\n",
    "# print(f\"\\n5Ô∏è‚É£ LABELED ANSWER SPAN IN THIS CHUNK\")\n",
    "# print(\"-\"*80)\n",
    "# print(f\"Gold answer: '{example['answer']}'\")\n",
    "# print(f\"Labeled span: '{labeled_answer}'\")\n",
    "# print(f\"Match: {labeled_answer.strip() == example['answer'].strip()}\")\n",
    "\n",
    "# # Show all chunks for this example\n",
    "# print(f\"\\n6Ô∏è‚É£ ALL CHUNKS FOR THIS EXAMPLE\")\n",
    "# print(\"-\"*80)\n",
    "# for i in range(len(processed['input_ids'])):\n",
    "#     start = processed['start_positions'][i]\n",
    "#     end = processed['end_positions'][i]\n",
    "#     if start == cls_idx and end == cls_idx:\n",
    "#         chunk_answer = \"[NO ANSWER]\"\n",
    "#     else:\n",
    "#         chunk_answer = tokenizer.decode(processed['input_ids'][i][start:end+1], skip_special_tokens=True).strip()\n",
    "#     has_answer = \"‚úÖ\" if chunk_answer == example['answer'].strip() else \"‚ùå\"\n",
    "#     print(f\"  Chunk {i}: {has_answer} '{chunk_answer[:50]}'\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## üîß Preprocessing Exploration: Raw Data ‚Üí Model Input\n",
    "\n",
    "Now let's see what happens during preprocessing - how we convert text to token IDs and create training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:52:26.419150Z",
     "iopub.status.busy": "2025-12-12T13:52:26.418874Z",
     "iopub.status.idle": "2025-12-12T13:52:26.439870Z",
     "shell.execute_reply": "2025-12-12T13:52:26.439127Z",
     "shell.execute_reply.started": "2025-12-12T13:52:26.419125Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_answer(text):\n",
    "    text = (text or \"\").lower()\n",
    "    def remove_articles(s):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n",
    "    def remove_punctuation(s):\n",
    "        return \"\".join(ch for ch in s if ch not in string.punctuation)\n",
    "    def white_space_fix(s):\n",
    "        return \" \".join(s.split())\n",
    "    return white_space_fix(remove_articles(remove_punctuation(text)))\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    pred_tokens = normalize_answer(prediction).split()\n",
    "    gold_tokens = normalize_answer(ground_truth).split()\n",
    "    if not gold_tokens:\n",
    "        return 1.0 if not pred_tokens else 0.0\n",
    "    if not pred_tokens:\n",
    "        return 0.0\n",
    "    common = Counter(pred_tokens) & Counter(gold_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gold_tokens)\n",
    "    # BUGFIX: Prevent division by zero if both precision and recall are 0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def decode_prediction(input_ids, start_idx, end_idx):\n",
    "\n",
    "    global tokenizer\n",
    "    \n",
    "    # Dynamic CLS handling\n",
    "    cls_index = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n",
    "    \n",
    "    # No answer case (both point to CLS)\n",
    "    if start_idx == cls_index and end_idx == cls_index:\n",
    "        return \"\"\n",
    "    \n",
    "    # Invalid range (start after end) - treat as no answer\n",
    "    if start_idx > end_idx:\n",
    "        return \"\"\n",
    "    \n",
    "    # Defensive bounds checking\n",
    "    if start_idx < 0 or end_idx < 0:\n",
    "        return \"\"\n",
    "    if start_idx >= len(input_ids) or end_idx >= len(input_ids):\n",
    "        return \"\"\n",
    "    \n",
    "    # Clamp to valid range (additional safety)\n",
    "    start_idx = max(start_idx, 0)\n",
    "    end_idx = min(end_idx, len(input_ids) - 1)\n",
    "    \n",
    "    # Decode with inclusive slicing [start:end+1]\n",
    "    text = tokenizer.decode(input_ids[start_idx:end_idx + 1], skip_special_tokens=True)\n",
    "    return text.strip()\n",
    "\n",
    "def gold_answer(example):\n",
    "    if example[\"answer_start\"] == -1:\n",
    "        return \"\"\n",
    "    return example[\"answer\"]\n",
    "\n",
    "def edit_distance_score(prediction, ground_truth):\n",
    "    return Levenshtein.ratio(normalize_answer(prediction), normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "#--- CHANGED TO MATCH TYDIQA APPROACH\n",
    "def evaluate_checkpoint(checkpoint_path=None):\n",
    "    \"\"\"\n",
    "    EXACT REPLICA of TyDiQA evaluation approach.\n",
    "    Loads checkpoint from disk (or uses provided path).\n",
    "    \"\"\"\n",
    "    # Load base model + trained adapter (TyDiQA approach)\n",
    "    base_model = CanineForQuestionAnswering.from_pretrained(\n",
    "        model_name, \n",
    "        trust_remote_code=False\n",
    "    )\n",
    "    \n",
    "    from peft import PeftModel\n",
    "    model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Exact TyDiQA eval args\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=\"outputs/canine-s-uqa-impossible\",\n",
    "        per_device_eval_batch_size=1,  # Match TyDiQA exactly\n",
    "        dataloader_drop_last=False,\n",
    "        fp16=False,  # TyDiQA uses False\n",
    "        bf16=False,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    eval_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=eval_args,\n",
    "        eval_dataset=processed_val,\n",
    "        processing_class=tokenizer,  # Use processing_class\n",
    "    )\n",
    "    \n",
    "    # Progress bar (optional, TyDiQA has this)\n",
    "    print(f\"üß™ Evaluating checkpoint: {checkpoint_path}\")\n",
    "    from tqdm.auto import tqdm\n",
    "    with tqdm(total=len(processed_val), desc=\"Evaluating\", unit=\"samples\") as pbar:\n",
    "        predictions = eval_trainer.predict(processed_val)\n",
    "        pbar.update(len(processed_val))\n",
    "    \n",
    "    start_logits, end_logits = predictions.predictions\n",
    "    \n",
    "    # EXACT TyDiQA aggregation logic\n",
    "    best_predictions = {}\n",
    "    for feature_index, feature in enumerate(processed_val):\n",
    "        sample_idx = int(feature[\"overflow_to_sample_mapping\"])\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        \n",
    "        start_idx = int(np.argmax(start_logits[feature_index]))\n",
    "        end_idx = int(np.argmax(end_logits[feature_index]))\n",
    "        score = float(start_logits[feature_index][start_idx] + end_logits[feature_index][end_idx])\n",
    "        prediction_text = decode_prediction(input_ids, start_idx, end_idx)\n",
    "        \n",
    "        stored = best_predictions.get(sample_idx)\n",
    "        if stored is None or score > stored[0]:\n",
    "            best_predictions[sample_idx] = (score, prediction_text)\n",
    "\n",
    "    # TEST!\n",
    "    # After best_predictions loop, before computing metrics:\n",
    "    print(f\"\\nüîç Debug: Sample predictions:\")\n",
    "    for idx in list(best_predictions.keys())[:5]:\n",
    "        score, pred = best_predictions[idx]\n",
    "        gold = gold_answer(uqa_val[idx])\n",
    "        print(f\"  Pred: '{pred[:50]}' | Gold: '{gold[:50]}'\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    em_scores = []\n",
    "    f1_scores = []\n",
    "    edit_dist_scores = []\n",
    "    for sample_idx, (_, prediction_text) in best_predictions.items():\n",
    "        reference = gold_answer(uqa_val[int(sample_idx)])\n",
    "        em_scores.append(exact_match_score(prediction_text, reference))\n",
    "        f1_scores.append(f1_score(prediction_text, reference))\n",
    "        edit_dist_scores.append(edit_distance_score(prediction_text, reference))\n",
    "    \n",
    "    em = float(np.mean(em_scores)) if em_scores else 0.0\n",
    "    f1 = float(np.mean(f1_scores)) if f1_scores else 0.0\n",
    "    edit_dist = float(np.mean(edit_dist_scores)) if edit_dist_scores else 0.0\n",
    "    \n",
    "    print(f\"Examples evaluated: {len(em_scores)}\")\n",
    "    print(f\"Exact Match: {em * 100:.2f}\")\n",
    "    print(f\"F1: {f1 * 100:.2f}\")\n",
    "    print(f\"Edit Distance (normalized): {edit_dist * 100:.2f}\")\n",
    "    \n",
    "    return {\"exact_match\": em, \"f1\": f1, \"edit_distance\": edit_dist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "b1984a6d29864e2d940119370816a37e",
      "a307de6c263a4c20a6418344cbd98c0c",
      "1db208af0dbc4f2facee78148266a207",
      "d44d38a959ec44aa90e91c15a83abbd6",
      "527baa5fc421480da4d2dc7041e19b1f",
      "d398c81b546d4527a41dd97bd87ad7d8",
      "ab4047c7f0144667857fe835d452f6c7",
      "0120d513dd4d4fccac2d528eb7ff4696",
      "c3e0981c2924416fbdf9ceab3e6b04ab",
      "bc970c64373f4f69b6c6936087ed978a",
      "4a74d22a2c334fbda54a95c5e29e712a"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-12-12T13:52:26.441006Z",
     "iopub.status.busy": "2025-12-12T13:52:26.440752Z",
     "iopub.status.idle": "2025-12-12T13:54:25.904017Z",
     "shell.execute_reply": "2025-12-12T13:54:25.903135Z",
     "shell.execute_reply.started": "2025-12-12T13:52:26.440989Z"
    },
    "id": "d11807b9",
    "outputId": "64fc2534-2871-4bd2-b3fa-4b37973486e2",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874dee823c5a4654a302a494d9dc958b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2365 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c832e5c7754a778ba60f45e4bd9e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‚ö†Ô∏è CRITICAL: Must regenerate preprocessed data with FILTERED dataset\n",
    "# The old cache was created from unfiltered data - indices won't match!\n",
    "\n",
    "# print(\"üîÑ Preprocessing filtered dataset (this will take a few minutes)...\")\n",
    "processed_train = uqa_train.map(\n",
    "    lambda examples, indices: preprocess_uqa(examples, tokenizer, indices=indices), \n",
    "    batched=True, \n",
    "    remove_columns=uqa_train.column_names, \n",
    "    with_indices=True\n",
    ")\n",
    "processed_val = uqa_val.map(\n",
    "    lambda examples, indices: preprocess_uqa(examples, tokenizer, indices=indices), \n",
    "    batched=True, \n",
    "    remove_columns=uqa_val.column_names, \n",
    "    with_indices=True\n",
    ")\n",
    "\n",
    "# print(f\"‚úÖ Preprocessing complete!\")\n",
    "# print(f\"   Training chunks: {len(processed_train):,}\")\n",
    "# print(f\"   Validation chunks: {len(processed_val):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d909f3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:54:25.905909Z",
     "iopub.status.busy": "2025-12-12T13:54:25.905603Z",
     "iopub.status.idle": "2025-12-12T13:54:25.910445Z",
     "shell.execute_reply": "2025-12-12T13:54:25.909574Z",
     "shell.execute_reply.started": "2025-12-12T13:54:25.905889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(\"=\"*80)\n",
    "# print(\"üìà DATASET STATISTICS AFTER PREPROCESSING\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Count chunks per example\n",
    "# from collections import Counter\n",
    "# chunks_per_example = Counter(processed_train[\"overflow_to_sample_mapping\"])\n",
    "# chunks_distribution = Counter(chunks_per_example.values())\n",
    "\n",
    "# print(f\"\\nüì¶ Chunks Distribution:\")\n",
    "# print(f\"   Total original examples: {len(uqa_train):,}\")\n",
    "# print(f\"   Total preprocessed chunks: {len(processed_train):,}\")\n",
    "# print(f\"   Average chunks per example: {len(processed_train)/len(uqa_train):.2f}\")\n",
    "# print(f\"\\n   Distribution:\")\n",
    "# for num_chunks in sorted(chunks_distribution.keys())[:10]:\n",
    "#     count = chunks_distribution[num_chunks]\n",
    "#     print(f\"     {num_chunks} chunk(s): {count:,} examples ({count/len(uqa_train)*100:.1f}%)\")\n",
    "\n",
    "# # Count examples with answers in at least one chunk\n",
    "# examples_with_answers = 0\n",
    "# for orig_idx in range(len(uqa_train)):\n",
    "#     # Find all chunks for this example\n",
    "#     chunk_indices = [i for i, x in enumerate(processed_train[\"overflow_to_sample_mapping\"]) if x == orig_idx]\n",
    "    \n",
    "#     # Check if any chunk has an answer (not pointing to CLS)\n",
    "#     has_answer = False\n",
    "#     for chunk_idx in chunk_indices:\n",
    "#         input_ids = processed_train[chunk_idx][\"input_ids\"]\n",
    "#         start_pos = processed_train[chunk_idx][\"start_positions\"]\n",
    "#         end_pos = processed_train[chunk_idx][\"end_positions\"]\n",
    "#         cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n",
    "        \n",
    "#         if not (start_pos == cls_idx and end_pos == cls_idx):\n",
    "#             has_answer = True\n",
    "#             break\n",
    "    \n",
    "#     if has_answer:\n",
    "#         examples_with_answers += 1\n",
    "\n",
    "# print(f\"\\n‚úÖ Answer Coverage:\")\n",
    "# print(f\"   Examples with answer in at least one chunk: {examples_with_answers:,}/{len(uqa_train):,} ({examples_with_answers/len(uqa_train)*100:.1f}%)\")\n",
    "# print(f\"   Expected: ~100% (since we filtered impossible questions)\")\n",
    "\n",
    "# print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "917b817e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:54:25.913750Z",
     "iopub.status.busy": "2025-12-12T13:54:25.913490Z",
     "iopub.status.idle": "2025-12-12T13:54:26.639308Z",
     "shell.execute_reply": "2025-12-12T13:54:26.638540Z",
     "shell.execute_reply.started": "2025-12-12T13:54:25.913706Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(\"=\"*80)\n",
    "# print(\"üîç BOUNDARY LOGIC VERIFICATION\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Test the critical boundary check logic\n",
    "# # Find examples where answer is near chunk boundaries\n",
    "\n",
    "# boundary_cases_found = 0\n",
    "# boundary_cases_correct = 0\n",
    "\n",
    "# for proc_idx in random.sample(range(len(processed_train)), min(500, len(processed_train))):\n",
    "#     proc_example = processed_train[proc_idx]\n",
    "#     orig_idx = proc_example[\"overflow_to_sample_mapping\"]\n",
    "#     orig_example = uqa_train[orig_idx]\n",
    "    \n",
    "#     input_ids = proc_example[\"input_ids\"]\n",
    "#     start_pos = proc_example[\"start_positions\"]\n",
    "#     end_pos = proc_example[\"end_positions\"]\n",
    "    \n",
    "#     cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n",
    "    \n",
    "#     # Skip no-answer cases\n",
    "#     if start_pos == cls_idx and end_pos == cls_idx:\n",
    "#         continue\n",
    "    \n",
    "#     # Check if this is a boundary case (answer near end of chunk)\n",
    "#     # Context starts after first SEP token\n",
    "#     sep_indices = [k for k, x in enumerate(input_ids) if x == tokenizer.sep_token_id]\n",
    "#     if not sep_indices:\n",
    "#         continue\n",
    "    \n",
    "#     context_start = sep_indices[0] + 1\n",
    "#     # Find context end (before padding or second SEP)\n",
    "#     try:\n",
    "#         context_end = sep_indices[1] if len(sep_indices) > 1 else len(input_ids)\n",
    "#     except:\n",
    "#         context_end = len(input_ids)\n",
    "    \n",
    "#     # Check if answer ends near chunk boundary (within last 10 tokens)\n",
    "#     if context_end - end_pos <= 10:\n",
    "#         boundary_cases_found += 1\n",
    "        \n",
    "#         # Verify the answer is correct\n",
    "#         predicted_answer = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True).strip()\n",
    "#         gold_ans = orig_example[\"answer\"].strip()\n",
    "        \n",
    "#         if predicted_answer == gold_ans:\n",
    "#             boundary_cases_correct += 1\n",
    "\n",
    "# print(f\"\\nüìä Boundary cases found: {boundary_cases_found}\")\n",
    "# if boundary_cases_found > 0:\n",
    "#     print(f\"‚úÖ Boundary cases correct: {boundary_cases_correct}/{boundary_cases_found} ({boundary_cases_correct/boundary_cases_found*100:.1f}%)\")\n",
    "# else:\n",
    "#     print(f\"‚ö†Ô∏è  No boundary cases found in sample (may need more examples)\")\n",
    "\n",
    "# print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd016dd8",
   "metadata": {},
   "source": [
    "## ‚úÖ Verification: Test Preprocessed Results\n",
    "\n",
    "Before training, let's verify that the new preprocessor produces correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58a880a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:54:26.640197Z",
     "iopub.status.busy": "2025-12-12T13:54:26.640020Z",
     "iopub.status.idle": "2025-12-12T13:54:29.669413Z",
     "shell.execute_reply": "2025-12-12T13:54:29.668521Z",
     "shell.execute_reply.started": "2025-12-12T13:54:26.640184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(\"=\"*80)\n",
    "# print(\"üß™ TEST 1: Training Data Integrity\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Verify training data format\n",
    "# print(\"\\n1Ô∏è‚É£ Checking training dataset structure...\")\n",
    "# required_columns = [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"start_positions\", \"end_positions\", \"overflow_to_sample_mapping\"]\n",
    "# missing = [col for col in required_columns if col not in processed_train.column_names]\n",
    "\n",
    "# if missing:\n",
    "#     print(f\"‚ùå CRITICAL: Missing columns: {missing}\")\n",
    "# else:\n",
    "#     print(f\"‚úÖ All required columns present: {required_columns}\")\n",
    "\n",
    "# # Check shapes and ranges\n",
    "# print(\"\\n2Ô∏è‚É£ Validating tensor shapes and ranges...\")\n",
    "# issues = []\n",
    "\n",
    "# for i in range(min(100, len(processed_train))):\n",
    "#     example = processed_train[i]\n",
    "    \n",
    "#     # Check lengths\n",
    "#     if len(example[\"input_ids\"]) != MAX_SEQ_LENGTH:\n",
    "#         issues.append(f\"Example {i}: input_ids length {len(example['input_ids'])} != {MAX_SEQ_LENGTH}\")\n",
    "#     if len(example[\"attention_mask\"]) != MAX_SEQ_LENGTH:\n",
    "#         issues.append(f\"Example {i}: attention_mask length mismatch\")\n",
    "#     if len(example[\"token_type_ids\"]) != MAX_SEQ_LENGTH:\n",
    "#         issues.append(f\"Example {i}: token_type_ids length mismatch\")\n",
    "    \n",
    "#     # Check position ranges\n",
    "#     start = example[\"start_positions\"]\n",
    "#     end = example[\"end_positions\"]\n",
    "#     if start < 0 or start >= MAX_SEQ_LENGTH:\n",
    "#         issues.append(f\"Example {i}: start_position {start} out of range\")\n",
    "#     if end < 0 or end >= MAX_SEQ_LENGTH:\n",
    "#         issues.append(f\"Example {i}: end_position {end} out of range\")\n",
    "#     if start > end:\n",
    "#         issues.append(f\"Example {i}: start {start} > end {end}\")\n",
    "\n",
    "# if issues:\n",
    "#     print(f\"‚ùå Found {len(issues)} issues:\")\n",
    "#     for issue in issues[:10]:  # Show first 10\n",
    "#         print(f\"   {issue}\")\n",
    "# else:\n",
    "#     print(f\"‚úÖ All shapes and ranges valid (checked 100 examples)\")\n",
    "\n",
    "# # Check overflow mapping\n",
    "# print(\"\\n3Ô∏è‚É£ Validating overflow_to_sample_mapping...\")\n",
    "# max_orig_idx = max(processed_train[\"overflow_to_sample_mapping\"])\n",
    "# if max_orig_idx >= len(uqa_train):\n",
    "#     print(f\"‚ùå CRITICAL: overflow_to_sample_mapping has index {max_orig_idx} >= dataset size {len(uqa_train)}\")\n",
    "# else:\n",
    "#     print(f\"‚úÖ overflow_to_sample_mapping valid (max={max_orig_idx}, dataset size={len(uqa_train)})\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e0279bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:54:29.670773Z",
     "iopub.status.busy": "2025-12-12T13:54:29.670375Z",
     "iopub.status.idle": "2025-12-12T13:54:33.006852Z",
     "shell.execute_reply": "2025-12-12T13:54:33.005841Z",
     "shell.execute_reply.started": "2025-12-12T13:54:29.670746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# from collections import defaultdict\n",
    "\n",
    "# print(\"=\"*80)\n",
    "# print(\"üß™ FIXED TEST: Answer Extraction Accuracy (OPTIMIZED)\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Step 1: Build reverse index ONCE (O(n) instead of O(n¬≤))\n",
    "# print(\"Building chunk index...\")\n",
    "# chunk_index = defaultdict(list)\n",
    "# for chunk_idx, sample_idx in enumerate(processed_train[\"overflow_to_sample_mapping\"]):\n",
    "#     chunk_index[sample_idx].append(chunk_idx)\n",
    "\n",
    "# # Step 2: Test on random original examples\n",
    "# num_samples = 200\n",
    "# test_orig_indices = random.sample(range(len(uqa_train)), num_samples)\n",
    "\n",
    "# correct = 0\n",
    "# incorrect = 0\n",
    "# failed_examples = []\n",
    "\n",
    "# for orig_idx in test_orig_indices:\n",
    "#     orig_example = uqa_train[orig_idx]\n",
    "#     gold_ans = orig_example[\"answer\"].strip()\n",
    "    \n",
    "#     # Get all chunks for this example (O(1) lookup!)\n",
    "#     chunk_indices = chunk_index[orig_idx]\n",
    "    \n",
    "#     # Check if ANY chunk has the correct answer\n",
    "#     found_correct = False\n",
    "#     for chunk_idx in chunk_indices:\n",
    "#         proc = processed_train[chunk_idx]\n",
    "#         input_ids = proc[\"input_ids\"]\n",
    "#         start = proc[\"start_positions\"]\n",
    "#         end = proc[\"end_positions\"]\n",
    "        \n",
    "#         cls_idx = input_ids.index(tokenizer.cls_token_id)\n",
    "        \n",
    "#         # Skip chunks without answer\n",
    "#         if start == cls_idx and end == cls_idx:\n",
    "#             continue\n",
    "        \n",
    "#         # Extract answer\n",
    "#         predicted = tokenizer.decode(input_ids[start:end+1], skip_special_tokens=True).strip()\n",
    "        \n",
    "#         if predicted.lower() == gold_ans.lower():\n",
    "#             found_correct = True\n",
    "#             break\n",
    "    \n",
    "#     if found_correct or not gold_ans:\n",
    "#         correct += 1\n",
    "#     else:\n",
    "#         incorrect += 1\n",
    "#         if len(failed_examples) < 5:\n",
    "#             failed_examples.append({\n",
    "#                 \"idx\": orig_idx,\n",
    "#                 \"question\": orig_example[\"question\"][:60],\n",
    "#                 \"gold\": gold_ans[:50],\n",
    "#                 \"num_chunks\": len(chunk_indices)\n",
    "#             })\n",
    "\n",
    "# accuracy = correct / num_samples * 100\n",
    "# print(f\"\\nüìä Results: ‚úÖ {correct}/{num_samples} ({accuracy:.1f}%)\")\n",
    "\n",
    "# if accuracy >= 95:\n",
    "#     print(\"‚úÖ PASSED - Preprocessor working correctly!\")\n",
    "# else:\n",
    "#     print(f\"‚ùå FAILED - Only {accuracy:.1f}% accuracy\")\n",
    "#     if failed_examples:\n",
    "#         print(f\"\\n‚ö†Ô∏è First {len(failed_examples)} failures:\")\n",
    "#         for ex in failed_examples:\n",
    "#             print(f\"  #{ex['idx']}: '{ex['question']}...'\")\n",
    "#             print(f\"    Expected: '{ex['gold']}', Chunks: {ex['num_chunks']}\")\n",
    "\n",
    "# print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29130506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:54:33.007987Z",
     "iopub.status.busy": "2025-12-12T13:54:33.007675Z",
     "iopub.status.idle": "2025-12-12T13:54:33.434829Z",
     "shell.execute_reply": "2025-12-12T13:54:33.434031Z",
     "shell.execute_reply.started": "2025-12-12T13:54:33.007954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(\"=\"*80)\n",
    "# print(\"üß™ TEST 3: Validation Data Integrity\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Same checks for validation data\n",
    "# print(\"\\n1Ô∏è‚É£ Checking validation dataset structure...\")\n",
    "# missing_val = [col for col in required_columns if col not in processed_val.column_names]\n",
    "\n",
    "# if missing_val:\n",
    "#     print(f\"‚ùå CRITICAL: Missing columns: {missing_val}\")\n",
    "# # else:\n",
    "#     print(f\"‚úÖ All required columns present\")\n",
    "\n",
    "# # Check validation mapping\n",
    "# print(\"\\n2Ô∏è‚É£ Validating overflow_to_sample_mapping...\")\n",
    "# max_val_idx = max(processed_val[\"overflow_to_sample_mapping\"])\n",
    "# if max_val_idx >= len(uqa_val):\n",
    "#     print(f\"‚ùå CRITICAL: overflow_to_sample_mapping has index {max_val_idx} >= dataset size {len(uqa_val)}\")\n",
    "# else:\n",
    "#     print(f\"‚úÖ overflow_to_sample_mapping valid (max={max_val_idx}, dataset size={len(uqa_val)})\")\n",
    "\n",
    "# # Test extraction on validation\n",
    "# print(\"\\n3Ô∏è‚É£ Testing answer extraction on validation set...\")\n",
    "# val_correct = 0\n",
    "# val_incorrect = 0\n",
    "# val_samples = min(100, len(processed_val))\n",
    "\n",
    "# for proc_idx in range(val_samples):\n",
    "#     proc_example = processed_val[proc_idx]\n",
    "#     orig_idx = proc_example[\"overflow_to_sample_mapping\"]\n",
    "#     orig_example = uqa_val[orig_idx]\n",
    "    \n",
    "#     input_ids = proc_example[\"input_ids\"]\n",
    "#     start_pos = proc_example[\"start_positions\"]\n",
    "#     end_pos = proc_example[\"end_positions\"]\n",
    "    \n",
    "#     cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n",
    "    \n",
    "#     if start_pos == cls_idx and end_pos == cls_idx:\n",
    "#         predicted_answer = \"\"\n",
    "#     else:\n",
    "#         predicted_answer = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True).strip()\n",
    "    \n",
    "#     gold_ans = orig_example[\"answer\"].strip()\n",
    "\n",
    "#     print(f\"GOLD: {gold_ans.lower()}\")\n",
    "#     print(f\"PREDICTED: {predicted_answer.lower()}\\n\")\n",
    "    \n",
    "#     if predicted_answer.lower() == gold_ans.lower():\n",
    "#         val_correct += 1\n",
    "#     else:\n",
    "#         val_incorrect += 1\n",
    "\n",
    "# val_accuracy = val_correct / val_samples * 100\n",
    "# print(f\"   Validation accuracy: {val_correct}/{val_samples} ({val_accuracy:.1f}%)\")\n",
    "\n",
    "# if val_accuracy < 95:\n",
    "#     print(f\"   ‚ùå WARNING: Validation accuracy is low!\")\n",
    "# else:\n",
    "#     print(f\"   ‚úÖ Validation data is correct!\")\n",
    "\n",
    "# print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2600f9-5e66-462f-a6d6-198fde492516",
   "metadata": {},
   "source": [
    "### !!! KEY TAKEAWAY FROM ABOVE CELLS!\n",
    "\n",
    "A lot of chunks do not have the answer in the chunked context, so (0, 0) -> `[CLS]` tok is being predicted!\n",
    "This may be giving way to a lot of mispredictions in evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f38b20f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:54:33.435921Z",
     "iopub.status.busy": "2025-12-12T13:54:33.435664Z",
     "iopub.status.idle": "2025-12-12T13:54:33.449489Z",
     "shell.execute_reply": "2025-12-12T13:54:33.448755Z",
     "shell.execute_reply.started": "2025-12-12T13:54:33.435903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(\"=\"*80)\n",
    "# print(\"üß™ TEST 4: Evaluation Functions Correctness\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Test the metric functions\n",
    "# print(\"\\n1Ô∏è‚É£ Testing normalize_answer()...\")\n",
    "# test_cases = [\n",
    "#     (\"Hello World\", \"hello world\"),\n",
    "#     (\"The quick fox\", \"quick fox\"),\n",
    "#     (\"Test!\", \"test\"),\n",
    "#     (\"  spaces  \", \"spaces\"),\n",
    "# ]\n",
    "\n",
    "# for input_text, expected in test_cases:\n",
    "#     result = normalize_answer(input_text)\n",
    "#     status = \"‚úÖ\" if result == expected else \"‚ùå\"\n",
    "#     print(f\"   {status} normalize_answer('{input_text}') = '{result}' (expected: '{expected}')\")\n",
    "\n",
    "# # Test exact_match_score\n",
    "# print(\"\\n2Ô∏è‚É£ Testing exact_match_score()...\")\n",
    "# em_tests = [\n",
    "#     (\"hello\", \"hello\", 1.0),\n",
    "#     (\"hello\", \"Hello\", 1.0),  # Case insensitive\n",
    "#     (\"the answer\", \"answer\", 1.0),  # Articles removed\n",
    "#     (\"hello\", \"world\", 0.0),\n",
    "#     (\"\", \"\", 1.0),\n",
    "# ]\n",
    "\n",
    "# for pred, gold, expected in em_tests:\n",
    "#     result = exact_match_score(pred, gold)\n",
    "#     status = \"‚úÖ\" if result == expected else \"‚ùå\"\n",
    "#     print(f\"   {status} EM('{pred}', '{gold}') = {result} (expected: {expected})\")\n",
    "\n",
    "# # Test f1_score\n",
    "# print(\"\\n3Ô∏è‚É£ Testing f1_score()...\")\n",
    "# f1_tests = [\n",
    "#     (\"hello world\", \"hello world\", 1.0),\n",
    "#     (\"hello\", \"world\", 0.0),\n",
    "#     (\"hello world\", \"hello\", 0.67),  # Approximate\n",
    "#     (\"\", \"\", 1.0),\n",
    "#     (\"hello\", \"\", 0.0),\n",
    "# ]\n",
    "\n",
    "# all_f1_ok = True\n",
    "# for pred, gold, expected in f1_tests:\n",
    "#     result = f1_score(pred, gold)\n",
    "#     # Allow small tolerance for floating point\n",
    "#     ok = abs(result - expected) < 0.01 or (expected == 0 and result == 0)\n",
    "#     status = \"‚úÖ\" if ok else \"‚ùå\"\n",
    "#     if not ok:\n",
    "#         all_f1_ok = False\n",
    "#     print(f\"   {status} F1('{pred}', '{gold}') = {result:.2f} (expected: ~{expected})\")\n",
    "\n",
    "# # Test decode_prediction\n",
    "# print(\"\\n4Ô∏è‚É£ Testing decode_prediction()...\")\n",
    "# sample_ids = tokenizer.encode(\"This is a test answer\", add_special_tokens=True)\n",
    "# cls_idx = sample_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "# decode_tests = [\n",
    "#     (sample_ids, cls_idx, cls_idx, \"\"),  # No answer case\n",
    "#     (sample_ids, 5, 3, \"\"),  # Invalid range (start > end)\n",
    "#     (sample_ids, -1, 5, \"\"),  # Negative index\n",
    "#     (sample_ids, 2, 5, \"non-empty\"),  # Valid range should return something\n",
    "# ]\n",
    "\n",
    "# for ids, start, end, expected_type in decode_tests:\n",
    "#     result = decode_prediction(ids, start, end)\n",
    "#     if expected_type == \"\":\n",
    "#         ok = result == \"\"\n",
    "#         status = \"‚úÖ\" if ok else \"‚ùå\"\n",
    "#         print(f\"   {status} decode_prediction(..., {start}, {end}) = '{result}' (expected empty)\")\n",
    "#     else:\n",
    "#         ok = len(result) > 0\n",
    "#         status = \"‚úÖ\" if ok else \"‚ùå\"\n",
    "#         print(f\"   {status} decode_prediction(..., {start}, {end}) = '{result}' (expected non-empty)\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d93ea4be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:54:33.451425Z",
     "iopub.status.busy": "2025-12-12T13:54:33.450762Z",
     "iopub.status.idle": "2025-12-12T13:54:34.401959Z",
     "shell.execute_reply": "2025-12-12T13:54:34.401188Z",
     "shell.execute_reply.started": "2025-12-12T13:54:33.451403Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(\"=\"*80)\n",
    "# print(\"üß™ TEST 5: Model Forward Pass (Sanity Check)\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Test that model can process a batch\n",
    "# print(\"\\n1Ô∏è‚É£ Testing model forward pass...\")\n",
    "\n",
    "# try:\n",
    "#     # Take a small batch\n",
    "#     batch_size = 4\n",
    "#     sample_batch = processed_train.select(range(batch_size))\n",
    "    \n",
    "#     # Convert to tensors\n",
    "#     input_ids = torch.tensor(sample_batch[\"input_ids\"]).to(device)\n",
    "#     attention_mask = torch.tensor(sample_batch[\"attention_mask\"]).to(device)\n",
    "#     token_type_ids = torch.tensor(sample_batch[\"token_type_ids\"]).to(device)\n",
    "#     start_positions = torch.tensor(sample_batch[\"start_positions\"]).to(device)\n",
    "#     end_positions = torch.tensor(sample_batch[\"end_positions\"]).to(device)\n",
    "    \n",
    "#     print(f\"   Input shape: {input_ids.shape}\")\n",
    "#     print(f\"   Attention mask shape: {attention_mask.shape}\")\n",
    "#     print(f\"   Token type IDs shape: {token_type_ids.shape}\")\n",
    "    \n",
    "#     # Forward pass\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             token_type_ids=token_type_ids,\n",
    "#             start_positions=start_positions,\n",
    "#             end_positions=end_positions\n",
    "#         )\n",
    "    \n",
    "#     print(f\"\\n   ‚úÖ Forward pass successful!\")\n",
    "#     print(f\"   Loss: {outputs.loss.item():.4f}\")\n",
    "#     print(f\"   Start logits shape: {outputs.start_logits.shape}\")\n",
    "#     print(f\"   End logits shape: {outputs.end_logits.shape}\")\n",
    "    \n",
    "#     # Check logits are valid\n",
    "#     if torch.isnan(outputs.start_logits).any() or torch.isnan(outputs.end_logits).any():\n",
    "#         print(f\"   ‚ùå WARNING: NaN values in logits!\")\n",
    "#     else:\n",
    "#         print(f\"   ‚úÖ Logits are valid (no NaN)\")\n",
    "    \n",
    "#     # Check loss is reasonable\n",
    "#     if outputs.loss.item() < 0 or outputs.loss.item() > 100:\n",
    "#         print(f\"   ‚ö†Ô∏è  WARNING: Loss seems unusual: {outputs.loss.item()}\")\n",
    "#     else:\n",
    "#         print(f\"   ‚úÖ Loss is in reasonable range\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"   ‚ùå CRITICAL ERROR during forward pass: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5cb1b468",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:54:34.403034Z",
     "iopub.status.busy": "2025-12-12T13:54:34.402813Z",
     "iopub.status.idle": "2025-12-12T13:54:35.483471Z",
     "shell.execute_reply": "2025-12-12T13:54:35.482790Z",
     "shell.execute_reply.started": "2025-12-12T13:54:34.403017Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(\"=\"*80)\n",
    "# print(\"üß™ TEST 6: Critical Boundary Cases\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Verify the fix for the <= vs < bug\n",
    "# print(\"\\n1Ô∏è‚É£ Testing chunk boundary logic (the critical bug fix)...\")\n",
    "\n",
    "# boundary_correct = 0\n",
    "# boundary_total = 0\n",
    "\n",
    "# for proc_idx in range(min(1000, len(processed_train))):\n",
    "#     proc_example = processed_train[proc_idx]\n",
    "#     orig_idx = proc_example[\"overflow_to_sample_mapping\"]\n",
    "#     orig_example = uqa_train[orig_idx]\n",
    "    \n",
    "#     input_ids = proc_example[\"input_ids\"]\n",
    "#     start_pos = proc_example[\"start_positions\"]\n",
    "#     end_pos = proc_example[\"end_positions\"]\n",
    "    \n",
    "#     cls_idx = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n",
    "    \n",
    "#     # Skip no-answer cases\n",
    "#     if start_pos == cls_idx:\n",
    "#         continue\n",
    "    \n",
    "#     # Find context boundaries\n",
    "#     sep_indices = [k for k, x in enumerate(input_ids) if x == tokenizer.sep_token_id]\n",
    "#     if not sep_indices:\n",
    "#         continue\n",
    "    \n",
    "#     context_start = sep_indices[0] + 1\n",
    "    \n",
    "#     # Check if answer is near end of context chunk (within last 5 positions)\n",
    "#     # This is where the bug would manifest\n",
    "#     if len(sep_indices) > 1:\n",
    "#         context_end = sep_indices[1]\n",
    "#     else:\n",
    "#         # Find first padding token\n",
    "#         context_end = next((i for i, x in enumerate(input_ids) if x == tokenizer.pad_token_id), len(input_ids))\n",
    "    \n",
    "#     if context_end - end_pos <= 5:\n",
    "#         boundary_total += 1\n",
    "        \n",
    "#         # Verify extraction is correct\n",
    "#         predicted = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True).strip()\n",
    "#         gold = orig_example[\"answer\"].strip()\n",
    "        \n",
    "#         if predicted.lower() == gold.lower():\n",
    "#             boundary_correct += 1\n",
    "\n",
    "# print(f\"\\n   Found {boundary_total} boundary cases (answer near chunk end)\")\n",
    "# if boundary_total > 0:\n",
    "#     boundary_accuracy = boundary_correct / boundary_total * 100\n",
    "#     print(f\"   Boundary cases correct: {boundary_correct}/{boundary_total} ({boundary_accuracy:.1f}%)\")\n",
    "    \n",
    "#     if boundary_accuracy < 95:\n",
    "#         print(f\"   ‚ùå WARNING: Boundary logic may still have issues!\")\n",
    "#     else:\n",
    "#         print(f\"   ‚úÖ Boundary fix is working correctly!\")\n",
    "# else:\n",
    "#     print(f\"   ‚ö†Ô∏è  No boundary cases found in first 1000 examples\")\n",
    "\n",
    "# # Test the specific case from verification script\n",
    "# print(\"\\n2Ô∏è‚É£ Testing the exact bug scenario...\")\n",
    "# # Answer [90, 99] inclusive, Chunk [0, 100) exclusive\n",
    "# test_start = 90\n",
    "# test_end = 99  # inclusive\n",
    "# chunk_start = 0\n",
    "# chunk_end = 100  # exclusive\n",
    "\n",
    "# # Correct logic (what we implemented)\n",
    "# correct_result = test_start >= chunk_start and test_end < chunk_end\n",
    "# # Buggy logic (what we fixed)\n",
    "# buggy_result = test_start >= chunk_start and test_end <= chunk_end\n",
    "\n",
    "# print(f\"   Scenario: answer=[{test_start},{test_end}], chunk=[{chunk_start},{chunk_end})\")\n",
    "# print(f\"   Correct logic (< for end): {correct_result}\")\n",
    "# print(f\"   Buggy logic (<= for end): {buggy_result}\")\n",
    "\n",
    "# if correct_result == True and buggy_result == True:\n",
    "#     print(f\"   ‚úÖ Both agree when answer is inside chunk\")\n",
    "# elif correct_result != buggy_result:\n",
    "#     print(f\"   ‚ö†Ô∏è  Logics differ - this is where the bug would cause mislabeling\")\n",
    "\n",
    "# # Now test the failing case\n",
    "# test_end = 100  # Now extends beyond\n",
    "# correct_result = test_start >= chunk_start and test_end < chunk_end\n",
    "# buggy_result = test_start >= chunk_start and test_end <= chunk_end\n",
    "\n",
    "# print(f\"\\n   Scenario: answer=[{test_start},{test_end}], chunk=[{chunk_start},{chunk_end})\")\n",
    "# print(f\"   Correct logic (< for end): {correct_result} ‚úÖ\")\n",
    "# print(f\"   Buggy logic (<= for end): {buggy_result} ‚ùå\")\n",
    "\n",
    "# if correct_result == False and buggy_result == True:\n",
    "#     print(f\"   ‚úÖ Fix verified: correct logic rejects, buggy logic accepts (WRONG)\")\n",
    "# else:\n",
    "#     print(f\"   ‚ùå Something is wrong with the logic\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1f131",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¨ COMPREHENSIVE QA PIPELINE VERIFICATION\n",
    "\n",
    "Before training, let's verify **every single component** of the QA pipeline end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:54:35.484466Z",
     "iopub.status.busy": "2025-12-12T13:54:35.484262Z",
     "iopub.status.idle": "2025-12-12T13:54:35.488755Z",
     "shell.execute_reply": "2025-12-12T13:54:35.487989Z",
     "shell.execute_reply.started": "2025-12-12T13:54:35.484452Z"
    },
    "id": "D-emFQTIaZRL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# processed_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:54:35.489863Z",
     "iopub.status.busy": "2025-12-12T13:54:35.489575Z",
     "iopub.status.idle": "2025-12-12T13:54:35.503327Z",
     "shell.execute_reply": "2025-12-12T13:54:35.502613Z",
     "shell.execute_reply.started": "2025-12-12T13:54:35.489843Z"
    },
    "id": "Yy3SiWwCabEi",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# processed_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "05d936e2dc9d412b8637c174a3c0be64",
      "7e0b41aa16f241a4ba3bb8a2f3525984",
      "2bebe7e1f3f341dfaabf29963d2c5995",
      "41b300b02ed2413ba80865aaa99ece2a",
      "b4740a7137e742d687e2075b60d2be8a",
      "80132c8e4fa743fca850936ecfebc7f7",
      "303bb3d75f7d4e94aeb60c5491ea6e61",
      "d7463faafecf4e46a87dc6863a646cea",
      "bc199cddba714aeda650d97fef015a14",
      "1a508a6457bb460ba17d5adb0a9e9f85",
      "3aac2656907a416291a622717ccaf929",
      "fa1af70d9c95443c9f09666359ba3769",
      "ddb717fd4dbc40c6bd8422a02f925060",
      "6d1342eeaf4f4f0489fe0746ceaaeb09",
      "a10683e5c1164f349cbdc75b1567994c",
      "71cfe2c8df474badb255d7d28da04348",
      "077fbb403e5f4069841e558a3cc0c065",
      "b068b9fac9f24eca9bd430bab30ea70c",
      "8cbfc6f4ec434674ac59d3fbdbddcd3b",
      "4d675a6788b641c6a09604ef17514dec",
      "bd9b9f21be9744c49d99ac4bc76f11e1",
      "89469ab4bd6f48d8b9aa369473c7230f"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-12-12T13:54:35.504397Z",
     "iopub.status.busy": "2025-12-12T13:54:35.504207Z",
     "iopub.status.idle": "2025-12-12T13:54:36.210137Z",
     "shell.execute_reply": "2025-12-12T13:54:36.209573Z",
     "shell.execute_reply.started": "2025-12-12T13:54:35.504382Z"
    },
    "id": "77ecdd17",
    "outputId": "602e648b-4a75-424b-da09-d58f3295a65e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save newly processed data (OPTIONAL - for future reuse with same filtered dataset)\n",
    "# processed_train.save_to_disk(\"/kaggle/working/cache/processed_train_uqa_filtered\")\n",
    "# processed_val.save_to_disk(\"/kaggle/working/cache/processed_val_uqa_filtered\")\n",
    "\n",
    "# # ‚ùå DO NOT load old cache - it has index mismatches with filtered data!\n",
    "# # If you've already run the preprocessing cell above, skip this cell\n",
    "\n",
    "# processed_train = load_from_disk(\"/kaggle/working/cache/processed_train_uqa_filtered\")\n",
    "# processed_val = load_from_disk(\"/kaggle/working/cache/processed_val_uqa_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:54:36.211138Z",
     "iopub.status.busy": "2025-12-12T13:54:36.210965Z",
     "iopub.status.idle": "2025-12-12T13:54:36.215441Z",
     "shell.execute_reply": "2025-12-12T13:54:36.214758Z",
     "shell.execute_reply.started": "2025-12-12T13:54:36.211123Z"
    },
    "id": "c0e06e6b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-12T13:54:36.216705Z",
     "iopub.status.busy": "2025-12-12T13:54:36.216362Z",
     "iopub.status.idle": "2025-12-12T13:54:36.281097Z",
     "shell.execute_reply": "2025-12-12T13:54:36.280196Z",
     "shell.execute_reply.started": "2025-12-12T13:54:36.216682Z"
    },
    "id": "ba9eeeed",
    "outputId": "27071b6e-b703-4b47-9288-9a1c6f3eba55",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 345602 || all params: 132430084 || trainable%: 0.26096940329661045\n"
     ]
    }
   ],
   "source": [
    "# build LoRA model\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.gradient_checkpointing_enable()\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a98237c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Model Training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:54:36.316807Z",
     "iopub.status.busy": "2025-12-12T13:54:36.316502Z",
     "iopub.status.idle": "2025-12-12T13:54:36.362869Z",
     "shell.execute_reply": "2025-12-12T13:54:36.362227Z",
     "shell.execute_reply.started": "2025-12-12T13:54:36.316787Z"
    },
    "id": "c4abaaab",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/canine-s-uqa-impossible\",\n",
    "\n",
    "    per_device_train_batch_size=4,  # increased train_batch_size from tydiqa\n",
    "    per_device_eval_batch_size=16,\n",
    "\n",
    "    gradient_accumulation_steps=4,  # decreased grad accum from tydiqa\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    num_train_epochs=1, # same as tydiqa\n",
    "    learning_rate=3e-5,  \n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    eval_strategy=\"no\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,  # increased to 1000\n",
    "    logging_steps=50,\n",
    "    \n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"VohraAK/canine-s-uqa-impossible\",\n",
    "    hub_strategy=\"checkpoint\",\n",
    "    )\n",
    "\n",
    "# CustomEvalCallback - EXACT TyDiQA approach\n",
    "class CustomEvalCallback(TrainerCallback):\n",
    "    def __init__(self, eval_func, eval_dataset):\n",
    "        self.eval_func = eval_func\n",
    "        self.eval_dataset = eval_dataset\n",
    "\n",
    "    def on_save(self, args, state, control, model=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Runs AFTER checkpoint is saved.\n",
    "        Loads checkpoint from disk and evaluates it.\n",
    "        \"\"\"\n",
    "        checkpoint_path = f\"{args.output_dir}/checkpoint-{state.global_step}\"\n",
    "        print(f\"\\nüîç Running custom evaluation at step {state.global_step}...\")\n",
    "\n",
    "        # Call evaluation function (loads from checkpoint)\n",
    "        metrics = self.eval_func(checkpoint_path)\n",
    "\n",
    "        # Add metrics to state's log_history\n",
    "        state.log_history.append({\n",
    "            \"step\": state.global_step,\n",
    "            \"eval_exact_match\": metrics[\"exact_match\"],\n",
    "            \"eval_f1\": metrics[\"f1\"],\n",
    "            \"eval_edit_distance\": metrics[\"edit_distance\"],\n",
    "        })\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"‚úÖ Step {state.global_step}: EM={metrics['exact_match']*100:.2f}, F1={metrics['f1']*100:.2f}, EditDist={metrics['edit_distance']*100:.2f}\")\n",
    "\n",
    "        # Re-save trainer_state.json with updated metrics\n",
    "        state_path = f\"{checkpoint_path}/trainer_state.json\"\n",
    "        try:\n",
    "            with open(state_path, 'r') as f:\n",
    "                state_dict = json.load(f)\n",
    "            state_dict['log_history'] = state.log_history\n",
    "            with open(state_path, 'w') as f:\n",
    "                json.dump(state_dict, f, indent=2)\n",
    "            print(f\"üíæ Updated trainer_state.json with custom metrics\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Could not update trainer_state.json: {e}\")\n",
    "\n",
    "        # Push to Hub\n",
    "        try:\n",
    "            print(f\"‚òÅÔ∏è  Pushing checkpoint-{state.global_step} to Hub...\")\n",
    "            api = HfApi()\n",
    "            api.upload_folder(\n",
    "                folder_path=checkpoint_path,\n",
    "                repo_id=args.hub_model_id,\n",
    "                path_in_repo=f\"checkpoint-{state.global_step}\",\n",
    "                commit_message=f\"Add checkpoint {state.global_step} (EM={metrics['exact_match']*100:.1f}%, F1={metrics['f1']*100:.1f}%)\",\n",
    "                repo_type=\"model\"\n",
    "            )\n",
    "            print(f\"‚úÖ Pushed checkpoint-{state.global_step} to Hub\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Could not push to Hub: {e}\")\n",
    "\n",
    "        return control\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T13:54:36.364020Z",
     "iopub.status.busy": "2025-12-12T13:54:36.363703Z",
     "iopub.status.idle": "2025-12-12T13:54:36.919098Z",
     "shell.execute_reply": "2025-12-12T13:54:36.918341Z",
     "shell.execute_reply.started": "2025-12-12T13:54:36.363994Z"
    },
    "id": "055f5dda",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer_cb = CustomEvalCallback(evaluate_checkpoint, processed_val)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_train,\n",
    "    eval_dataset=processed_val,\n",
    "    callbacks=[trainer_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-12-12T13:54:36.920216Z",
     "iopub.status.busy": "2025-12-12T13:54:36.919985Z",
     "iopub.status.idle": "2025-12-12T17:52:21.825625Z",
     "shell.execute_reply": "2025-12-12T17:52:21.824970Z",
     "shell.execute_reply.started": "2025-12-12T13:54:36.920191Z"
    },
    "id": "TOUimesUX5Re",
    "outputId": "cfa62dcd-8eb4-475a-910b-1c38a3894cc2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "cc44692c-6652-4cda-9ba4-8a03acdab88d"
   },
   "source": [
    "### Diagnosing Preprocessing Functions!!!\n",
    "\n",
    "These functions are just analysing the preprocessing logic above, they're just using the base model, NOT our trained model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-12-12T17:52:21.827297Z",
     "iopub.status.busy": "2025-12-12T17:52:21.826772Z",
     "iopub.status.idle": "2025-12-12T17:52:21.832979Z",
     "shell.execute_reply": "2025-12-12T17:52:21.832178Z",
     "shell.execute_reply.started": "2025-12-12T17:52:21.827277Z"
    },
    "id": "49f3717d",
    "outputId": "38f435a4-1b55-4c2b-b6a5-86540fc23755",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Diagnostic cell (fixed): Investigate preprocessing and truncation for many samples\n",
    "# import random\n",
    "# import pandas as pd\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # Set display options to see full Urdu text\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# try:\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"google/canine-s\")\n",
    "# except Exception:\n",
    "#     tokenizer = None\n",
    "\n",
    "# num_samples = 20000  # Number of samples to check\n",
    "# results = []\n",
    "\n",
    "# for split_name, orig_data, proc_data in [\n",
    "#     (\"train\", uqa_train, processed_train),\n",
    "#     (\"val\", uqa_val, processed_val)\n",
    "# ]:\n",
    "#     # Sample random indices\n",
    "#     if len(proc_data) < num_samples:\n",
    "#         current_indices = range(len(proc_data))\n",
    "#     else:\n",
    "#         current_indices = random.sample(range(len(proc_data)), num_samples)\n",
    "\n",
    "#     for idx in current_indices:\n",
    "#         proc = proc_data[idx]\n",
    "#         # Use overflow_to_sample_mapping to get the correct original index\n",
    "#         orig_idx = proc[\"overflow_to_sample_mapping\"]\n",
    "#         orig = orig_data[orig_idx]\n",
    "\n",
    "#         input_ids = proc[\"input_ids\"]\n",
    "#         start_pos = proc[\"start_positions\"]\n",
    "#         end_pos = proc[\"end_positions\"]\n",
    "\n",
    "#         gold_answer = orig.get(\"gold_answer\", orig.get(\"answer\", \"\"))\n",
    "#         question = orig.get(\"question\", \"\")\n",
    "\n",
    "#         # Decode input_ids to text (for debugging context)\n",
    "#         if tokenizer:\n",
    "#             decoded_text = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "#         else:\n",
    "#             decoded_text = str(input_ids)\n",
    "\n",
    "#         # Extract predicted answer span\n",
    "#         if 0 <= start_pos < len(input_ids) and 0 <= end_pos < len(input_ids):\n",
    "#             if tokenizer:\n",
    "#                 pred_span = tokenizer.decode(input_ids[start_pos:end_pos+1], skip_special_tokens=True)\n",
    "#             else:\n",
    "#                 pred_span = str(input_ids[start_pos:end_pos+1])\n",
    "#         else:\n",
    "#             pred_span = \"[CLS]\" # Represents no answer found in this chunk or invalid\n",
    "\n",
    "#         # Check if pred_span matches gold answer\n",
    "#         # We strip() to ignore minor whitespace differences\n",
    "#         pred_matches_gold = pred_span.strip() == gold_answer.strip()\n",
    "\n",
    "#         # Check if gold is even reachable in this chunk\n",
    "#         gold_in_decoded = gold_answer in decoded_text\n",
    "\n",
    "#         results.append({\n",
    "#             \"Split\": split_name,\n",
    "#             \"Question\": question,\n",
    "#             \"Gold Answer\": gold_answer,\n",
    "#             \"Extracted Answer\": pred_span,\n",
    "#             \"Match\": pred_matches_gold,\n",
    "#             \"Gold Reachable\": gold_in_decoded,\n",
    "#             \"orig_idx\": orig_idx\n",
    "#         })\n",
    "\n",
    "# # Create DataFrame\n",
    "# results_df = pd.DataFrame(results)\n",
    "\n",
    "# # --- SIDE BY SIDE COMPARISON ---\n",
    "\n",
    "# # 1. Filter for Solvable Mismatches (Gold was there, but we predicted wrong)\n",
    "# problem_cases = results_df[\n",
    "#     (results_df[\"Gold Reachable\"] == True) &\n",
    "#     (results_df[\"Match\"] == False)\n",
    "# ][[\"Question\", \"Gold Answer\", \"Extracted Answer\", \"Split\"]]\n",
    "\n",
    "# print(f\"üîç Checked {len(results_df)} samples.\")\n",
    "# print(f\"‚ùå Found {len(problem_cases)} cases where Gold was present but Extraction failed.\")\n",
    "\n",
    "# print(\"\\nüìä Side-by-Side Comparison (Top 20 Failures):\")\n",
    "# display(problem_cases.head(50))\n",
    "\n",
    "# print(\"\\n‚úÖ Side-by-Side Comparison (First 10 Rows - Mixed):\")\n",
    "# display(results_df[[\"Question\", \"Gold Answer\", \"Extracted Answer\", \"Match\"]].head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-12T17:52:21.835152Z",
     "iopub.status.busy": "2025-12-12T17:52:21.833967Z",
     "iopub.status.idle": "2025-12-12T17:52:21.853265Z",
     "shell.execute_reply": "2025-12-12T17:52:21.852455Z",
     "shell.execute_reply.started": "2025-12-12T17:52:21.835133Z"
    },
    "id": "e67abc12",
    "outputId": "c597ec41-a56e-4e5d-9eb6-e71bd0eafd38",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Accuracy: fraction of rows where extracted answer matches gold answer\n",
    "# accuracy = (results_df[\"Match\"]).mean()\n",
    "\n",
    "# # Precision: among rows where extracted answer is non-empty, fraction that matches gold\n",
    "# # We filter out cases where the model predicted nothing (empty string) or just whitespace\n",
    "# non_empty_pred = results_df[\"Extracted Answer\"].str.strip() != \"\"\n",
    "\n",
    "# # Avoid division by zero if no predictions were made\n",
    "# if non_empty_pred.sum() > 0:\n",
    "#     precision = (results_df[\"Match\"] & non_empty_pred).sum() / non_empty_pred.sum()\n",
    "# else:\n",
    "#     precision = 0.0\n",
    "\n",
    "# print(f\"Accuracy: {accuracy:.3f}\")\n",
    "# print(f\"Precision: {precision:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
