{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c186240c","cell_type":"code","source":"# %pip install peft evaluate transformers Levenshtein ipywidgets\n# %pip install protobuf==3.20.3","metadata":{"id":"c186240c","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:36:57.880927Z","iopub.execute_input":"2025-11-20T09:36:57.881334Z","iopub.status.idle":"2025-11-20T09:36:57.884936Z","shell.execute_reply.started":"2025-11-20T09:36:57.881301Z","shell.execute_reply":"2025-11-20T09:36:57.884137Z"}},"outputs":[],"execution_count":10},{"id":"cd8da8ab","cell_type":"code","source":"import os\nos.environ[\"TRANSFORMERS_DISABLE_CHAT_TEMPLATES\"] = \"1\"\nos.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\nos.environ[\"TRANSFORMERS_NO_ADDITIONAL_CHAT_TEMPLATES\"] = \"1\"","metadata":{"id":"cd8da8ab","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:36:57.886464Z","iopub.execute_input":"2025-11-20T09:36:57.886715Z","iopub.status.idle":"2025-11-20T09:36:57.901731Z","shell.execute_reply.started":"2025-11-20T09:36:57.886700Z","shell.execute_reply":"2025-11-20T09:36:57.901077Z"}},"outputs":[],"execution_count":11},{"id":"d87eba82","cell_type":"code","source":"from datasets import load_dataset, load_from_disk\n# from UQA.canine_utils import preprocess_uqa, lora_config, print_trainable_parameters, normalize_answer, exact_match_score, f1_score, edit_distance_score, gold_answer, decode_prediction\nfrom transformers import CanineTokenizer\nfrom peft import LoraConfig, TaskType, get_peft_model\nimport re\nimport string\nfrom collections import Counter\nimport numpy as np\nimport Levenshtein\n\nfrom transformers import TrainingArguments, Trainer, TrainerCallback\nimport json\nfrom huggingface_hub import HfApi, notebook_login, whoami","metadata":{"id":"d87eba82","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:36:57.902307Z","iopub.execute_input":"2025-11-20T09:36:57.902539Z","iopub.status.idle":"2025-11-20T09:36:57.915318Z","shell.execute_reply.started":"2025-11-20T09:36:57.902515Z","shell.execute_reply":"2025-11-20T09:36:57.914626Z"}},"outputs":[],"execution_count":12},{"id":"0e98cebe-4c08-4850-b3c1-1529564fdb1b","cell_type":"code","source":"# notebook_login()\n# whoami()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:36:57.916190Z","iopub.execute_input":"2025-11-20T09:36:57.916521Z","iopub.status.idle":"2025-11-20T09:36:57.928043Z","shell.execute_reply.started":"2025-11-20T09:36:57.916497Z","shell.execute_reply":"2025-11-20T09:36:57.927388Z"}},"outputs":[],"execution_count":13},{"id":"d474e2e8","cell_type":"code","source":"uqa_dataset = load_dataset(\"uqa/UQA\")\nuqa_train = uqa_dataset[\"train\"].shuffle(seed=42).select(range(40000))\nuqa_val = uqa_dataset[\"validation\"].shuffle(seed=42).select(range(10000))","metadata":{"id":"d474e2e8","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:36:57.930153Z","iopub.execute_input":"2025-11-20T09:36:57.930467Z","iopub.status.idle":"2025-11-20T09:37:01.926369Z","shell.execute_reply.started":"2025-11-20T09:36:57.930442Z","shell.execute_reply":"2025-11-20T09:37:01.925766Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/898 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77b809eb903a430885db38e2d4066f0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001-bac007e8ca7192(‚Ä¶):   0%|          | 0.00/30.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a2e99a87a4b49eda0b59ab22fd56a6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001-cf8a6960d(‚Ä¶):   0%|          | 0.00/2.92M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e49093f42cd4d4db76170d1faff678c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/124745 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2ea378e95bb4c63a52ef23c75f8d29a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/16824 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f46bf4ff5b6474ba4aa53a88e3a815f"}},"metadata":{}}],"execution_count":14},{"id":"f2dd5a40","cell_type":"code","source":"from transformers import CanineTokenizer, CanineForQuestionAnswering\nimport torch\nmodel_name = 'google/canine-c'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n\ntokenizer = CanineTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=False)\nmodel = CanineForQuestionAnswering.from_pretrained(model_name, trust_remote_code=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f2dd5a40","outputId":"4ea979ff-002f-491f-bbb3-debb423da2b6","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:37:01.927023Z","iopub.execute_input":"2025-11-20T09:37:01.927227Z","iopub.status.idle":"2025-11-20T09:37:06.689854Z","shell.execute_reply.started":"2025-11-20T09:37:01.927211Z","shell.execute_reply":"2025-11-20T09:37:06.689240Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/892 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7510809053c43c9808fc31a995f4f02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/657 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52c6b8f3332a4c4f91b1cd17b758bf5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be638eb7c6174c91bd8254ba8602cfd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/528M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77ed901478fb406f928f9fcb9033f49b"}},"metadata":{}},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":15},{"id":"438d8765","cell_type":"code","source":"# preprocessors\nMAX_SEQ_LENGTH = 384\nDOC_STRIDE = 64\n\ndef _build_byte_to_char_index(text):\n    cumulative = [0]\n    for char in text:\n        cumulative.append(cumulative[-1] + len(char.encode(\"utf-8\")))\n    return cumulative\n\ndef _byte_to_char(cumulative_bytes, byte_index):\n    from bisect import bisect_right\n    position = bisect_right(cumulative_bytes, byte_index) - 1\n    return max(position, 0)\n\ndef preprocess_uqa(examples, tokenizer, max_length=MAX_SEQ_LENGTH, doc_stride=DOC_STRIDE):\n    questions = [q.strip() for q in examples[\"question\"]]\n    contexts = examples[\"context\"]\n    answers = examples[\"answer\"]\n    answer_starts = examples[\"answer_start\"]\n    special_tokens = tokenizer.num_special_tokens_to_add(pair=True)\n    encoded = {\n        \"input_ids\": [],\n        \"attention_mask\": [],\n        \"token_type_ids\": [],\n        \"start_positions\": [],\n        \"end_positions\": [],\n        \"overflow_to_sample_mapping\": [],\n    }\n    for example_idx, (question, context, answer, answer_start) in enumerate(zip(questions, contexts, answers, answer_starts)):\n        question_tokens = tokenizer.encode(question, add_special_tokens=False)\n        context_tokens = tokenizer.encode(context, add_special_tokens=False)\n        max_context_tokens = max_length - len(question_tokens) - special_tokens\n        if max_context_tokens <= 0 or not context_tokens:\n            continue\n        if answer and answer_start != -1:\n            byte_map = _build_byte_to_char_index(context)\n            start_char = _byte_to_char(byte_map, answer_start)\n            end_char = _byte_to_char(byte_map, max(answer_start + len(answer) - 1, answer_start))\n            answer_span = (start_char, end_char)\n        else:\n            answer_span = None\n        stride_tokens = max_context_tokens - doc_stride\n        if stride_tokens <= 0:\n            stride_tokens = max_context_tokens\n        span_start = 0\n        context_length = len(context_tokens)\n        while span_start < context_length:\n            span_end = min(span_start + max_context_tokens, context_length)\n            context_chunk = context_tokens[span_start:span_end]\n            input_ids = tokenizer.build_inputs_with_special_tokens(question_tokens, context_chunk)\n            token_type_ids = tokenizer.create_token_type_ids_from_sequences(question_tokens, context_chunk)\n            attention_mask = [1] * len(input_ids)\n            cls_index = input_ids.index(tokenizer.cls_token_id)\n            context_offset = len(input_ids) - len(context_chunk) - 1\n            if answer_span is None:\n                start_pos = cls_index\n                end_pos = cls_index\n            else:\n                start_char, end_char = answer_span\n                answer_in_chunk = start_char >= span_start and end_char < span_end\n                if answer_in_chunk:\n                    start_pos = context_offset + (start_char - span_start)\n                    end_pos = context_offset + (end_char - span_start)\n                else:\n                    start_pos = cls_index\n                    end_pos = cls_index\n            padding = max_length - len(input_ids)\n            if padding > 0:\n                pad_id = tokenizer.pad_token_id\n                input_ids += [pad_id] * padding\n                attention_mask += [0] * padding\n                token_type_ids += [0] * padding\n            else:\n                input_ids = input_ids[:max_length]\n                attention_mask = attention_mask[:max_length]\n                token_type_ids = token_type_ids[:max_length]\n                if start_pos >= max_length or end_pos >= max_length:\n                    start_pos = cls_index\n                    end_pos = cls_index\n            encoded[\"input_ids\"].append(input_ids)\n            encoded[\"attention_mask\"].append(attention_mask)\n            encoded[\"token_type_ids\"].append(token_type_ids)\n            encoded[\"start_positions\"].append(start_pos)\n            encoded[\"end_positions\"].append(end_pos)\n            encoded[\"overflow_to_sample_mapping\"].append(example_idx)\n            if span_end == context_length:\n                break\n            span_start += stride_tokens\n    return encoded\n\n\n","metadata":{"id":"438d8765","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:37:06.690642Z","iopub.execute_input":"2025-11-20T09:37:06.690828Z","iopub.status.idle":"2025-11-20T09:37:06.702721Z","shell.execute_reply.started":"2025-11-20T09:37:06.690813Z","shell.execute_reply":"2025-11-20T09:37:06.701887Z"}},"outputs":[],"execution_count":16},{"id":"a3e95eec","cell_type":"code","source":"# LoRA config\nlora_config = LoraConfig(\n    task_type=TaskType.QUESTION_ANS,\n    r=32,   # changed from 8\n    lora_alpha=64,  # changed from 32\n    lora_dropout=0.1,\n    target_modules=[\"query\", \"value\", \"key\"],   # added key, output.dense\n    bias=\"none\",\n    modules_to_save=[\"qa_outputs\"],\n)\n\ndef print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n","metadata":{"id":"a3e95eec","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:37:06.703522Z","iopub.execute_input":"2025-11-20T09:37:06.703770Z","iopub.status.idle":"2025-11-20T09:37:06.719743Z","shell.execute_reply.started":"2025-11-20T09:37:06.703746Z","shell.execute_reply":"2025-11-20T09:37:06.718944Z"}},"outputs":[],"execution_count":17},{"id":"d11807b9","cell_type":"code","source":"# preprocess the train and val splits\nprocessed_train = uqa_train.map(lambda examples: preprocess_uqa(examples, tokenizer), batched=True, remove_columns=uqa_train.column_names)\nprocessed_val = uqa_val.map(lambda examples: preprocess_uqa(examples, tokenizer), batched=True, remove_columns=uqa_val.column_names)","metadata":{"id":"d11807b9","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:37:06.720494Z","iopub.execute_input":"2025-11-20T09:37:06.720751Z","iopub.status.idle":"2025-11-20T09:38:15.899344Z","shell.execute_reply.started":"2025-11-20T09:37:06.720728Z","shell.execute_reply":"2025-11-20T09:38:15.898611Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21e0ecfc9321415c84c8af9d66e22f54"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (3179 > 2048). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92a01036d1184d96940d96d5c9276b53"}},"metadata":{}}],"execution_count":18},{"id":"D-emFQTIaZRL","cell_type":"code","source":"processed_train","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-emFQTIaZRL","outputId":"eece57cd-d7ed-4931-dda0-b7a34d9c6e95","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:38:15.900476Z","iopub.execute_input":"2025-11-20T09:38:15.900716Z","iopub.status.idle":"2025-11-20T09:38:15.906273Z","shell.execute_reply.started":"2025-11-20T09:38:15.900696Z","shell.execute_reply":"2025-11-20T09:38:15.905552Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'token_type_ids', 'start_positions', 'end_positions', 'overflow_to_sample_mapping'],\n    num_rows: 116995\n})"},"metadata":{}}],"execution_count":19},{"id":"Yy3SiWwCabEi","cell_type":"code","source":"processed_val","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yy3SiWwCabEi","outputId":"52db5812-b46b-4a23-8781-bec0571d73c2","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:38:15.907113Z","iopub.execute_input":"2025-11-20T09:38:15.907389Z","iopub.status.idle":"2025-11-20T09:38:16.132738Z","shell.execute_reply.started":"2025-11-20T09:38:15.907365Z","shell.execute_reply":"2025-11-20T09:38:16.132069Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'token_type_ids', 'start_positions', 'end_positions', 'overflow_to_sample_mapping'],\n    num_rows: 31446\n})"},"metadata":{}}],"execution_count":20},{"id":"77ecdd17","cell_type":"code","source":"processed_train.save_to_disk(\"/kaggle/working/cache/processed_train_uqa\")\nprocessed_val.save_to_disk(\"/kaggle/working/cache/processed_val_uqa\")   # cached it\n\n\nprocessed_train = load_from_disk(\"/kaggle/working/cache/processed_train_uqa\")\nprocessed_val = load_from_disk(\"/kaggle/working/cache/processed_val_uqa\")","metadata":{"id":"77ecdd17","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:38:16.135085Z","iopub.execute_input":"2025-11-20T09:38:16.135334Z","iopub.status.idle":"2025-11-20T09:38:16.618172Z","shell.execute_reply.started":"2025-11-20T09:38:16.135315Z","shell.execute_reply":"2025-11-20T09:38:16.617540Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/116995 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22c72b8500a14f16b24f2185b2272cf0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/31446 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53f0d8538a78476e8f7e5080f22f7cbd"}},"metadata":{}}],"execution_count":21},{"id":"c0e06e6b","cell_type":"code","source":"def print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n\n","metadata":{"id":"c0e06e6b","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:38:16.619009Z","iopub.execute_input":"2025-11-20T09:38:16.619293Z","iopub.status.idle":"2025-11-20T09:38:16.623845Z","shell.execute_reply.started":"2025-11-20T09:38:16.619257Z","shell.execute_reply":"2025-11-20T09:38:16.623086Z"}},"outputs":[],"execution_count":22},{"id":"ba9eeeed","cell_type":"code","source":"# build LoRA model\n\npeft_model = get_peft_model(model, lora_config)\npeft_model.gradient_checkpointing_enable()\nprint_trainable_parameters(peft_model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ba9eeeed","outputId":"e987b70d-b2a2-4618-d538-578bff1909bb","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:38:16.624723Z","iopub.execute_input":"2025-11-20T09:38:16.625039Z","iopub.status.idle":"2025-11-20T09:38:16.714194Z","shell.execute_reply.started":"2025-11-20T09:38:16.625014Z","shell.execute_reply":"2025-11-20T09:38:16.713330Z"}},"outputs":[{"name":"stdout","text":"trainable params: 2065922 || all params: 134150404 || trainable%: 1.5400043074040985\n","output_type":"stream"}],"execution_count":23},{"id":"61c5c7a7","cell_type":"code","source":"# evals\n\n\ndef normalize_answer(text):\n    text = (text or \"\").lower()\n    def remove_articles(s):\n        return re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n    def remove_punctuation(s):\n        return \"\".join(ch for ch in s if ch not in string.punctuation)\n    def white_space_fix(s):\n        return \" \".join(s.split())\n    return white_space_fix(remove_articles(remove_punctuation(text)))\n\ndef exact_match_score(prediction, ground_truth):\n    return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n\ndef f1_score(prediction, ground_truth):\n    pred_tokens = normalize_answer(prediction).split()\n    gold_tokens = normalize_answer(ground_truth).split()\n    if not gold_tokens:\n        return 1.0 if not pred_tokens else 0.0\n    if not pred_tokens:\n        return 0.0\n    common = Counter(pred_tokens) & Counter(gold_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0.0\n    precision = num_same / len(pred_tokens)\n    recall = num_same / len(gold_tokens)\n    return 2 * precision * recall / (precision + recall)\n\ndef edit_distance_score(prediction, ground_truth):\n    pred_norm = normalize_answer(prediction)\n    gold_norm = normalize_answer(ground_truth)\n    if not gold_norm and not pred_norm:\n        return 1.0\n    if not gold_norm or not pred_norm:\n        return 0.0\n    distance = Levenshtein.distance(pred_norm, gold_norm)\n    max_len = max(len(pred_norm), len(gold_norm))\n    return 1.0 - (distance / max_len) if max_len > 0 else 1.0\n\ndef gold_answer(example):\n    # Extracts the gold answer substring from the context using character offsets\n    answer = example.get(\"answer\")\n    context = example.get(\"context\")\n    answer_start = example.get(\"answer_start\", -1)\n    if answer and answer_start is not None and answer_start != -1:\n        return context[answer_start: answer_start + len(answer)]\n    return \"[CLS]\"\n\ndef decode_prediction(input_ids, start_idx, end_idx, tokenizer=None):\n    if start_idx > end_idx:\n        start_idx, end_idx = end_idx, start_idx\n    if tokenizer is None:\n        raise ValueError(\"Tokenizer must be provided for decoding.\")\n    cls_index = input_ids.index(tokenizer.cls_token_id)\n    # If both point to CLS token, return [CLS] sentinel\n    if start_idx == cls_index and end_idx == cls_index:\n        return \"[CLS]\"\n    start_idx = max(start_idx, 0)\n    end_idx = min(end_idx, len(input_ids) - 1)\n    if start_idx > end_idx:\n        return \"[CLS]\"\n    text = tokenizer.decode(input_ids[start_idx:end_idx + 1], skip_special_tokens=True)\n    text = text.strip()\n    return text if text else \"[CLS]\"\n\ndef evaluate_checkpoint(checkpoint_path=None):\n    # Load base CANINE and wrap with the LoRA adapter from checkpoint_path\n    base_model = CanineForQuestionAnswering.from_pretrained(model_name, trust_remote_code=False)\n    model = get_peft_model(base_model, lora_config)\n    # Try loading adapter weights; fall back to PeftModel.from_pretrained if needed\n    try:\n        model.load_adapter(checkpoint_path)\n    except Exception:\n        from peft import PeftModel\n        model = PeftModel.from_pretrained(base_model, checkpoint_path)\n    model.to(device)\n\n    eval_args = TrainingArguments(\n    # Small evaluation config; uses cpu/mps if no gpu during eval\n        output_dir=\"outputs/canine-uqa\",\n        per_device_eval_batch_size=1,\n        dataloader_drop_last=False,\n        fp16=False,\n        bf16=False,\n        report_to=\"none\",\n    )\n\n    eval_trainer = Trainer(\n        model=model,\n        args=eval_args,\n        eval_dataset=processed_val,\n        tokenizer=tokenizer,\n    )\n\n    # Run predictions and collapse overlapping features by score\n    predictions = eval_trainer.predict(processed_val)\n    start_logits, end_logits = predictions.predictions\n    best_predictions = {}\n    for feature_index, feature in enumerate(processed_val):\n        sample_idx = int(feature[\"overflow_to_sample_mapping\"])\n        input_ids = feature[\"input_ids\"]\n        start_idx = int(np.argmax(start_logits[feature_index]))\n        end_idx = int(np.argmax(end_logits[feature_index]))\n        score = float(start_logits[feature_index][start_idx] + end_logits[feature_index][end_idx])\n        prediction_text = decode_prediction(input_ids, start_idx, end_idx, tokenizer=tokenizer)\n        stored = best_predictions.get(sample_idx)\n        if stored is None or score > stored[0]:\n            best_predictions[sample_idx] = (score, prediction_text)\n\n    em_scores = []\n    f1_scores = []\n    edit_dist_scores = []\n    for sample_idx, (_, prediction_text) in best_predictions.items():\n        reference = gold_answer(uqa_val[int(sample_idx)])\n        em_scores.append(exact_match_score(prediction_text, reference))\n        f1_scores.append(f1_score(prediction_text, reference))\n        edit_dist_scores.append(edit_distance_score(prediction_text, reference))\n\n    em = float(np.mean(em_scores)) if em_scores else 0.0\n    f1 = float(np.mean(f1_scores)) if f1_scores else 0.0\n    edit_dist = float(np.mean(edit_dist_scores)) if edit_dist_scores else 0.0\n    print(f\"Examples evaluated: {len(em_scores)}\")\n    print(f\"Exact Match: {em * 100:.2f}\")\n    print(f\"F1: {f1 * 100:.2f}\")\n    print(f\"Edit Distance (normalized): {edit_dist * 100:.2f}\")\n    return {\"exact_match\": em, \"f1\": f1, \"edit_distance\": edit_dist}","metadata":{"id":"61c5c7a7","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:38:16.715069Z","iopub.execute_input":"2025-11-20T09:38:16.715432Z","iopub.status.idle":"2025-11-20T09:38:16.731494Z","shell.execute_reply.started":"2025-11-20T09:38:16.715412Z","shell.execute_reply":"2025-11-20T09:38:16.730659Z"}},"outputs":[],"execution_count":24},{"id":"c4abaaab","cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"outputs/canine-uqa\",\n    \n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=16,\n    \n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    \n    num_train_epochs=1,\n    learning_rate=3e-4,\n    weight_decay=0.01,\n    eval_strategy=\"no\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=500,\n    logging_steps=25,\n    fp16=True,\n    bf16=False,\n    report_to=\"none\",\n    push_to_hub=True,\n    hub_model_id=\"VohraAK/canine-uqa\",\n    hub_strategy=\"checkpoint\",\n    )\n\nclass CustomEvalCallback(TrainerCallback):\n    def __init__(self, eval_func, eval_dataset):\n        self.eval_func = eval_func\n        self.eval_dataset = eval_dataset\n    def on_save(self, args, state, control, model=None, **kwargs):\n        checkpoint_path = f\"{args.output_dir}/checkpoint-{state.global_step}\"\n        print(f\"\\nüîç Running custom evaluation at step {state.global_step}...\")\n        metrics = self.eval_func(checkpoint_path)\n        state.log_history.append({\n            \"step\": state.global_step,\n            \"eval_exact_match\": metrics[\"exact_match\"],\n            \"eval_f1\": metrics[\"f1\"],\n            \"eval_edit_distance\": metrics[\"edit_distance\"],\n        })\n        print(f\"‚úÖ Step {state.global_step}: EM={metrics['exact_match']*100:.2f}, F1={metrics['f1']*100:.2f}, EditDist={metrics['edit_distance']*100:.2f}\")\n        state_path = f\"{checkpoint_path}/trainer_state.json\"\n        try:\n            with open(state_path, 'r') as f:\n                state_dict = json.load(f)\n            state_dict['log_history'] = state.log_history\n            with open(state_path, 'w') as f:\n                json.dump(state_dict, f, indent=2)\n            print(f\"üíæ Updated trainer_state.json with custom metrics\")\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Warning: Could not update trainer_state.json: {e}\")\n        try:\n            print(f\"‚òÅÔ∏è  Pushing checkpoint-{state.global_step} to Hub...\")\n            api = HfApi()\n            api.upload_folder(\n                folder_path=checkpoint_path,\n                repo_id=args.hub_model_id,\n                path_in_repo=f\"checkpoint-{state.global_step}\",\n                commit_message=f\"Add checkpoint {state.global_step} (EM={metrics['exact_match']*100:.1f}%, F1={metrics['f1']*100:.1f}%)\",\n                repo_type=\"model\"\n            )\n            print(f\"‚úÖ Pushed checkpoint-{state.global_step} to Hub\")\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Warning: Could not push to Hub: {e}\")\n        return control\n","metadata":{"id":"c4abaaab","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:38:16.732271Z","iopub.execute_input":"2025-11-20T09:38:16.732571Z","iopub.status.idle":"2025-11-20T09:38:16.777513Z","shell.execute_reply.started":"2025-11-20T09:38:16.732542Z","shell.execute_reply":"2025-11-20T09:38:16.776717Z"}},"outputs":[],"execution_count":25},{"id":"055f5dda","cell_type":"code","source":"trainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=processed_train,\n    eval_dataset=processed_val,\n    callbacks=[CustomEvalCallback(evaluate_checkpoint, processed_val)],\n)\n","metadata":{"id":"055f5dda","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:38:16.778364Z","iopub.execute_input":"2025-11-20T09:38:16.778639Z","iopub.status.idle":"2025-11-20T09:38:17.225252Z","shell.execute_reply.started":"2025-11-20T09:38:16.778614Z","shell.execute_reply":"2025-11-20T09:38:17.224691Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":26},{"id":"TOUimesUX5Re","cell_type":"code","source":"trainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"TOUimesUX5Re","outputId":"c0ada6de-de08-41ce-e8fc-5c59232de758","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:38:17.226119Z","iopub.execute_input":"2025-11-20T09:38:17.226746Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1001' max='7313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1001/7313 18:39 < 1:57:53, 0.89 it/s, Epoch 0.14/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>5.688700</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>5.304500</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>4.864200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>4.672000</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>4.253200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>4.228800</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>3.800800</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>3.535300</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>3.365600</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>3.483900</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>3.383900</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>3.081600</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>3.047300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>2.910400</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>2.867700</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.737200</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>2.713700</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>2.723600</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>2.427500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.934400</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>2.522600</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>2.512000</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>2.403500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.275600</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>2.714100</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>2.307600</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>2.376300</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>2.516900</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>2.449400</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>2.363300</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>2.254900</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>2.229100</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>2.580800</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>2.145700</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>2.183000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>2.095900</td>\n    </tr>\n    <tr>\n      <td>925</td>\n      <td>2.373900</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>2.558600</td>\n    </tr>\n    <tr>\n      <td>975</td>\n      <td>2.116400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nüîç Running custom evaluation at step 500...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_112/627878087.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Examples evaluated: 1000\nExact Match: 33.10\nF1: 33.10\nEdit Distance (normalized): 33.23\n‚úÖ Step 500: EM=33.10, F1=33.10, EditDist=33.23\nüíæ Updated trainer_state.json with custom metrics\n‚òÅÔ∏è  Pushing checkpoint-500 to Hub...\n‚úÖ Pushed checkpoint-500 to Hub\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nüîç Running custom evaluation at step 1000...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/tmp/ipykernel_112/627878087.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Trainer(\nNo label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15390' max='31446' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15390/31446 07:27 < 07:47, 34.38 it/s]\n    </div>\n    "},"metadata":{}}],"execution_count":null}]}