{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c186240c",
      "metadata": {
        "id": "c186240c"
      },
      "outputs": [],
      "source": [
        "# %pip install peft evaluate transformers Levenshtein ipywidgets\n",
        "# %pip install protobuf==3.20.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd8da8ab",
      "metadata": {
        "id": "cd8da8ab"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "os.environ[\"TRANSFORMERS_DISABLE_CHAT_TEMPLATES\"] = \"1\"\n",
        "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
        "os.environ[\"TRANSFORMERS_NO_ADDITIONAL_CHAT_TEMPLATES\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d87eba82",
      "metadata": {
        "id": "d87eba82"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_from_disk\n",
        "# from UQA.canine_utils import preprocess_uqa, lora_config, print_trainable_parameters, normalize_answer, exact_match_score, f1_score, edit_distance_score, gold_answer, decode_prediction\n",
        "from transformers import CanineTokenizer\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import Levenshtein\n",
        "\n",
        "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
        "import json\n",
        "from huggingface_hub import HfApi, notebook_login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d474e2e8",
      "metadata": {
        "id": "d474e2e8"
      },
      "outputs": [],
      "source": [
        "uqa_dataset = load_dataset(\"uqa/UQA\")\n",
        "uqa_train = uqa_dataset[\"train\"].shuffle(seed=42).select(range(40000))\n",
        "uqa_val = uqa_dataset[\"validation\"].shuffle(seed=42).select(range(10000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2dd5a40",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2dd5a40",
        "outputId": "4ea979ff-002f-491f-bbb3-debb423da2b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import CanineTokenizer, CanineForQuestionAnswering\n",
        "import torch\n",
        "model_name = 'google/canine-c'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
        "\n",
        "tokenizer = CanineTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=False)\n",
        "model = CanineForQuestionAnswering.from_pretrained(model_name, trust_remote_code=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "438d8765",
      "metadata": {
        "id": "438d8765"
      },
      "outputs": [],
      "source": [
        "# preprocessors\n",
        "MAX_SEQ_LENGTH = 384\n",
        "DOC_STRIDE = 64\n",
        "\n",
        "def _build_byte_to_char_index(text):\n",
        "    cumulative = [0]\n",
        "    for char in text:\n",
        "        cumulative.append(cumulative[-1] + len(char.encode(\"utf-8\")))\n",
        "    return cumulative\n",
        "\n",
        "def _byte_to_char(cumulative_bytes, byte_index):\n",
        "    from bisect import bisect_right\n",
        "    position = bisect_right(cumulative_bytes, byte_index) - 1\n",
        "    return max(position, 0)\n",
        "\n",
        "def preprocess_uqa(examples, tokenizer, max_length=MAX_SEQ_LENGTH, doc_stride=DOC_STRIDE):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    contexts = examples[\"context\"]\n",
        "    answers = examples[\"answer\"]\n",
        "    answer_starts = examples[\"answer_start\"]\n",
        "    special_tokens = tokenizer.num_special_tokens_to_add(pair=True)\n",
        "    encoded = {\n",
        "        \"input_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"start_positions\": [],\n",
        "        \"end_positions\": [],\n",
        "        \"overflow_to_sample_mapping\": [],\n",
        "    }\n",
        "    for example_idx, (question, context, answer, answer_start) in enumerate(zip(questions, contexts, answers, answer_starts)):\n",
        "        question_tokens = tokenizer.encode(question, add_special_tokens=False)\n",
        "        context_tokens = tokenizer.encode(context, add_special_tokens=False)\n",
        "        max_context_tokens = max_length - len(question_tokens) - special_tokens\n",
        "        if max_context_tokens <= 0 or not context_tokens:\n",
        "            continue\n",
        "        if answer and answer_start != -1:\n",
        "            byte_map = _build_byte_to_char_index(context)\n",
        "            start_char = _byte_to_char(byte_map, answer_start)\n",
        "            end_char = _byte_to_char(byte_map, max(answer_start + len(answer) - 1, answer_start))\n",
        "            answer_span = (start_char, end_char)\n",
        "        else:\n",
        "            answer_span = None\n",
        "        stride_tokens = max_context_tokens - doc_stride\n",
        "        if stride_tokens <= 0:\n",
        "            stride_tokens = max_context_tokens\n",
        "        span_start = 0\n",
        "        context_length = len(context_tokens)\n",
        "        while span_start < context_length:\n",
        "            span_end = min(span_start + max_context_tokens, context_length)\n",
        "            context_chunk = context_tokens[span_start:span_end]\n",
        "            input_ids = tokenizer.build_inputs_with_special_tokens(question_tokens, context_chunk)\n",
        "            token_type_ids = tokenizer.create_token_type_ids_from_sequences(question_tokens, context_chunk)\n",
        "            attention_mask = [1] * len(input_ids)\n",
        "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "            context_offset = len(input_ids) - len(context_chunk) - 1\n",
        "            if answer_span is None:\n",
        "                start_pos = cls_index\n",
        "                end_pos = cls_index\n",
        "            else:\n",
        "                start_char, end_char = answer_span\n",
        "                answer_in_chunk = start_char >= span_start and end_char < span_end\n",
        "                if answer_in_chunk:\n",
        "                    start_pos = context_offset + (start_char - span_start)\n",
        "                    end_pos = context_offset + (end_char - span_start)\n",
        "                else:\n",
        "                    start_pos = cls_index\n",
        "                    end_pos = cls_index\n",
        "            padding = max_length - len(input_ids)\n",
        "            if padding > 0:\n",
        "                pad_id = tokenizer.pad_token_id\n",
        "                input_ids += [pad_id] * padding\n",
        "                attention_mask += [0] * padding\n",
        "                token_type_ids += [0] * padding\n",
        "            else:\n",
        "                input_ids = input_ids[:max_length]\n",
        "                attention_mask = attention_mask[:max_length]\n",
        "                token_type_ids = token_type_ids[:max_length]\n",
        "                if start_pos >= max_length or end_pos >= max_length:\n",
        "                    start_pos = cls_index\n",
        "                    end_pos = cls_index\n",
        "            encoded[\"input_ids\"].append(input_ids)\n",
        "            encoded[\"attention_mask\"].append(attention_mask)\n",
        "            encoded[\"token_type_ids\"].append(token_type_ids)\n",
        "            encoded[\"start_positions\"].append(start_pos)\n",
        "            encoded[\"end_positions\"].append(end_pos)\n",
        "            encoded[\"overflow_to_sample_mapping\"].append(example_idx)\n",
        "            if span_end == context_length:\n",
        "                break\n",
        "            span_start += stride_tokens\n",
        "    return encoded\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3e95eec",
      "metadata": {
        "id": "a3e95eec"
      },
      "outputs": [],
      "source": [
        "# LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.QUESTION_ANS,\n",
        "    r=16,   # changed from 8\n",
        "    lora_alpha=64,  # changed from 32\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"query\", \"value\", \"key\"],   # added key\n",
        "    bias=\"none\",\n",
        "    modules_to_save=[\"qa_outputs\"],\n",
        ")\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d11807b9",
      "metadata": {
        "id": "d11807b9"
      },
      "outputs": [],
      "source": [
        "# preprocess the train and val splits\n",
        "# processed_train = uqa_train.map(lambda examples: preprocess_uqa(examples, tokenizer), batched=True, remove_columns=uqa_train.column_names)\n",
        "# processed_val = uqa_val.map(lambda examples: preprocess_uqa(examples, tokenizer), batched=True, remove_columns=uqa_val.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D-emFQTIaZRL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-emFQTIaZRL",
        "outputId": "eece57cd-d7ed-4931-dda0-b7a34d9c6e95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'token_type_ids', 'start_positions', 'end_positions', 'overflow_to_sample_mapping'],\n",
              "    num_rows: 116995\n",
              "})"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Yy3SiWwCabEi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy3SiWwCabEi",
        "outputId": "52db5812-b46b-4a23-8781-bec0571d73c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'token_type_ids', 'start_positions', 'end_positions', 'overflow_to_sample_mapping'],\n",
              "    num_rows: 31446\n",
              "})"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77ecdd17",
      "metadata": {
        "id": "77ecdd17"
      },
      "outputs": [],
      "source": [
        "# processed_train.save_to_disk(\"cache/processed_train_uqa\")\n",
        "# processed_val.save_to_disk(\"cache/processed_val_uqa\")   # cached it\n",
        "\n",
        "\n",
        "processed_train = load_from_disk(\"cache/processed_train_uqa\")\n",
        "processed_val = load_from_disk(\"cache/processed_val_uqa\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0e06e6b",
      "metadata": {
        "id": "c0e06e6b"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba9eeeed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba9eeeed",
        "outputId": "e987b70d-b2a2-4618-d538-578bff1909bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1033730 || all params: 133118212 || trainable%: 0.7765503941714602\n"
          ]
        }
      ],
      "source": [
        "# build LoRA model\n",
        "\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "peft_model.gradient_checkpointing_enable()\n",
        "print_trainable_parameters(peft_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61c5c7a7",
      "metadata": {
        "id": "61c5c7a7"
      },
      "outputs": [],
      "source": [
        "# evals\n",
        "\n",
        "\n",
        "def normalize_answer(text):\n",
        "    text = (text or \"\").lower()\n",
        "    def remove_articles(s):\n",
        "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n",
        "    def remove_punctuation(s):\n",
        "        return \"\".join(ch for ch in s if ch not in string.punctuation)\n",
        "    def white_space_fix(s):\n",
        "        return \" \".join(s.split())\n",
        "    return white_space_fix(remove_articles(remove_punctuation(text)))\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return float(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    pred_tokens = normalize_answer(prediction).split()\n",
        "    gold_tokens = normalize_answer(ground_truth).split()\n",
        "    if not gold_tokens:\n",
        "        return 1.0 if not pred_tokens else 0.0\n",
        "    if not pred_tokens:\n",
        "        return 0.0\n",
        "    common = Counter(pred_tokens) & Counter(gold_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0.0\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall = num_same / len(gold_tokens)\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "def edit_distance_score(prediction, ground_truth):\n",
        "    pred_norm = normalize_answer(prediction)\n",
        "    gold_norm = normalize_answer(ground_truth)\n",
        "    if not gold_norm and not pred_norm:\n",
        "        return 1.0\n",
        "    if not gold_norm or not pred_norm:\n",
        "        return 0.0\n",
        "    distance = Levenshtein.distance(pred_norm, gold_norm)\n",
        "    max_len = max(len(pred_norm), len(gold_norm))\n",
        "    return 1.0 - (distance / max_len) if max_len > 0 else 1.0\n",
        "\n",
        "def gold_answer(example):\n",
        "    # Extracts the gold answer substring from the context using character offsets\n",
        "    answer = example.get(\"answer\")\n",
        "    context = example.get(\"context\")\n",
        "    answer_start = example.get(\"answer_start\", -1)\n",
        "    if answer and answer_start is not None and answer_start != -1:\n",
        "        return context[answer_start: answer_start + len(answer)]\n",
        "    return \"[CLS]\"\n",
        "\n",
        "def decode_prediction(input_ids, start_idx, end_idx, tokenizer=None):\n",
        "    if start_idx > end_idx:\n",
        "        start_idx, end_idx = end_idx, start_idx\n",
        "    if tokenizer is None:\n",
        "        raise ValueError(\"Tokenizer must be provided for decoding.\")\n",
        "    cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "    # If both point to CLS token, return [CLS] sentinel\n",
        "    if start_idx == cls_index and end_idx == cls_index:\n",
        "        return \"[CLS]\"\n",
        "    start_idx = max(start_idx, 0)\n",
        "    end_idx = min(end_idx, len(input_ids) - 1)\n",
        "    if start_idx > end_idx:\n",
        "        return \"[CLS]\"\n",
        "    text = tokenizer.decode(input_ids[start_idx:end_idx + 1], skip_special_tokens=True)\n",
        "    text = text.strip()\n",
        "    return text if text else \"[CLS]\"\n",
        "\n",
        "def evaluate_checkpoint(checkpoint_path=None):\n",
        "    # Load base CANINE and wrap with the LoRA adapter from checkpoint_path\n",
        "    base_model = CanineForQuestionAnswering.from_pretrained(model_name, trust_remote_code=False)\n",
        "    model = get_peft_model(base_model, lora_config)\n",
        "    # Try loading adapter weights; fall back to PeftModel.from_pretrained if needed\n",
        "    try:\n",
        "        model.load_adapter(checkpoint_path)\n",
        "    except Exception:\n",
        "        from peft import PeftModel\n",
        "        model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
        "    model.to(device)\n",
        "\n",
        "    eval_args = TrainingArguments(\n",
        "    # Small evaluation config; uses cpu/mps if no gpu during eval\n",
        "        output_dir=\"outputs/canine-uqa\",\n",
        "        per_device_eval_batch_size=1,\n",
        "        dataloader_drop_last=False,\n",
        "        fp16=False,\n",
        "        bf16=False,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    eval_trainer = Trainer(\n",
        "        model=model,\n",
        "        args=eval_args,\n",
        "        eval_dataset=processed_val,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    # Run predictions and collapse overlapping features by score\n",
        "    predictions = eval_trainer.predict(processed_val)\n",
        "    start_logits, end_logits = predictions.predictions\n",
        "    best_predictions = {}\n",
        "    for feature_index, feature in enumerate(processed_val):\n",
        "        sample_idx = int(feature[\"overflow_to_sample_mapping\"])\n",
        "        input_ids = feature[\"input_ids\"]\n",
        "        start_idx = int(np.argmax(start_logits[feature_index]))\n",
        "        end_idx = int(np.argmax(end_logits[feature_index]))\n",
        "        score = float(start_logits[feature_index][start_idx] + end_logits[feature_index][end_idx])\n",
        "        prediction_text = decode_prediction(input_ids, start_idx, end_idx, tokenizer=tokenizer)\n",
        "        stored = best_predictions.get(sample_idx)\n",
        "        if stored is None or score > stored[0]:\n",
        "            best_predictions[sample_idx] = (score, prediction_text)\n",
        "\n",
        "    em_scores = []\n",
        "    f1_scores = []\n",
        "    edit_dist_scores = []\n",
        "    for sample_idx, (_, prediction_text) in best_predictions.items():\n",
        "        reference = gold_answer(uqa_val[int(sample_idx)])\n",
        "        em_scores.append(exact_match_score(prediction_text, reference))\n",
        "        f1_scores.append(f1_score(prediction_text, reference))\n",
        "        edit_dist_scores.append(edit_distance_score(prediction_text, reference))\n",
        "\n",
        "    em = float(np.mean(em_scores)) if em_scores else 0.0\n",
        "    f1 = float(np.mean(f1_scores)) if f1_scores else 0.0\n",
        "    edit_dist = float(np.mean(edit_dist_scores)) if edit_dist_scores else 0.0\n",
        "    print(f\"Examples evaluated: {len(em_scores)}\")\n",
        "    print(f\"Exact Match: {em * 100:.2f}\")\n",
        "    print(f\"F1: {f1 * 100:.2f}\")\n",
        "    print(f\"Edit Distance (normalized): {edit_dist * 100:.2f}\")\n",
        "    return {\"exact_match\": em, \"f1\": f1, \"edit_distance\": edit_dist}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4abaaab",
      "metadata": {
        "id": "c4abaaab"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"outputs/canine-uqa\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"no\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    logging_steps=25,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"VohraAK/canine-uqa\",\n",
        "    hub_strategy=\"checkpoint\",\n",
        "    )\n",
        "\n",
        "class CustomEvalCallback(TrainerCallback):\n",
        "    def __init__(self, eval_func, eval_dataset):\n",
        "        self.eval_func = eval_func\n",
        "        self.eval_dataset = eval_dataset\n",
        "    def on_save(self, args, state, control, model=None, **kwargs):\n",
        "        checkpoint_path = f\"{args.output_dir}/checkpoint-{state.global_step}\"\n",
        "        print(f\"\\nüîç Running custom evaluation at step {state.global_step}...\")\n",
        "        metrics = self.eval_func(checkpoint_path)\n",
        "        state.log_history.append({\n",
        "            \"step\": state.global_step,\n",
        "            \"eval_exact_match\": metrics[\"exact_match\"],\n",
        "            \"eval_f1\": metrics[\"f1\"],\n",
        "            \"eval_edit_distance\": metrics[\"edit_distance\"],\n",
        "        })\n",
        "        print(f\"‚úÖ Step {state.global_step}: EM={metrics['exact_match']*100:.2f}, F1={metrics['f1']*100:.2f}, EditDist={metrics['edit_distance']*100:.2f}\")\n",
        "        state_path = f\"{checkpoint_path}/trainer_state.json\"\n",
        "        try:\n",
        "            with open(state_path, 'r') as f:\n",
        "                state_dict = json.load(f)\n",
        "            state_dict['log_history'] = state.log_history\n",
        "            with open(state_path, 'w') as f:\n",
        "                json.dump(state_dict, f, indent=2)\n",
        "            print(f\"üíæ Updated trainer_state.json with custom metrics\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Warning: Could not update trainer_state.json: {e}\")\n",
        "        try:\n",
        "            print(f\"‚òÅÔ∏è  Pushing checkpoint-{state.global_step} to Hub...\")\n",
        "            api = HfApi()\n",
        "            api.upload_folder(\n",
        "                folder_path=checkpoint_path,\n",
        "                repo_id=args.hub_model_id,\n",
        "                path_in_repo=f\"checkpoint-{state.global_step}\",\n",
        "                commit_message=f\"Add checkpoint {state.global_step} (EM={metrics['exact_match']*100:.1f}%, F1={metrics['f1']*100:.1f}%)\",\n",
        "                repo_type=\"model\"\n",
        "            )\n",
        "            print(f\"‚úÖ Pushed checkpoint-{state.global_step} to Hub\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Warning: Could not push to Hub: {e}\")\n",
        "        return control\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "055f5dda",
      "metadata": {
        "id": "055f5dda"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=processed_train,\n",
        "    eval_dataset=processed_val,\n",
        "    callbacks=[CustomEvalCallback(evaluate_checkpoint, processed_val)],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TOUimesUX5Re",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TOUimesUX5Re",
        "outputId": "c0ada6de-de08-41ce-e8fc-5c59232de758"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2501' max='7313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2501/7313 1:34:19 < 3:01:38, 0.44 it/s, Epoch 0.34/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>5.920700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>5.872800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>5.817400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>5.797200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>5.743800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>5.731800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>5.659500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>5.630200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>5.582900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>5.583400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>5.536700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>5.470100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>5.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>5.401600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>5.369800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>5.347800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>5.299500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>5.262600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>5.193900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>5.251800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>5.155200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>5.117400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>5.075000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>5.041800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>5.059500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>4.950000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>675</td>\n",
              "      <td>4.949800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>4.961600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>725</td>\n",
              "      <td>4.940700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>4.863800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>775</td>\n",
              "      <td>4.788200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>4.721300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>825</td>\n",
              "      <td>4.851400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>4.706500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>875</td>\n",
              "      <td>4.699300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>4.585800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>925</td>\n",
              "      <td>4.652000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>4.701500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>975</td>\n",
              "      <td>4.556500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>4.627400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1025</td>\n",
              "      <td>4.557300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>4.537500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1075</td>\n",
              "      <td>4.554100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>4.577100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1125</td>\n",
              "      <td>4.506200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>4.400900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1175</td>\n",
              "      <td>4.460900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>4.334600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1225</td>\n",
              "      <td>4.366400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>4.411100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1275</td>\n",
              "      <td>4.272700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>4.354600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1325</td>\n",
              "      <td>4.252400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>4.175000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1375</td>\n",
              "      <td>4.266500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>4.321800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1425</td>\n",
              "      <td>4.163000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>4.234000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1475</td>\n",
              "      <td>4.075300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>4.162500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1525</td>\n",
              "      <td>4.092100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>4.250600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1575</td>\n",
              "      <td>4.042300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>4.028700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1625</td>\n",
              "      <td>4.020500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>4.084700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1675</td>\n",
              "      <td>3.894600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>3.982500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1725</td>\n",
              "      <td>3.916200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>3.933000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1775</td>\n",
              "      <td>3.873200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>3.751400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1825</td>\n",
              "      <td>4.044600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>3.816100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1875</td>\n",
              "      <td>3.712800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>3.772600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1925</td>\n",
              "      <td>3.774300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>3.657900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1975</td>\n",
              "      <td>3.636300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>3.797800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2025</td>\n",
              "      <td>3.497200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>3.943900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2075</td>\n",
              "      <td>3.737100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>3.686700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2125</td>\n",
              "      <td>3.687100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>3.753400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2175</td>\n",
              "      <td>3.599500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>3.704600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2225</td>\n",
              "      <td>3.648600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>3.610600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2275</td>\n",
              "      <td>3.605000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>3.567900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2325</td>\n",
              "      <td>3.647900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>3.334800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2375</td>\n",
              "      <td>3.344100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>3.344300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2425</td>\n",
              "      <td>3.395400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>3.343700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2475</td>\n",
              "      <td>3.525000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Running custom evaluation at step 500...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1108388997.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  eval_trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Examples evaluated: 1000\n",
            "Exact Match: 31.10\n",
            "F1: 31.18\n",
            "Edit Distance (normalized): 31.54\n",
            "‚úÖ Step 500: EM=31.10, F1=31.18, EditDist=31.54\n",
            "üíæ Updated trainer_state.json with custom metrics\n",
            "‚òÅÔ∏è  Pushing checkpoint-500 to Hub...\n",
            "‚úÖ Pushed checkpoint-500 to Hub\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Running custom evaluation at step 1000...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1108388997.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  eval_trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Examples evaluated: 1000\n",
            "Exact Match: 32.90\n",
            "F1: 32.91\n",
            "Edit Distance (normalized): 33.07\n",
            "‚úÖ Step 1000: EM=32.90, F1=32.91, EditDist=33.07\n",
            "üíæ Updated trainer_state.json with custom metrics\n",
            "‚òÅÔ∏è  Pushing checkpoint-1000 to Hub...\n",
            "‚úÖ Pushed checkpoint-1000 to Hub\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Running custom evaluation at step 1500...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1108388997.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  eval_trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Examples evaluated: 1000\n",
            "Exact Match: 33.10\n",
            "F1: 33.10\n",
            "Edit Distance (normalized): 33.23\n",
            "‚úÖ Step 1500: EM=33.10, F1=33.10, EditDist=33.23\n",
            "üíæ Updated trainer_state.json with custom metrics\n",
            "‚òÅÔ∏è  Pushing checkpoint-1500 to Hub...\n",
            "‚úÖ Pushed checkpoint-1500 to Hub\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Running custom evaluation at step 2000...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1108388997.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  eval_trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Examples evaluated: 1000\n",
            "Exact Match: 33.10\n",
            "F1: 33.10\n",
            "Edit Distance (normalized): 33.23\n",
            "‚úÖ Step 2000: EM=33.10, F1=33.10, EditDist=33.23\n",
            "üíæ Updated trainer_state.json with custom metrics\n",
            "‚òÅÔ∏è  Pushing checkpoint-2000 to Hub...\n",
            "‚úÖ Pushed checkpoint-2000 to Hub\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Running custom evaluation at step 2500...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of CanineForQuestionAnswering were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1108388997.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  eval_trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30623' max='31446' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [30623/31446 15:41 < 00:25, 32.52 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
